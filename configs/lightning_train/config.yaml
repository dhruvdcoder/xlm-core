defaults:
  - _self_
  - paths: default
  - hydra: default
  - datamodule: ??? # NEEDED
  - noise_schedule: dummy # default
  - model_type: ??? #NEEDED: ilm_stopping # discrete_time_absorbing
  - model: ??? # NEEDED: rotary_transformer_small_ilm_stopping # replace with ???
  - callbacks: default_v2
  - loggers: default
  - trainer_strategy: single_device
  - experiment: null # inject arbitrary overrides
  - debug: null


# Keep common utils
hydra:
  job:
    env_set:
      PROJECT_ROOT: "."
      OMP_NUM_THREADS: "1"
  searchpath:
    - file://${oc.env:PROJECT_ROOT}/configs/common

################################################################################
# region: main global config that controls the components
seed: 1
job_type: train
job_name: job_type=${job_type}__model_type=${tags.model_type}__model=${tags.model}__scheduler=${tags.noise_schedule}__dataset=${tags.dataset}__tokenizer=${tags.tokenizer}__collation=${tags.collation_strategy}__debug=${tags.debug}
global_flags: null
checkpointing_dir: ${paths.output_dir}/checkpoints
resume_from_checkpoint: true
resume_checkpoint_path: null
compile: false
per_device_batch_size: 64
global_batch_size: 512
block_size: 128
num_dataloader_workers: ${oc.decode:${oc.env:SLURM_CPUS_PER_TASK,4}}
num_dataset_workers: ${oc.decode:${oc.env:SLURM_CPUS_PER_TASK,4}}
dataloader_prefetch_factor: 5
dataloader_persistent_workers: false # INFO: see: https://github.com/huggingface/datasets/issues/7447
dataloader_pin_memory: true
# endregion: main global config that controls the components
################################################################################

################################################################################
# region: global components
global_components:
  _target_: builtins.dict
  tokenizer:
    _target_: xlm.datamodule.BertTokenizerFast.from_pretrained
    pretrained_model_name_or_path: bert-base-uncased
  noise_schedule:
    _target_: xlm.noise.DummyNoiseSchedule

# endregion: global components

################################################################################
# region: components
trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  strategy: auto
  devices: ${device_count:}
  max_steps: 1000_000 # 1M steps # TODO (config): make this dynamically calculated
  default_root_dir: ${paths.output_dir}
  accumulate_grad_batches: ${find_grad_accum:${global_batch_size},${per_device_batch_size},${.devices},${.num_nodes}}
  gradient_clip_val: 1.0
  log_every_n_steps: 50
  val_check_interval: 50000
  num_sanity_val_steps: 5
  # TODO (efficiency): precision bf16 or other

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.01

lr_scheduler:
  name: "constant_with_warmup" # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L435
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 1000
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"

generative_perplexity:
  num_samples: 64 # total samples
  evaluators: null

log_predictions:
  _target_: xlm.log_predictions.LogPredictions
  fields_to_keep_in_output:
    - text
  writers:
    - file
    - logger

tags:
  debug: false
  model_type: ???
  model: ???
  dataset: ???
  noise_schedule: ???
  job_type: ${job_type}