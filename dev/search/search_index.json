{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#a-unified-framework-for-non-autoregressive-language-models","title":"A Unified Framework for Non-Autoregressive Language Models","text":"<p>XLM is a modular, research-friendly framework for developing and comparing non-autoregressive language models. Built on PyTorch and PyTorch Lightning, with Hydra for configuration management, XLM makes it effortless to experiment with cutting-edge NAR architectures.</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description Modular Design Plug-and-play components\u2014swap models, losses, predictors, and collators independently Lightning-Powered Distributed training, mixed precision, and logging out of the box Hydra Configs Hierarchical configuration with runtime overrides\u2014no code changes needed Multiple Architectures Multiple model families ready to use as baselines Research-First Lightweight, and type annotated with <code>jaxtyping</code>, several debug for quick testing, and flexible code injection points for practially limitless customization Hub Integration Push trained models directly to Hugging Face Hub"},{"location":"#available-models","title":"Available Models","text":"Model Full Name Description <code>mlm</code> Masked Language Model Classic BERT-style masked prediction <code>ilm</code> Insertion Language Model Insertion-based generation <code>arlm</code> Autoregressive LM Standard left-to-right baseline <code>mdlm</code> Masked Diffusion LM Discrete diffusion with masking"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install xlm-core\n</code></pre> <p>For model implementations, also install:</p> <pre><code>pip install xlm-models\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>XLM uses a simple CLI with three main arguments:</p> <pre><code>xlm job_type=&lt;JOB&gt; job_name=&lt;NAME&gt; experiment=&lt;CONFIG&gt;\n</code></pre> Argument Description <code>job_type</code> One of <code>prepare_data</code>, <code>train</code>, <code>eval</code>, or <code>generate</code> <code>job_name</code> A descriptive name for your run <code>experiment</code> Path to your Hydra experiment config"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2013 Installation, CLI usage, and example workflow</li> <li>API Reference \u2013 xlm-core and xlm-models API documentation</li> <li>Contributing \u2013 Guidelines for adding new models and features</li> </ul>"},{"location":"guide/building-docs/","title":"Building Docs","text":"<p>This page describes how the XLM documentation is built and deployed.</p>"},{"location":"guide/building-docs/#local-build","title":"Local Build","text":"<p>Install dependencies and build the docs:</p> <pre><code>pip install -e .\npip install -e xlm-models/\npip install -r requirements/docs_requirements.txt\nmkdocs build\n</code></pre>"},{"location":"guide/building-docs/#local-serve","title":"Local Serve","text":"<p>To preview the docs with live reload:</p> <pre><code>mkdocs serve\n</code></pre> <p>Open http://127.0.0.1:8000 in your browser.</p>"},{"location":"guide/building-docs/#versioning","title":"Versioning","text":"<p>The docs use mike for multi-version deployment:</p> Version URL path Description <code>dev</code> <code>/dev/</code> Development build (main branch) <code>latest</code> <code>/latest/</code> Newest release (alias) <code>1.4</code> <code>/1.4/</code> Release v1.4.0 <ul> <li>dev \u2013 Always tracks <code>main</code>; deployed on every push to main.</li> <li>latest \u2013 Points to the most recent release; updated when a new release is published.</li> <li>Versioned releases \u2013 Each release (e.g., v1.4.0) is deployed to its <code>major.minor</code> path (e.g., <code>/1.4/</code>).</li> </ul>"},{"location":"guide/building-docs/#build-and-serve-all-versions-locally","title":"Build and serve all versions locally","text":"<p>If <code>gh-pages</code> already has versions (e.g. from CI):</p> <pre><code>git fetch origin gh-pages\nmike serve\n</code></pre> <p>Then open http://localhost:8000 and use the version selector.</p> <p>If <code>gh-pages</code> is empty or you want to test new versions locally before pushing:</p> <pre><code># Checkout main for dev\ngit checkout main\n\n# Install deps (if you already have, use the load the virtual environment)\npip install -e . -e xlm-models/ -r requirements/docs_requirements.txt\n\n# Deploy dev locally (no --push)\nmike deploy dev\n\n# Optional: deploy a release version (e.g. after checking out a tag)\ngit checkout v1.4.0  # or whatever tag\nmike deploy 1.4 latest --update-aliases\n</code></pre> <p>Then serve:</p> <pre><code>mike serve\n</code></pre> <p>For a quick single-version preview without multi-version support, use <code>mkdocs serve</code> instead.</p>"},{"location":"guide/building-docs/#structure","title":"Structure","text":"<p>The docs use:</p> <ul> <li>MkDocs \u2013 Static site generator</li> <li>Material theme \u2013 Navigation tabs, sections, integrated TOC, and version selector</li> <li>mkdocstrings \u2013 API reference generated from Python docstrings</li> <li>mike \u2013 Versioning and multi-version deployment</li> </ul>"},{"location":"guide/building-docs/#ci-deployment","title":"CI Deployment","text":""},{"location":"guide/building-docs/#main-branch-dev","title":"Main branch (dev)","text":"<p>The .github/workflows/docs.yml workflow:</p> <ol> <li>Triggers on push to <code>main</code> or manual <code>workflow_dispatch</code></li> <li>Installs xlm-core, xlm-models, and idlm</li> <li>Installs MkDocs, Material, mkdocstrings, and mike</li> <li>Runs <code>mike deploy dev --push</code> to deploy to the <code>/dev/</code> path on the <code>gh-pages</code> branch</li> </ol>"},{"location":"guide/building-docs/#releases-versioned-latest","title":"Releases (versioned + latest)","text":"<p>The .github/workflows/docs-release.yml workflow:</p> <ol> <li>Triggers when a GitHub release is published</li> <li>Checks out the release tag</li> <li>Extracts the version (e.g., <code>v1.4.0</code> \u2192 <code>1.4</code>)</li> <li>Runs <code>mike deploy 1.4 --update-aliases latest --push</code></li> <li>Runs <code>mike set-default latest --push</code> so the site root redirects to the latest release</li> </ol> <p>Prerequisite: Enable GitHub Pages in repo Settings \u2192 Pages \u2192 Source: \"Deploy from a branch\" \u2192 Branch: <code>gh-pages</code> / <code>/(root)</code>.</p>"},{"location":"guide/building-docs/#dependencies","title":"Dependencies","text":"<p>Install docs dependencies from <code>requirements/docs_requirements.txt</code>:</p> <pre><code>mkdocs&gt;=1.6\nmkdocs-material&gt;=9.0\nmkdocstrings[python]&gt;=0.24\nmike&gt;=2.0\n</code></pre>"},{"location":"guide/contributing/","title":"Adding a new language model architecture to XLM","text":"<p>For scaffolding a new external model, see External Models. This document outlines the process for adding a new language model architecture to the XLM codebase. The code follows a modular design with four main components that work together to provide a complete language modeling solution. You need to implement all of them in order to add a new working model.</p>"},{"location":"guide/contributing/#overview-of-main-components","title":"Overview of Main Components","text":""},{"location":"guide/contributing/#1-lossfunction","title":"1. LossFunction","text":"<p>The <code>LossFunction</code> is responsible for computing the training loss during model training, validation and optionally test time.</p> <p>Key Responsibilities: - Compute loss between model predictions and ground truth targets - Return a dictionary with \"loss\" key and any other additional values that you want to track. </p> <p>Interface:</p> <pre><code>class LossFunction(Generic[T_in, T_out], Protocol):\n    model: Any\n    tokenizer: Tokenizer\n\n    def loss_fn(self, batch: T_in, ...) -&gt; T_out: ...\n    def configure(self, pl_module: \"Harness\"): ...\n        \"\"\"Converts scalar to tensor such that loss_fn becomes compile friendly. If you don't want to compile you don't need to implement this.\n        \"\"\"\n</code></pre> <p>Examples: <code>xlm-models/arlm/loss_arlm.py</code></p>"},{"location":"guide/contributing/#2-predictor","title":"2. Predictor","text":"<p>The <code>Predictor</code> handles generating output sequences from the model. </p> <p>Key Responsibilities: - Run the model (typically in a loop) to produce a sequence of tokens. - Convert generated token_ids to text.</p> <p>Interface:</p> <pre><code>class Predictor(Generic[T_in, T_out_pred], Protocol):\n    tokenizer: Tokenizer\n    noise_schedule: NoiseSchedule\n    model: Any\n\n    def predict(self, batch: T_in, ...) -&gt; T_out_pred: ...\n    def to_dict(self, batch: T_in, preds: T_out_pred, ...) -&gt; List[Dict[str, Any]]: ...\n</code></pre> <p>Examples: <code>xlm-models/arlm/predictor_arlm.py</code></p>"},{"location":"guide/contributing/#3-collator","title":"3. Collator","text":"<p>The <code>Collator</code> prepares batches of data for training and inference. It handles data preprocessing, padding, and batching.</p> <p>Key Responsibilities: - It receives raw token_ids and converts them to a dict which is passed in as a batch to the model. - Handle padding and truncation.</p> <p>Interface:</p> <pre><code>class Collator(Protocol):\n    \"\"\"For pre-training the model on language modeling.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n\nclass Seq2SeqCollator(Collator):\n    \"\"\"For training the model on seq2seq tasks.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n\nclass Seq2SeqCollatorPrediction(Collator):\n    \"\"\"For generating predictions for seq2seq tasks.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n</code></pre> <p>Examples: <code>xlm-models/arlm/datamodule_arlm.py</code></p>"},{"location":"guide/contributing/#4-model","title":"4. Model","text":"<p>The <code>Model</code> is the bare neural network architecture for your LM. It defines the forward pass and model parameters.</p> <p>Key Responsibilities: - Define the neural network architecture. - Implement the forward pass.</p> <p>Interface:</p> <pre><code>class Model:\n    def forward(self, input_ids: Tensor, attention_mask: Optional[Tensor] = None, ...) -&gt; Tensor: ...\n</code></pre> <p>Examples: <code>xlm-models/arlm/model_arlm.py</code></p> <p>All these four components are designed to be aware of each other, and are only expected to run with each other for the same LM and not with any other LM. This is a key design choice that allows one to implement really esoteric models without worrying about how to abstract them such that their dataflow becomes compatible with other LMs.</p>"},{"location":"guide/contributing/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>See the External Models guide for the recommended workflow using <code>xlm-scaffold</code>. The scaffold creates the directory structure, type definitions, and config stubs. You then implement the four components (Model, Loss, Predictor, Collator) and register them in Hydra configuration.</p> <p>Key config locations (paths may vary for external models in <code>xlm-models/</code>):</p> <ul> <li><code>configs/lightning_train/collator/</code> \u2013 Collator configs</li> <li><code>configs/lightning_train/datamodule/</code> \u2013 Datamodule configs</li> <li><code>configs/lightning_train/model/</code> \u2013 Model (neural network) configs</li> <li><code>configs/lightning_train/model_type/</code> \u2013 Loss, predictor, metrics</li> <li><code>configs/lightning_train/experiment/</code> \u2013 Experiment configs</li> </ul>"},{"location":"guide/contributing/#design","title":"Design","text":""},{"location":"guide/contributing/#why-is-there-so-much-nesting-in-the-datamodule-config","title":"Why is there so much nesting in the datamodule config?","text":"<p>The main component of the datamodule are the <code>dataset_managers</code>. Each <code>dataset_manager</code> will generate its own <code>dataloader</code> with its own <code>collator</code> and processing functions. This design allows:</p> <ol> <li>Chaining arbitrary number of \"eval\" tasks/datasets during validation or testing</li> <li>Injecting new eval tasks post-training</li> </ol>"},{"location":"guide/contributing/#best-practices","title":"Best Practices","text":"<ul> <li>Type Safety: Use <code>jaxtyping</code> for tensor type annotations; define clear interfaces with <code>TypedDict</code> and <code>Protocol</code></li> <li>Modularity: Keep components loosely coupled; use dependency injection through configuration</li> <li>Testing: Use debug mode configs (<code>debug=overfit</code>, <code>debug=small_data</code>) for quick testing</li> <li>Documentation: Document all public interfaces; include examples in docstrings</li> </ul>"},{"location":"guide/contributing/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Hydra Errors: If you see <code>Unable to find or instantiate abc.xyz.MyClass</code>, try importing manually: <code>python -c \"from abc.xyz import MyClass\"</code>.</li> <li>Unable to implement a component: Check existing models in <code>xlm-models/</code> (arlm, mlm, ilm, mdlm) for reference.</li> </ol>"},{"location":"guide/design/","title":"Design","text":"<p>This document describes the data pipeline and design decisions in XLM.</p>"},{"location":"guide/design/#data-pipeline-simplifications","title":"Data Pipeline Simplifications","text":"<ol> <li>Separate task types \u2013 Keep their pipelines separate:</li> <li>Pre-training (map-style)</li> <li>Pre-training (iterable-style)</li> <li>Left-conditional generation (seq2seq)</li> <li> <p>Arbitrary infilling generation (infill)</p> </li> <li> <p>Special tokens \u2013 Do not add special tokens while tokenizing. Add them during collation (or on-the-fly for packed sequences).</p> </li> </ol> When Model Task during collation ILM pre-training during collation ILM left-conditional generation training during collation ILM left-conditional generation inference during collation ILM arbitrary infilling generation training during collation ILM arbitrary infilling generation inference during collation ARLM unpacked pre-training during on-the-fly ARLM packed pre-training during collation ARLM left-conditional generation training during collation ARLM left-conditional generation inference during collation MDLM unpacked pre-training during on-the-fly MDLM packed pre-training during on-the-fly IT pre-training"},{"location":"guide/design/#special-tokens","title":"Special Tokens","text":"<p>For ILM we need three special tokens: CLS for classification, BOS for target starting. Placing the BOS after prefix signals to the model that the prefix is immutable.</p> <ul> <li>Case 1: Place CLS before the prefix</li> <li>(Pro) The position of the CLS token is fixed</li> <li> <p>(Con) In seq2seq setting, because of left-padding the position of the CLS token can get staggered</p> </li> <li> <p>Case 2: Place CLS before the target</p> </li> <li>(Pro) Can be generalized to per-gap CLS token</li> <li>(Con) The position is not fixed; we need to modify the model to read-out this position dynamically</li> </ul> <p>For now, we place the CLS token before the prefix.</p>"},{"location":"guide/design/#general-flow","title":"General Flow","text":"<p>For each stage (train, val, test, predict), we can have multiple datasets. The <code>DatasetManager</code> abstraction performs:</p> <ol> <li>Prepare \u2013 On rank 0 only: download and tokenize</li> <li>Setup \u2013 On all ranks: load data; for iterable datasets, split by rank/world size</li> <li>Create dataloaders \u2013 One per dataset, with appropriate sampler for DDP/non-DDP and map-style/iterable-style</li> </ol>"},{"location":"guide/design/#ddp-with-iterable-dataset","title":"DDP with Iterable Dataset","text":"<ol> <li>Prepare \u2013 On rank 0, save to disk using <code>.save_to_disk(num_shards)</code></li> <li>Setup \u2013 <code>load_from_disk()</code> followed by <code>.to_iterable_dataset(num_shards)</code>, <code>.shuffle(buffer_size)</code>, <code>.split_dataset_by_node()</code></li> <li>Create dataloaders \u2013 Use <code>StatefulDataLoader</code> without a sampler</li> </ol>"},{"location":"guide/design/#best-practices","title":"Best Practices","text":"<ul> <li>Reduce the number of uneven shards to avoid partial batches across workers</li> <li>Choose <code>num_shards</code> such that remainder conditions are satisfied</li> <li>Ensure no node can accumulate more than 1 micro batch worth of extra examples</li> </ul>"},{"location":"guide/external-models/","title":"External Language Models for XLM","text":"<p>XLM supports external language models that can be developed and maintained separately from the core framework. This allows researchers to keep their model code clean and self-contained, and share models without including the entire XLM codebase.</p>"},{"location":"guide/external-models/#quick-start","title":"Quick Start","text":""},{"location":"guide/external-models/#1-scaffold-a-new-model","title":"1. Scaffold a New Model","text":"<p>Use the scaffolding script to create a complete model structure:</p> <pre><code>xlm-scaffold my_awesome_model\n</code></pre> <p>This creates:</p> <ul> <li>A complete Python package with skeleton implementations</li> <li>All necessary Hydra configuration files</li> <li>Registers the model in <code>xlm_models.json</code></li> </ul>"},{"location":"guide/external-models/#2-implement-your-model","title":"2. Implement Your Model","text":"<p>The scaffolded files contain detailed TODOs and docstrings. Key files to implement:</p> <ul> <li><code>my_awesome_model/types_my_awesome_model.py</code> - Type definitions</li> <li><code>my_awesome_model/model_my_awesome_model.py</code> - Neural network architecture</li> <li><code>my_awesome_model/loss_my_awesome_model.py</code> - Loss computation</li> <li><code>my_awesome_model/predictor_my_awesome_model.py</code> - Inference/generation logic</li> <li><code>my_awesome_model/datamodule_my_awesome_model.py</code> - Data preprocessing</li> <li><code>my_awesome_model/metrics_my_awesome_model.py</code> - Metrics computation</li> </ul>"},{"location":"guide/external-models/#3-test-your-model","title":"3. Test Your Model","text":"<pre><code>xlm job_type=train \\\n  job_name=my_model_test \\\n  experiment=star_easy_my_awesome_model \\\n  debug=overfit\n</code></pre>"},{"location":"guide/external-models/#model-structure","title":"Model Structure","text":"<p>Each external model follows this structure:</p> <pre><code>my_awesome_model/                    # Model root directory\n\u251c\u2500\u2500 my_awesome_model/                # Python package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 types_my_awesome_model.py\n\u2502   \u251c\u2500\u2500 model_my_awesome_model.py\n\u2502   \u251c\u2500\u2500 loss_my_awesome_model.py\n\u2502   \u251c\u2500\u2500 predictor_my_awesome_model.py\n\u2502   \u251c\u2500\u2500 datamodule_my_awesome_model.py\n\u2502   \u2514\u2500\u2500 metrics_my_awesome_model.py\n\u251c\u2500\u2500 configs/                         # Hydra configurations\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 model_type/\n\u2502   \u251c\u2500\u2500 collator/\n\u2502   \u251c\u2500\u2500 datamodule/\n\u2502   \u251c\u2500\u2500 experiment/\n\u2502   \u2514\u2500\u2500 commands.yaml                # Optional: custom commands\n\u251c\u2500\u2500 setup.py                         # Package installation (optional)\n\u2514\u2500\u2500 README.md                        # Documentation\n</code></pre>"},{"location":"guide/external-models/#discovery-methods","title":"Discovery Methods","text":"<p>XLM discovers external models through two approaches:</p>"},{"location":"guide/external-models/#1-directory-based-discovery","title":"1. Directory-Based Discovery","text":"<p>Place your model directory in one of these locations:</p> <ul> <li>Current directory (<code>.</code>)</li> <li><code>xlm-models/</code> directory</li> <li>Directory specified by <code>XLM_MODELS_PATH</code> environment variable</li> </ul> <p>Create a <code>xlm_models.json</code> file in the search directory:</p> <pre><code>{\n  \"my_awesome_model\": \"my_awesome_model\",\n  \"another_model\": \"path/to/another_model\"\n}\n</code></pre> <p>The paths are relative to the directory containing <code>xlm_models.json</code>.</p> <p>Example:</p> <pre><code># Project structure\n.\n\u251c\u2500\u2500 xlm_models.json          # {\"my_model\": \"my_model\"}\n\u251c\u2500\u2500 my_model/\n\u2502   \u251c\u2500\u2500 my_model/            # Python package\n\u2502   \u2514\u2500\u2500 configs/\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/external-models/#2-package-based-discovery","title":"2. Package-Based Discovery","text":"<p>Install your model as a Python package and register it via the <code>XLM_MODELS_PACKAGES</code> environment variable.</p> <p>Package structure requirements:</p> <p>Each model needs its own <code>setup.py</code> that packages the configs:</p> <pre><code># setup.py\nsetup(\n    name=\"my_awesome_model\",\n    packages=[\"my_awesome_model\"],\n    package_data={\n        \"my_awesome_model\": [\"configs/**/*.yaml\", \"configs/**/*.yml\"],\n    },\n    include_package_data=True,\n)\n</code></pre> <p>Installation and registration:</p> <p>Each model must be installed independently:</p> <pre><code># Install first model\npip install -e ./my_awesome_model\n\n# Install second model (separate setup.py)\npip install -e ./another_model\n\n# Register both installed packages\nexport XLM_MODELS_PACKAGES=\"my_awesome_model:another_model\"\n</code></pre> <p>Core models (<code>arlm</code>, <code>mlm</code>, <code>ilm</code>, <code>mdlm</code>) are automatically discovered and don't need to be added to <code>XLM_MODELS_PACKAGES</code>.</p>"},{"location":"guide/external-models/#custom-commands","title":"Custom Commands","text":"<p>Models can define custom commands that extend XLM's CLI by creating <code>configs/commands.yaml</code>:</p> <pre><code># configs/commands.yaml\nmy_custom_command: \"my_awesome_model.commands.my_function\"\npreprocess_data: \"my_awesome_model.commands.preprocess\"\n</code></pre> <p>Usage:</p> <pre><code>xlm command=my_custom_command arg1=value1 arg2=value2\n</code></pre> <p>The command functions should accept an <code>omegaconf.DictConfig</code> parameter containing the Hydra configuration.</p>"},{"location":"guide/external-models/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>XLM_MODELS_PATH</code>: Additional directory to search for <code>xlm_models.json</code> files</li> </ul> <p><code>bash   export XLM_MODELS_PATH=\"/path/to/external/models\"</code></p> <ul> <li><code>XLM_MODELS_PACKAGES</code>: Colon-separated list of installed Python packages to discover</li> </ul> <p><code>bash   export XLM_MODELS_PACKAGES=\"my_model1:my_model2\"</code></p>"},{"location":"guide/external-models/#configuration","title":"Configuration","text":"<p>External models integrate with Hydra's configuration system. Use them in your experiments:</p> <pre><code># configs/experiment/my_experiment.yaml\ndefaults:\n  - override /model: my_awesome_model\n  - override /model_type: my_awesome_model\n  - override /datamodule: star_easy_my_awesome_model\n</code></pre> <p>Or via command line:</p> <pre><code>xlm job_type=train model=my_awesome_model model_type=my_awesome_model\n</code></pre>"},{"location":"guide/external-models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/external-models/#model-not-found","title":"Model Not Found","text":"<pre><code>Error: No module named 'my_model'\n</code></pre> <p>Check:</p> <ul> <li>Model is listed in <code>xlm_models.json</code> (directory-based)</li> <li>Model is installed and listed in <code>XLM_MODELS_PACKAGES</code> (package-based)</li> <li>Directory structure is correct</li> <li><code>__init__.py</code> exists in the Python package</li> </ul>"},{"location":"guide/external-models/#duplicate-model-names","title":"Duplicate Model Names","text":"<pre><code>ExternalModelConflictError: Duplicate model name: my_model\n</code></pre> <p>Solution: Each model must have a unique name. Check for duplicate entries in <code>xlm_models.json</code> files or conflicts with core XLM models.</p>"},{"location":"guide/external-models/#config-not-found","title":"Config Not Found","text":"<pre><code>hydra.errors.ConfigCompositionException: Could not find 'model/my_model'\n</code></pre> <p>Check:</p> <ul> <li><code>configs/model/my_model.yaml</code> exists</li> <li><code>configs/model_type/my_model.yaml</code> exists</li> <li>Config files have valid YAML syntax</li> <li><code>_target_</code> paths point to correct classes</li> </ul>"},{"location":"guide/external-models/#examples","title":"Examples","text":"<p>See the core models in the <code>xlm-models</code> package for complete examples:</p> <ul> <li><code>arlm</code> - Auto-regressive language model</li> <li><code>mlm</code> - Masked language model</li> <li><code>ilm</code> - Infilling language model</li> <li><code>mdlm</code> - Masked diffusion language model</li> </ul>"},{"location":"guide/faq/","title":"FAQ","text":""},{"location":"guide/faq/#general","title":"General","text":"<p>What is XLM? XLM is a unified framework for developing and comparing non-autoregressive language models. It provides modular components for models, losses, predictors, and data collation.</p> <p>Which models are available? The framework includes MLM, ILM, ARLM, MDLM, and IDLM. See the Quick Start for the full list.</p>"},{"location":"guide/faq/#usage","title":"Usage","text":"<p>How do I train on a new dataset? Use the appropriate experiment config for your model and dataset. For example: <code>xlm job_type=train job_name=my_run experiment=lm1b_ilm</code>.</p> <p>How do I debug training? Add <code>debug=overfit</code> to overfit on a single batch, or use other debug configs from <code>configs/lightning_train/debug/</code>.</p>"},{"location":"guide/faq/#contributing","title":"Contributing","text":"<p>How do I add a new model? See the Contributing Guide for a complete walkthrough of the four components: Model, Loss, Predictor, and Collator.</p>"},{"location":"guide/generation/","title":"Generation","text":"<p>To perform unconditional generation, specify <code>job_type=generate</code>, <code>job_name</code>, and <code>experiment</code>. The example below shows how to generate from an ILM training checkpoint:</p> <pre><code>xlm \"job_type=generate\" \\\n  \"job_name=owt_ilm\" \\\n  \"experiment=owt_ilm\" \\\n  \"debug=[overfit,print_predictions]\" \\\n  \"+generation.ckpt_path=logs/owt_ilm5/checkpoints/40-422500.ckpt\" \\\n  \"datamodule.dataset_managers.predict.unconditional_prediction.num_examples=5\" \\\n  \"predictor.stopping_threshold=0.9\"\n</code></pre>"},{"location":"guide/generation/#demo","title":"Demo","text":"<pre><code>python src/xlm/commands/cli_demo.py \"job_type=demo\" \"job_name=owt_ilm_demo\" \"experiment=owt_ilm\" predictor.stopping_threshold=0.9 +global_flags.DEBUG_PRINT_PREDS=true +hub/checkpoint=ilm_owt\n</code></pre>"},{"location":"guide/quickstart/","title":"Quick Start","text":""},{"location":"guide/quickstart/#installation","title":"Installation","text":"<pre><code>pip install xlm-core\n</code></pre> <p>For existing model implementations, also install:</p> <pre><code>pip install xlm-models\n</code></pre>"},{"location":"guide/quickstart/#cli-usage","title":"CLI Usage","text":"<p>XLM uses a simple CLI with three main arguments:</p> <pre><code>xlm job_type=&lt;JOB&gt; job_name=&lt;NAME&gt; experiment=&lt;CONFIG&gt;\n</code></pre> Argument Description <code>job_type</code> One of <code>prepare_data</code>, <code>train</code>, <code>eval</code>, or <code>generate</code> <code>job_name</code> A descriptive name for your run <code>experiment</code> Path to your Hydra experiment config"},{"location":"guide/quickstart/#example-ilm-on-lm1b","title":"Example: ILM on LM1B","text":"<p>A complete workflow demonstrating the Insertion Language Model on the LM1B dataset:</p>"},{"location":"guide/quickstart/#1-prepare-data","title":"1. Prepare Data","text":"<pre><code>xlm job_type=prepare_data job_name=lm1b_prepare experiment=lm1b_ilm\n</code></pre>"},{"location":"guide/quickstart/#2-train","title":"2. Train","text":"<pre><code># Quick debug run (overfit a single batch)\nxlm job_type=train job_name=lm1b_ilm experiment=lm1b_ilm debug=overfit\n\n# Full training\nxlm job_type=train job_name=lm1b_ilm experiment=lm1b_ilm\n</code></pre>"},{"location":"guide/quickstart/#3-evaluate","title":"3. Evaluate","text":"<pre><code>xlm job_type=eval job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +eval.ckpt_path=&lt;CHECKPOINT_PATH&gt;\n</code></pre>"},{"location":"guide/quickstart/#4-generate","title":"4. Generate","text":"<pre><code>xlm job_type=generate job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +generation.ckpt_path=&lt;CHECKPOINT_PATH&gt;\n</code></pre> <p>Tip: Add <code>debug=[overfit,print_predictions]</code> to print generated samples to the console:</p> <pre><code>xlm job_type=generate job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +generation.ckpt_path=&lt;CHECKPOINT_PATH&gt; \\\n    debug=[overfit,print_predictions]\n</code></pre>"},{"location":"guide/quickstart/#5-push-to-hugging-face-hub","title":"5. Push to Hugging Face Hub","text":"<pre><code>xlm job_type=push_to_hub job_name=lm1b_ilm_hub experiment=lm1b_ilm \\\n    +hub_checkpoint_path=&lt;CHECKPOINT_PATH&gt; \\\n    +hub.repo_id=&lt;YOUR_REPO_ID&gt;\n</code></pre>"},{"location":"guide/releasing/","title":"Releasing to PyPI","text":"<p>This page describes how to publish xlm-core to PyPI.</p>"},{"location":"guide/releasing/#pre-release-step","title":"Pre-release Step","text":"<p>Update the version in <code>src/xlm/version.py</code>. The <code>setup.py</code> reads <code>VERSION</code> from this file via <code>exec()</code>. The publish workflow sets <code>XLM_CORE_VERSION_*</code> environment variables from the GitHub release tag, overriding <code>version.py</code> at build time. For consistency and local development (<code>import xlm; xlm.__version__</code>), update <code>version.py</code> (defaults or hardcoded values) to match the release version before creating the release.</p>"},{"location":"guide/releasing/#release-workflow","title":"Release Workflow","text":"<p>The .github/workflows/publish.yml workflow triggers on GitHub release creation (<code>release: types: [published]</code>).</p>"},{"location":"guide/releasing/#process","title":"Process","text":"<ol> <li>Update <code>src/xlm/version.py</code> to match the version you will tag (e.g., <code>0.1.2</code>).</li> <li>Commit and push the version change.</li> <li>Create a new release on GitHub (Releases \u2192 Draft a new release).</li> <li>Create a tag matching the version (e.g., <code>v0.1.2</code> or <code>0.1.2</code>).</li> <li>Publish the release. The workflow extracts the version from the tag, builds the package, and uploads to PyPI via Twine.</li> </ol>"},{"location":"guide/releasing/#required-secrets","title":"Required Secrets","text":"<p>Configure these in the repo Settings \u2192 Secrets and variables \u2192 Actions:</p> <ul> <li><code>PYPI_USERNAME</code> \u2013 PyPI username</li> <li><code>PYPI_TOKEN</code> \u2013 PyPI API token</li> </ul>"},{"location":"guide/releasing/#how-it-works","title":"How It Works","text":"<p>The workflow parses the release tag (stripping the <code>v</code> prefix) and passes <code>XLM_CORE_VERSION_MAJOR</code>, <code>XLM_CORE_VERSION_MINOR</code>, <code>XLM_CORE_VERSION_PATCH</code>, and <code>XLM_CORE_VERSION_SUFFIX</code> as environment variables to override <code>version.py</code> at build time.</p>"},{"location":"guide/training/","title":"Training","text":"<p>To train the model in interactive mode, specify <code>job_type=train</code>, <code>job_name</code>, and <code>experiment</code>. Example command to train the ILM model on OpenWebText:</p> <pre><code>xlm \"job_type=train\" \"job_name=owt_ilm\" \"experiment=owt_ilm\"\n</code></pre>"},{"location":"guide/training/#debugging","title":"Debugging","text":"<p>We have various debugging config overrides in the <code>configs/lightning_train/debug</code> directory. The most useful one is <code>debug=overfit</code>, which overfits the model on a single batch. To use it, add it to the command line arguments:</p> <pre><code>xlm \"job_type=train\" \"job_name=owt_ilm\" \"experiment=owt_ilm\" \"debug=overfit\"\n</code></pre> <p>You can create your own debug configs and use them. See the <code>configs/lightning_train/debug/</code> directory for examples.</p>"},{"location":"reference/api-models/","title":"API Reference (xlm-models)","text":""},{"location":"reference/api-models/#arlm","title":"ARLM","text":""},{"location":"reference/api-models/#arlm.model_arlm.RotaryTransformerARLMModel","title":"<code>RotaryTransformerARLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder for auto-regressive language modeling.</p>"},{"location":"reference/api-models/#arlm.model_arlm.RotaryTransformerARLMModel.__init__","title":"<code>__init__(num_embeddings, d_model, num_layers, nhead, padding_idx=0, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, rotary_emb_dim=64, max_length=1024, force_flash_attn=False, final_layer_without_normalization=False)</code>","text":"<p>Initialize the ARLM transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the vocabulary.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers.</p> required <code>nhead</code> <code>int</code> <p>Number of attention heads.</p> required <code>padding_idx</code> <code>int</code> <p>Index of the padding token.</p> <code>0</code> <code>dim_feedforward</code> <code>Optional[int]</code> <p>Dimension of the feedforward network.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function.</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>Epsilon for layer normalization.</p> <code>1e-05</code> <code>rotary_emb_dim</code> <code>int</code> <p>Dimension of rotary embeddings.</p> <code>64</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> <code>1024</code> <code>force_flash_attn</code> <code>bool</code> <p>Whether to force flash attention.</p> <code>False</code> <code>final_layer_without_normalization</code> <code>bool</code> <p>Whether to use final layer without normalization.</p> <code>False</code>"},{"location":"reference/api-models/#arlm.model_arlm.RotaryTransformerARLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Forward pass of the ARLM model.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len seq_len']]</code> <p>The attention mask of shape (batch, seq_len, seq_len) for full attention matrix,           or (batch, seq_len) for simple mask. True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The token type ids of shape (*batch, seq_len)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>vocab_logits</code> <code>Float[Tensor, ' *batch seq_len vocab_size']</code> <p>The vocabulary logits of shape (*batch, seq_len, vocab_size)</p>"},{"location":"reference/api-models/#arlm.model_arlm.RotaryTransformerARLMModel.get_named_params_for_no_weight_decay","title":"<code>get_named_params_for_no_weight_decay()</code>","text":"<p>Get parameters for no weight decay (biases and layer-norm parameters).</p>"},{"location":"reference/api-models/#arlm.model_arlm.RotaryTransformerARLMModel.get_named_params_for_weight_decay","title":"<code>get_named_params_for_weight_decay()</code>","text":"<p>Get parameters for weight decay (all parameters except biases and layer-norm parameters).</p>"},{"location":"reference/api-models/#arlm.loss_arlm.ARLMLoss","title":"<code>ARLMLoss</code>","text":"<p>               Bases: <code>LossFunction[ARLMBatch, ARLMLossDict]</code></p> <p>Loss function for Auto-Regressive Language Modeling (ARLM).</p> <p>This loss function implements causal language modeling where the model predicts the next token given the previous tokens. The loss is computed using cross-entropy on the target sequence (which is already shifted in the batch).</p> <p>For seq2seq tasks, loss is only computed on suffix tokens (non-prompt tokens).</p>"},{"location":"reference/api-models/#arlm.loss_arlm.ARLMLoss.__call__","title":"<code>__call__(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the loss for the given batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch containing input_ids, attention_mask, and target_ids.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/api-models/#arlm.loss_arlm.ARLMLoss.__init__","title":"<code>__init__(model=None, tokenizer=None)</code>","text":"<p>Initialize the ARLM loss function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer for processing tokens.</p> <code>None</code>"},{"location":"reference/api-models/#arlm.loss_arlm.ARLMLoss.configure","title":"<code>configure(pl_module)</code>","text":"<p>Configure the loss function with the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>Harness</code> <p>The lightning module instance.</p> required"},{"location":"reference/api-models/#arlm.loss_arlm.ARLMLoss.loss_fn","title":"<code>loss_fn(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the causal language modeling loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor","title":"<code>ARLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[Dict[str, Any], ARLMPredictionDict]</code></p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, sampling_method='sample', top=1000, p=0.9, model=None)</code>","text":"<p>Constructor for ARLMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer to use.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> <code>None</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>Sampling method to use.</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>Top-k parameter for sampling.</p> <code>1000</code> <code>p</code> <code>float</code> <p>Top-p parameter for sampling.</p> <code>0.9</code> <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Decode the predicted sequence.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ARLMStepResults</code> <p>Step results containing the predicted sequence.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str], Integer[Tensor, ' batch seq_len']]</code> <p>Tuple of (decoded_text, decoded_text_with_special_tokens, token_ids).</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.predict","title":"<code>predict(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None, max_len=0)</code>","text":"<p>Predict the complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch containing input_ids and attention_mask.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <code>max_len</code> <code>int</code> <p>Maximum length for prediction.</p> <code>0</code> <p>Returns:</p> Type Description <code>ARLMPredictionDict</code> <p>Prediction results containing text, token IDs, and attention mask.</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.predict_single_step","title":"<code>predict_single_step(step_results, current_step)</code>","text":"<p>Predict the next token in the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results containing x, attention_mask, and logits.</p> required <code>current_step</code> <code>int</code> <p>Current prediction step.</p> required <code>final_step</code> <p>Whether this is the final step.</p> required <p>Returns:</p> Type Description <code>ARLMStepResults</code> <p>Updated step results with the next token predicted.</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.stop","title":"<code>stop(step_results, current_length)</code>","text":"<p>Check if prediction should stop.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results.</p> required <code>current_length</code> <code>int</code> <p>Current sequence length.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if prediction should stop, False otherwise.</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMPredictor.to_dict","title":"<code>to_dict(batch, preds, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Convert predictions to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>preds</code> <code>ARLMPredictionDict</code> <p>Prediction results.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing prediction results.</p>"},{"location":"reference/api-models/#arlm.predictor_arlm.ARLMStepResults","title":"<code>ARLMStepResults</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Step results for ARLM prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Bool[TT, \" batch seq_len\"] Mask of the current sequence.</p> <code>logits</code> <code>Optional[Float[Tensor, ' batch seq_len vocab_size']]</code> <p>Float[TT, \" batch seq_len vocab_size\"] Logits of the current sequence.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMEmptyDataset","title":"<code>ARLMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples)</code>","text":"<p>Initialize the ARLM empty dataset.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>num_examples</code> <code>int</code> <p>Number of empty examples to generate.</p> required"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMEmptyDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Generate empty examples for ARLM training.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMSeq2SeqCollator","title":"<code>ARLMSeq2SeqCollator</code>","text":""},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMSeq2SeqCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMSeq2SeqCollator.__init__","title":"<code>__init__(tokenizer, noise_schedule, block_size=None, input_block_size=None, add_bos=None, add_eos=False, truncate='block')</code>","text":"<p>Initialize the ARLM sequence-to-sequence collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the target.</p> <code>None</code> <code>input_block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the input.</p> <code>None</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the suffix.</p> <code>False</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMSeq2SeqPredCollator","title":"<code>ARLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ARLMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.ARLMSeq2SeqPredCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence prediction.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.DefaultARLMCollator","title":"<code>DefaultARLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.DefaultARLMCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[BaseCollatorInput]</code> <p>List of examples with input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMBatch</code> <p>ARLMBatch with input_ids, attention_mask, and target_ids.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.DefaultARLMCollator.__init__","title":"<code>__init__(tokenizer, block_size, noise_schedule, truncate='block', add_eos=False)</code>","text":"<p>Initialize the ARLM collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>block_size</code> <code>int</code> <p>Maximum sequence length.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the sequence.</p> <code>False</code>"},{"location":"reference/api-models/#arlm.datamodule_arlm.prepare_prefix_ids_arlm","title":"<code>prepare_prefix_ids_arlm(prefix_ids, pad_token_id, bos_token_id=None, eos_token_id=None, max_seq_len=None, truncate='block', add_bos=None, add_eos=False)</code>","text":"<p>Prepare prefix ids for ARLM seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List of prefix token sequences.</p> required <code>pad_token_id</code> <code>int</code> <p>Padding token ID.</p> required <code>bos_token_id</code> <code>Optional[int]</code> <p>BOS token ID.</p> <code>None</code> <code>eos_token_id</code> <code>Optional[int]</code> <p>EOS token ID.</p> <code>None</code> <code>max_seq_len</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the prefix.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, List[List[int]]]</code> <p>Dictionary with input_ids and attention_mask as lists.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.prepare_suffix_ids_arlm","title":"<code>prepare_suffix_ids_arlm(suffix_ids, pad_token_id, bos_token_id=None, eos_token_id=None, max_seq_len=None, truncate='block', add_bos=None, add_eos=False)</code>","text":"<p>Prepare suffix ids for ARLM seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>suffix_ids</code> <code>List[List[int]]</code> <p>List of suffix token sequences.</p> required <code>pad_token_id</code> <code>int</code> <p>Padding token ID.</p> required <code>bos_token_id</code> <code>Optional[int]</code> <p>BOS token ID.</p> <code>None</code> <code>eos_token_id</code> <code>Optional[int]</code> <p>EOS token ID.</p> <code>None</code> <code>max_seq_len</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the suffix.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, List[List[int]]]</code> <p>Dictionary with input_ids, attention_mask, and target_ids as lists.</p>"},{"location":"reference/api-models/#arlm.datamodule_arlm.print_batch_arlm","title":"<code>print_batch_arlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging ARLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/api-models/#mlm","title":"MLM","text":""},{"location":"reference/api-models/#mlm.model_mlm.RotaryTransformerMLMModel","title":"<code>RotaryTransformerMLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/api-models/#mlm.model_mlm.RotaryTransformerMLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#mlm.predictor_mlm.MLMPredictor","title":"<code>MLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MLMBatch, MLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/api-models/#mlm.predictor_mlm.MLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None, skip_special_tokens=True)</code>","text":"<p>Initialize MLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MLMModel]</code> <p>The MLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/api-models/#mlm.predictor_mlm.MLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/api-models/#mlm.datamodule_mlm.DefaultMLMCollator","title":"<code>DefaultMLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/api-models/#mlm.datamodule_mlm.MLMInfillWithExactTargetPredCollator","title":"<code>MLMInfillWithExactTargetPredCollator</code>","text":"<p>               Bases: <code>DefaultMLMCollator</code></p> <p>Identical to DefaultMLMCollator but expects the prompt_ids to already contain masks.</p>"},{"location":"reference/api-models/#mlm.datamodule_mlm.MLMSeq2SeqCollator","title":"<code>MLMSeq2SeqCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position. TODO (efficiency): This is not ideal for seq2seq training as we will be wasting a lot of tokens in padding. For training, we should only pad on one side.</li> </ul>"},{"location":"reference/api-models/#mlm.datamodule_mlm.MLMSeq2SeqPredCollator","title":"<code>MLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>MLMSeq2SeqCollator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p>"},{"location":"reference/api-models/#mlm.datamodule_mlm.MLMSeq2SeqTrainCollator","title":"<code>MLMSeq2SeqTrainCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/api-models/#mlm.datamodule_mlm.prepare_prefix_ids","title":"<code>prepare_prefix_ids(prefix_ids, pad_token_id, max_seq_len=None, truncate='block')</code>","text":"<p>Prepare prefix ids for seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List[List[int]]</p> required <code>pad_token_id</code> <code>int</code> <p>int</p> required <code>max_seq_len</code> <code>Optional[int]</code> <p>Optional[int]</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <ul> <li>\"max\": Truncate to max(max_seq_len, max_in_batch).<ul> <li>when max_seq_len is not provided, it is the max in the batch.</li> </ul> </li> <li>\"block\": Pad-Truncate to max_seq_len.</li> <li>None: Pad to max in the batch.</li> </ul> <code>'block'</code> <p>Note: Prefixes if truncated will be truncated from the left. Returns:     Dict[str, TT]:         input_ids: Integer[TT, \" batch seq_len\"]         attention_mask: Integer[TT, \" batch seq_len\"]</p>"},{"location":"reference/api-models/#mlm.datamodule_mlm.prepare_prefix_suffix_ids","title":"<code>prepare_prefix_suffix_ids(prefix_ids, suffix_ids, pad_token_id, mask_token_id, eos_token_id=None, bos_token_id=None, max_seq_len=None, truncate='block', loss_on_padding=True)</code>","text":"<p>Prepare concatenated prefix and suffix ids for seq2seq tasks with padding on the right only</p> <p>Parameters:</p> Name Type Description Default <code>loss_on_padding</code> <code>bool</code> <p>bool - If true, pad token is treated as a normal token: it has attention on it, it is predicted as a target token. - If false, it has no attention on it, it is not predicted as a target token (-100)</p> <code>True</code>"},{"location":"reference/api-models/#mlm.datamodule_mlm.print_batch_mlm","title":"<code>print_batch_mlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging MLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/api-models/#ilm","title":"ILM","text":""},{"location":"reference/api-models/#ilm.model_ilm.BaseGPT2ILMModel","title":"<code>BaseGPT2ILMModel</code>","text":"<p>               Bases: <code>GPT</code>, <code>Model</code></p>"},{"location":"reference/api-models/#ilm.model_ilm.BaseGPT2ILMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#ilm.model_ilm.BaseRotaryTransformerILMModel","title":"<code>BaseRotaryTransformerILMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/api-models/#ilm.model_ilm.BaseRotaryTransformerILMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None, cls_position=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#ilm.model_ilm.GPT2ILMModelWithClassification","title":"<code>GPT2ILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseGPT2ILMModel</code></p>"},{"location":"reference/api-models/#ilm.model_ilm.GPT2ILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#ilm.model_ilm.RotaryTransformerILMModelWithClassification","title":"<code>RotaryTransformerILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseRotaryTransformerILMModel</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/api-models/#ilm.model_ilm.RotaryTransformerILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None, cls_position=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictor","title":"<code>ILMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, input_constraint=False)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictorUtilitiesMixin","title":"<code>ILMPredictorUtilitiesMixin</code>","text":""},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictorUtilitiesMixin.clean_up_pred_ids","title":"<code>clean_up_pred_ids(pred_ids, hold_mask=None)</code>","text":"<p>Remove mask tokens inserted due to batched prediction.</p>"},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictorWithLengthClassification","title":"<code>ILMPredictorWithLengthClassification</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/api-models/#ilm.predictor_ilm.ILMPredictorWithLengthClassification.__init__","title":"<code>__init__(max_steps, max_length, stopping_threshold=0.5, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, force_predict_first_step=False, input_constraint=False, use_high_precision=False, stopping_temperature=1.0)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> <code>0.5</code> <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/api-models/#ilm.datamodule_ilm.DefaultILMCollator","title":"<code>DefaultILMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/api-models/#ilm.datamodule_ilm.ILMEmptyDataset","title":"<code>ILMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/api-models/#ilm.datamodule_ilm.ILMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer_kwargs</code> <p>Keyword arguments for the tokenizer.</p> required <code>empty_text</code> <p>For MLM, you will want to set the <code>empty_text</code> to a sequence of all mask tokens.</p> required"},{"location":"reference/api-models/#ilm.datamodule_ilm.ILMSeq2SeqCollator","title":"<code>ILMSeq2SeqCollator</code>","text":"<p>Drops tokens from the suffix only.</p>"},{"location":"reference/api-models/#ilm.datamodule_ilm.ILMSeq2SeqPredCollator","title":"<code>ILMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ILMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/api-models/#ilm.datamodule_ilm.ilm_drop_fn","title":"<code>ilm_drop_fn(segment_input_ids, bos_token_id, cls_token_id, global_offset=0, sample_n_drops_fn=_n_drop_uniformly, drop_indices_fn=_drop_uniformly)</code>","text":"<p>Drops tokens from a single segment of a single sequence. Adds bos. Adds cls as requested.</p>"},{"location":"reference/api-models/#mdlm","title":"MDLM","text":""},{"location":"reference/api-models/#mdlm.model_mdlm.MDLMModel","title":"<code>MDLMModel</code>","text":"<p>               Bases: <code>BaseMDLMModel</code></p> <p>DDiT based transformer that represents time/noise using AdaLN and uses rotary positional embeddings.</p>"},{"location":"reference/api-models/#mdlm.model_mdlm.MDLMModel.forward","title":"<code>forward(x_t, noise, attention_mask=None, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>noise</code> <code>Float[Tensor, ' *batch']</code> <p>The noise of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/api-models/#mdlm.predictor_mdlm.MDLMPredictor","title":"<code>MDLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MDLMBatch, MDLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/api-models/#mdlm.predictor_mdlm.MDLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None)</code>","text":"<p>Initialize MDLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MDLMModel]</code> <p>The MDLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/api-models/#mdlm.predictor_mdlm.MDLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MDLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.DefaultMDLMCollator","title":"<code>DefaultMDLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MDLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.MDLMEmptyDataset","title":"<code>MDLMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.MDLMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples, max_length)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer_kwargs</code> <p>Keyword arguments for the tokenizer.</p> required <code>TODO</code> <p>Might want the option to add BOS.</p> required"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.MDLMSeq2SeqPredCollator","title":"<code>MDLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p> <p>How is this different from MDLMSeq2SeqTrainCollator?     -  MDLMSeq2SeqTrainCollator's input_ids contain the joined sequence and target_ids also contain the target for the whole sequence. But MDLMSeq2SeqPredCollator's input_ids contain only the prefix and target_ids contain only the suffix if present.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: Input contains only the prefix</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: Target contains only the suffix if present.</li> <li>noise_rate: Float[TT, \" batch\"]: The noise rate for the model.</li> <li>total_noise: Float[TT, \" batch\"]: The total noise for the model.</li> <li>t: Float[TT, \" batch\"]: The time step for the model.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position.</li> </ul>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.MDLMSeq2SeqTrainCollator","title":"<code>MDLMSeq2SeqTrainCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MDLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.prepare_prefix_ids","title":"<code>prepare_prefix_ids(prefix_ids, pad_token_id, max_seq_len=None, truncate='block')</code>","text":"<p>Prepare prefix ids for seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List[List[int]]</p> required <code>pad_token_id</code> <code>int</code> <p>int</p> required <code>max_seq_len</code> <code>Optional[int]</code> <p>Optional[int]</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <ul> <li>\"max\": Truncate to max(max_seq_len, max_in_batch).<ul> <li>when max_seq_len is not provided, it is the max in the batch.</li> </ul> </li> <li>\"block\": Pad-Truncate to max_seq_len.</li> <li>None: Pad to max in the batch.</li> </ul> <code>'block'</code> <p>Note: Prefixes if truncated will be truncated from the left. Returns:     Dict[str, TT]:         input_ids: Integer[TT, \" batch seq_len\"]         attention_mask: Integer[TT, \" batch seq_len\"]</p>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.prepare_prefix_suffix_ids","title":"<code>prepare_prefix_suffix_ids(prefix_ids, suffix_ids, noise_schedule, pad_token_id, mask_token_id, eos_token_id=None, bos_token_id=None, max_seq_len=None, truncate='block', loss_on_padding=True, bos_location='after_prefix')</code>","text":"<p>Prepare concatenated prefix and suffix ids for seq2seq tasks with padding on the right only</p> <p>Parameters:</p> Name Type Description Default <code>loss_on_padding</code> <code>bool</code> <p>bool - If true, pad token is treated as a normal token: it has attention on it, it is predicted as a target token. - If false, it has no attention on it, it is not predicted as a target token (-100)</p> <code>True</code>"},{"location":"reference/api-models/#mdlm.datamodule_mdlm.print_batch_mdlm","title":"<code>print_batch_mdlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging MLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/api/","title":"API Reference (xlm-core)","text":""},{"location":"reference/api/#core","title":"Core","text":""},{"location":"reference/api/#xlm.harness.Harness","title":"<code>Harness</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>PyTorchModelHubMixin</code></p> <p>Main module that provides the scaffolding for the codebase.</p>"},{"location":"reference/api/#xlm.harness.Harness.tokenizer","title":"<code>tokenizer</code>  <code>instance-attribute</code>","text":"<p>Task Metrics usually consist of two types of metrics:     1. diagnostic metrics: These are typically different for different models as well as different tasks.     2. reported metrics: These are the same for all the models but different for different tasks. What we want too do is avoid a full blown (task x model) setup whenever we can but provide it as a last resort. The best case scenario is complete decopling. This happens when all the models adhere to the same output signature. But this never works for diagnostic metrics. In some cases, different tasks can share base metrics of both types. In these cases, we can use inheritance to avoid some code duplication. We would still have (task x model) number of classes though.</p>"},{"location":"reference/api/#xlm.harness.Harness.__init__","title":"<code>__init__(cfg, tokenizer=None, datamodule=None, write_per_sample_metrics=False, **kwargs)</code>","text":"<p>Initialize the Harness module.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration dictionary.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Optional tokenizer instance.</p> <code>None</code> <code>datamodule</code> <code>Optional[BaseDataModule]</code> <p>Optional datamodule instance.</p> <code>None</code> <code>write_per_sample_metrics</code> <code>bool</code> <p>Whether to write per-sample metrics.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"reference/api/#xlm.harness.Harness.compute_loss","title":"<code>compute_loss(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Computes loss based on the dataloader name.</p> <p>For 'lm', the loss function is applied. For 'prediction', the predictor's predict_step is used.</p>"},{"location":"reference/api/#xlm.harness.Harness.compute_post_hoc_metrics","title":"<code>compute_post_hoc_metrics(split, dataloader_name, epoch, step, update_logged_predictions=True)</code>","text":"<p>Compute post-hoc metrics on logged predictions.</p> <p>Similar to compute_generative_perplexity, but for arbitrary post-hoc metrics. Loads predictions from jsonl, computes per-sample and global metrics, and logs aggregated results.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>train/val/test/predict</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader</p> required <code>epoch</code> <code>int</code> <p>Current epoch</p> required <code>step</code> <code>int</code> <p>Current step</p> required <code>update_logged_predictions</code> <code>bool</code> <p>If True, update predictions jsonl with per-sample metrics</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary of aggregated metrics, or None if no evaluator</p>"},{"location":"reference/api/#xlm.harness.Harness.create_lr_scheduler","title":"<code>create_lr_scheduler(optimizer, name, num_warmup_steps=None, fraction_warmup_steps=None, num_training_steps=None, interval='step', frequency=1, monitor='train_loss', strict=True, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a learning rate noise_schedule with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Huggingface name of the learning rate noise_schedule. https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_scheduler</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use with the noise_schedule</p> required <code>num_training_steps</code> <code>Optional[int]</code> <p>The total number of training steps.</p> <code>None</code> <code>num_warmup_steps</code> <code>Optional[int]</code> <p>The number of warmup steps.</p> <code>None</code> <code>fraction_warmup_steps</code> <code>Optional[float]</code> <p>The fraction of training steps to use for warmup.</p> <code>None</code> <code>interval</code> <code>Literal['step', 'epoch']</code> <p>The interval at which to update the learning rate.</p> <code>'step'</code> <code>frequency</code> <code>int</code> <p>The frequency of the learning rate updates.</p> <code>1</code> <code>monitor</code> <code>Optional[str]</code> <p>The metric to monitor for the learning rate noise_schedule.</p> <code>'train_loss'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly follow the learning rate schedule.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the learning rate noise_schedule.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LRSchedulerWithConfig</code> <code>LRSchedulerWithConfig</code> <p>The configured learning rate scheduler.</p>"},{"location":"reference/api/#xlm.harness.Harness.extract_model_weights","title":"<code>extract_model_weights()</code>","text":"<p>Extract current model state dict.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model state dict (self.model.state_dict())</p>"},{"location":"reference/api/#xlm.harness.Harness.from_checkpoint","title":"<code>from_checkpoint(checkpoint_path, cfg=None, tokenizer=None, datamodule=None, apply_ema=False, map_location='cpu', **kwargs)</code>  <code>classmethod</code>","text":"<p>Load Harness from Lightning checkpoint with optional EMA application.</p> <p>This is the ONLY method that can apply EMA weights, as it has direct access to the checkpoint file containing the EMA state.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Union[str, Path]</code> <p>Path to the Lightning checkpoint file</p> required <code>cfg</code> <code>Optional[DictConfig]</code> <p>Optional config to override checkpoint config</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Optional tokenizer instance</p> <code>None</code> <code>datamodule</code> <code>Optional[BaseDataModule]</code> <p>Optional datamodule instance</p> <code>None</code> <code>apply_ema</code> <code>bool</code> <p>Whether to apply EMA weights from checkpoint</p> <code>False</code> <code>map_location</code> <code>str</code> <p>Device to load checkpoint to</p> <code>'cpu'</code> <code>**kwargs</code> <p>Additional arguments for load_from_checkpoint</p> <code>{}</code> <p>Returns:</p> Type Description <code>Harness</code> <p>Harness instance with loaded weights (and EMA applied if requested)</p> Example"},{"location":"reference/api/#xlm.harness.Harness.from_checkpoint--load-with-ema-weights-applied","title":"Load with EMA weights applied","text":"<p>harness = Harness.from_checkpoint(     \"checkpoint.ckpt\",     apply_ema=True,     cfg=cfg,     tokenizer=tokenizer,     datamodule=datamodule )</p>"},{"location":"reference/api/#xlm.harness.Harness.load_model_from_hub","title":"<code>load_model_from_hub(repo_id, revision=None, cache_dir=None, force_download=False, token=None, strict=True, **kwargs)</code>","text":"<p>Download and load model weights from HuggingFace Hub into self.model.</p> <p>This method downloads the model weights from the hub and loads them into the existing model. It does NOT reconstruct a new Harness instance.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace Hub repository ID (e.g., \"username/model\")</p> required <code>revision</code> <code>Optional[str]</code> <p>Git revision (branch, tag, or commit)</p> <code>None</code> <code>cache_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory to cache downloaded files</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Force re-download even if cached</p> <code>False</code> <code>token</code> <code>Optional[Union[str, bool]]</code> <p>HuggingFace Hub token for private repos</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys match</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments for hf_hub_download</p> <code>{}</code>"},{"location":"reference/api/#xlm.harness.Harness.load_model_weights","title":"<code>load_model_weights(path, strict=True)</code>","text":"<p>Load model weights from local file into self.model.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the model weights file</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys match</p> <code>True</code>"},{"location":"reference/api/#xlm.harness.Harness.prepare_batch_for_prediction","title":"<code>prepare_batch_for_prediction(batch)</code>","text":"<p>We need this for some tasks even if we have task sepecific collator, mainly because we want to clone some elements of the batch useful for computing metrics. TODO: Get rid of this method by cloning in the collator itself.</p>"},{"location":"reference/api/#xlm.harness.Harness.save_model_weights","title":"<code>save_model_weights(path, overwrite=False)</code>","text":"<p>Save current model weights to local file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to save the model weights</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing file</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file exists and overwrite is False</p>"},{"location":"reference/api/#xlm.harness.Harness.setup_metrics","title":"<code>setup_metrics(cfg)</code>","text":"<p>Attache metrics as modules</p>"},{"location":"reference/api/#xlm.harness.Harness.setup_post_hoc_evaluator","title":"<code>setup_post_hoc_evaluator(cfg)</code>","text":"<p>Setup post-hoc evaluator. Can be use for tasks like molecule generation.</p> <p>The post-hoc evaluator computes metrics on logged predictions at epoch end, enabling global metric computation (e.g., diversity on full generated set).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration dictionary</p> required"},{"location":"reference/api/#xlm.harness.LRSchedulerWithConfig","title":"<code>LRSchedulerWithConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>We follow the same structure as the one in LightningModule. lr_scheduler_config = {             # REQUIRED: The lr_scheduler instance             \"scheduler\": lr_scheduler,             # The unit of the lr_scheduler's step size, could also be 'step'.             # 'epoch' updates the lr_scheduler on epoch end whereas 'step'             # updates it after a optimizer update.             \"interval\": \"epoch\",             # How many epochs/steps should pass between calls to             # <code>lr_scheduler.step()</code>. 1 corresponds to updating the learning             # rate after every epoch/step.             \"frequency\": 1,             # Metric to to monitor for schedulers like <code>ReduceLROnPlateau</code>             \"monitor\": \"val_loss\",             # If set to <code>True</code>, will enforce that the value specified 'monitor'             # is available when the lr_scheduler is updated, thus stopping             # training if not found. If set to <code>False</code>, it will only produce a warning             \"strict\": True,         }</p>"},{"location":"reference/api/#xlm.harness.Predictor","title":"<code>Predictor</code>","text":"<p>               Bases: <code>Generic[T_in, T_out_pred]</code>, <code>Protocol</code></p>"},{"location":"reference/api/#xlm.harness.Predictor.to_dict","title":"<code>to_dict(batch, preds, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Create json lines from the predictions batch.</p>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin","title":"<code>PredictorHistoryMixin</code>","text":"<p>Mixin class for adding history tracking to predictors.</p> <p>This mixin provides generic history tracking capabilities that can be used by any predictor that implements iterative generation. History is stored as a list of tuples: (decoded_text, confidence_score, step_number).</p> Usage <p>class MyPredictor(torch.nn.Module, PredictorHistoryMixin, Predictor[...]):     def init(self, ..., return_history: bool = False):         super().init()         self.init_history(return_history=return_history)         ...</p> <pre><code>def predict(self, batch):\n    history = self.create_history(batch_size)\n    for step in range(steps):\n        # ... generation logic ...\n        history = self.update_history_from_state(history, state, step)\n    return {\"text\": ..., \"history\": history}\n</code></pre>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin.create_history","title":"<code>create_history(batch_size)</code>","text":"<p>Create empty history for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of sequences in the batch.</p> required <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Empty history list for each batch element.</p>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin.format_history_for_output","title":"<code>format_history_for_output(history, round_precision=4)</code>","text":"<p>Format history for output in to_dict methods.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Raw history list.</p> required <code>round_precision</code> <code>int</code> <p>Number of decimal places to round confidence values.</p> <code>4</code> <p>Returns:</p> Type Description <code>List[List[List[Any]]]</code> <p>Formatted history with rounded confidence values.</p>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin.init_history","title":"<code>init_history(return_history=False, decode_fn=None)</code>","text":"<p>Initialize history tracking.</p> <p>Parameters:</p> Name Type Description Default <code>return_history</code> <code>bool</code> <p>Whether to track history during generation.</p> <code>False</code> <code>decode_fn</code> <code>Optional[Callable]</code> <p>Optional custom decode function. If None, will use self.decode.</p> <code>None</code>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin.update_history_explicit","title":"<code>update_history_explicit(history, texts, confidences, step, active_mask=None)</code>","text":"<p>Update history with explicit values.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Current history list.</p> required <code>texts</code> <code>List[str]</code> <p>Decoded text for each batch element.</p> required <code>confidences</code> <code>Union[List[float], Tensor]</code> <p>Confidence/score for each batch element.</p> required <code>step</code> <code>int</code> <p>Current step number.</p> required <code>active_mask</code> <code>Optional[Union[List[bool], Tensor]]</code> <p>Optional mask indicating which samples are still active.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Updated history.</p>"},{"location":"reference/api/#xlm.harness.PredictorHistoryMixin.update_history_from_state","title":"<code>update_history_from_state(history, state, step, confidence_key='confidence', active_mask_key=None)</code>","text":"<p>Update history from a state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Current history list.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the current state (must be decodable).</p> required <code>step</code> <code>int</code> <p>Current step number.</p> required <code>confidence_key</code> <code>str</code> <p>Key in state dict for confidence values (default: \"confidence\").</p> <code>'confidence'</code> <code>active_mask_key</code> <code>Optional[str]</code> <p>Optional key for mask indicating which samples are still active.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Updated history.</p>"},{"location":"reference/api/#xlm.datamodule.BaseBatch","title":"<code>BaseBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with the keys that are present in input batches for all models.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Can depend on the model type. For ILM and IDLM: 0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p>"},{"location":"reference/api/#xlm.datamodule.BaseCollatorInput","title":"<code>BaseCollatorInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with values that are lists of raw input_ids of variable length.</p> <p>This is the input to the collator for pre-training.</p> <p>The elements of the lists can be of different lengths.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>List[int]</code> <p>The input ids.</p>"},{"location":"reference/api/#xlm.datamodule.BaseDataModule","title":"<code>BaseDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Base class for all datamodules.</p>"},{"location":"reference/api/#xlm.datamodule.BaseDataModule.tokenizer","title":"<code>tokenizer</code>  <code>instance-attribute</code>","text":"<p>The tokenizer.[Required]</p>"},{"location":"reference/api/#xlm.datamodule.BaseDataModule.print_batch","title":"<code>print_batch(batch, split, dataloader_idx=None)</code>","text":"<p>Required to print train and validation batches at the beginning of the epoch.</p>"},{"location":"reference/api/#xlm.datamodule.DefaultCollator","title":"<code>DefaultCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Simply stacks the input_ids, attention_mask, and token_type_ids and returns a batch.</p>"},{"location":"reference/api/#xlm.datamodule.DefaultCollatorWithDynamicPadding","title":"<code>DefaultCollatorWithDynamicPadding</code>","text":"<p>               Bases: <code>DefaultCollatorWithPadding</code></p> <p>Like DefaultCollator, but pads to the max length in the batch.</p>"},{"location":"reference/api/#xlm.datamodule.DefaultCollatorWithPadding","title":"<code>DefaultCollatorWithPadding</code>","text":"<p>               Bases: <code>DefaultCollator</code></p> <p>Like DefaultCollator, but pads (truncates if needed) the input_ids, attention_mask, and token_type_ids to self.max_length.</p>"},{"location":"reference/api/#xlm.datamodule.DefaultEmptyDataset","title":"<code>DefaultEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/api/#xlm.datamodule.DefaultEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples, tokenizer_kwargs=None, empty_text='')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Keyword arguments for the tokenizer.</p> <code>None</code> <code>empty_text</code> <code>str</code> <p>For MLM, you will want to set the <code>empty_text</code> to a sequence of all mask tokens.</p> <code>''</code>"},{"location":"reference/api/#xlm.datamodule.Seq2SeqCollatorInput","title":"<code>Seq2SeqCollatorInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with values that are lists of raw input_ids, attention_mask, and token_type_ids.</p> <p>This is the input to the collator for pre-training.</p> <p>The elements of the lists can be of different lengths.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>List[int]</code> <p>The input ids.</p> <code>prompt_ids</code> <code>List[int]</code> <p>The target ids.</p>"},{"location":"reference/api/#xlm.datamodule.SimpleSpaceTokenizer","title":"<code>SimpleSpaceTokenizer</code>","text":"<p>               Bases: <code>PreTrainedTokenizer</code></p> <p>Splits on spaces</p>"},{"location":"reference/api/#xlm.datamodule.SimpleSpaceTokenizer.__init__","title":"<code>__init__(vocab, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Sequence[str]</code> <p>List of desired tokens. Following are list of all of the special tokens with their corresponding ids:     \"[PAD]\": 0,     \"[UNK]\": 1,     \"[MASK]\": 2,     \"[EOS]\": 3,     \"[BOS]\": 4, an id (starting at 5) will be assigned to each character.</p> required <code>model_max_length</code> <code>int</code> <p>Model maximum sequence length.</p> required"},{"location":"reference/api/#xlm.datamodule.TokenizerMixin","title":"<code>TokenizerMixin</code>","text":"<ol> <li>Adds a <code>full_vocab_size</code> property.</li> <li>provides a <code>post_creation</code> method that should be called after creating the tokenizer.   to ensure all the special tokens are present.</li> </ol>"},{"location":"reference/api/#xlm.datamodule.TokenizerMixin.post_creation","title":"<code>post_creation()</code>","text":"<p>Check the presence of the special tokens and update the post processor.</p>"},{"location":"reference/api/#xlm.datamodule.UnconditionalGenerationDatasetManager","title":"<code>UnconditionalGenerationDatasetManager</code>","text":"<p>This is used for unconditional generation, where we don't have any input text.</p>"},{"location":"reference/api/#xlm.datamodule.ids_to_example_fn","title":"<code>ids_to_example_fn(example, tokenizer, block_size=None)</code>","text":"<p>Convert raw token_ids to input_ids, attention_mask, and token_type_ids.</p> Does <ol> <li>Calls <code>tokenizer.build_inputs_with_special_tokens</code> and <code>tokenizer.create_token_type_ids_from_sequences</code>     to produce <code>input_ids</code> and <code>token_type_ids</code>.</li> <li>Creates an <code>attention_mask</code> of all ones.</li> </ol> Does not do <ol> <li>Padding/truncation.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>Dict[Literal['token_ids'], List[int]]</code> <p>A dictionary with a \"token_ids\" key, and value which is a list of token ids.</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>A tokenizer that implements <code>PretrainedTokenizerBase</code> interface. Specifically, it should have <code>build_inputs_with_special_tokens</code> and <code>create_token_type_ids_from_sequences</code> methods overridden if the default implementations are not correct.</p> required <code>block_size</code> <code>Optional[int]</code> <p>The block size to pad/truncate the input_ids to.</p> <code>None</code> <p>Returns:     A dictionary with \"input_ids\", \"attention_mask\", and \"token_type_ids\" keys.</p>"},{"location":"reference/api/#xlm.datamodule.pad_prefix_suffix","title":"<code>pad_prefix_suffix(tokenizer, examples, max_seq_len)</code>","text":"<p>[][ ]"},{"location":"reference/api/#xlm.datamodule.pad_prefix_suffix2","title":"<code>pad_prefix_suffix2(examples, max_seq_len, pad_token_id, bos_token_id, eos_token_id, prefix_type_extension=0, suffix_type_extension=2, return_tensors=True)</code>","text":"<p>[][ ]"},{"location":"reference/api/#xlm.metrics.ExactMatch","title":"<code>ExactMatch</code>","text":"<p>               Bases: <code>MeanMetric</code></p>"},{"location":"reference/api/#xlm.metrics.ExactMatch.update","title":"<code>update(pred, target, pred_length=None, target_length=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>predicted tokens</p> required <code>target</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>target tokens</p> required <code>pred_length</code> <code>Optional[Integer[Tensor, ' *batch']]</code> <p>length of the predicted tokens</p> <code>None</code> <code>target_length</code> <code>Optional[Integer[Tensor, ' *batch']]</code> <p>length of the target tokens</p> <code>None</code>"},{"location":"reference/api/#xlm.metrics.MetricWrapper","title":"<code>MetricWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unified metric wrapper that works with both Lightning trainer and Fabric.</p> <p>Sends the raw <code>batch</code> and <code>loss_dict</code> output to the <code>update_fn</code> which transforms it into a dict of kwargs for the <code>metric</code>. The <code>update_fn</code> can contain task specific and model specific logic.</p> <p>For Lightning: Use the <code>log</code> method to log metrics via LightningModule. For Fabric: Use <code>compute</code> and <code>get_log_dict</code> methods for manual logging.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.full_name","title":"<code>full_name</code>  <code>property</code>","text":"<p>Get the full metric name with prefix.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.compute","title":"<code>compute()</code>","text":"<p>Compute the current metric value. Useful for Fabric-based training.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.get_log_dict","title":"<code>get_log_dict()</code>","text":"<p>Get a dictionary with the metric name and computed value for logging. Useful for Fabric-based training.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.log","title":"<code>log(pl_module, batch, metrics)</code>","text":"<p>Log the metric using Lightning's logging mechanism.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.reset","title":"<code>reset()</code>","text":"<p>Reset the metric state.</p>"},{"location":"reference/api/#xlm.metrics.MetricWrapper.update","title":"<code>update(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update the metric with the current batch and loss_dict.</p>"},{"location":"reference/api/#xlm.metrics.TokenAccuracy","title":"<code>TokenAccuracy</code>","text":"<p>               Bases: <code>MeanMetric</code></p>"},{"location":"reference/api/#xlm.metrics.TokenAccuracy.update","title":"<code>update(pred, target, pred_mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>predicted tokens</p> required <code>target</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>target tokens</p> required <code>pred_mask</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>True for positions that predicted.</p> <code>None</code>"},{"location":"reference/api/#xlm.metrics.exact_match_update_fn","title":"<code>exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" *batch target_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/api/#xlm.metrics.seq2seq_exact_match_update_fn","title":"<code>seq2seq_exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required <p>Note: We rely on having same number right pads in target and pred, which may not be true for ARLM.</p>"},{"location":"reference/api/#xlm.metrics.seq2seq_token_accuracy_update_fn","title":"<code>seq2seq_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/api/#commands","title":"Commands","text":"<p>Script to scaffold a new external language model for the XLM framework.</p> <p>Usage: xlm-scaffold  [options] <p>This script creates a complete external model structure with: - Python package with skeleton implementations - Configuration files for all necessary components - Documentation and examples</p>"},{"location":"reference/api/#xlm.commands.lightning_generate.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"Supports two modes <ol> <li>Load a model from full training checkpoint using <code>lightning_module.load_from_checkpoint(cfg.generation.ckpt_path)</code></li> <li>Load a model from model only checkpoint using <code>lightning_module.model.load_state_dict(torch.load(cfg.generation.model_only_checkpoint_path))</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>generation_ckpt_path</code> <p>Path to the generation checkpoint</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule</p> required"},{"location":"reference/api/#xlm.commands.lightning_eval.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"<p>Instantiate and load a model for evaluation.</p> Supports two modes <ol> <li>Load a model from full training checkpoint using <code>lightning_module.load_from_checkpoint(cfg.eval.checkpoint_path)</code></li> <li>Load a model from model only checkpoint using <code>lightning_module.model.load_state_dict(torch.load(cfg.eval.model_only_checkpoint_path))</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer</p> required <p>Returns:</p> Type Description <code>Harness</code> <p>Tuple of (lightning_module, ckpt_path) where ckpt_path is the full checkpoint path</p> <code>Optional[str]</code> <p>(or None if using model-only checkpoint)</p>"},{"location":"reference/api/#xlm.commands.push_to_hub.PushToHubConfig","title":"<code>PushToHubConfig</code>  <code>dataclass</code>","text":"<p>Configuration for pushing model to Hugging Face Hub.</p>"},{"location":"reference/api/#xlm.commands.push_to_hub.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"<p>Instantiate a model from checkpoint or config.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule instance</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer instance</p> required <p>Returns:</p> Name Type Description <code>Harness</code> <code>Harness</code> <p>The instantiated model</p>"},{"location":"reference/api/#xlm.commands.push_to_hub.main","title":"<code>main(cfg)</code>","text":"<p>Main function for Push to Hub.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.create_template_context","title":"<code>create_template_context(model_name)</code>","text":"<p>Create template context with all necessary variables.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_config_files","title":"<code>generate_config_files(config_dir, context, is_core=False)</code>","text":"<p>Generate all configuration files.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_datamodule_file","title":"<code>generate_datamodule_file(model_dir, context)</code>","text":"<p>Generate the datamodule_.py file with data processing logic."},{"location":"reference/api/#xlm.commands.scaffold_model.generate_documentation","title":"<code>generate_documentation(model_dir, context)</code>","text":"<p>Generate README and documentation.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_init_file","title":"<code>generate_init_file(model_dir, context, is_core=False)</code>","text":"<p>Generate the init.py file.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_loss_file","title":"<code>generate_loss_file(model_dir, context)</code>","text":"<p>Generate the loss_{model_name}.py file with loss computation.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_metrics_file","title":"<code>generate_metrics_file(model_dir, context)</code>","text":"<p>Generate the metrics_{model_name}.py file with metric computation logic.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_model_file","title":"<code>generate_model_file(model_dir, context)</code>","text":"<p>Generate the model_{model_name}.py file with the neural network implementation.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_predictor_file","title":"<code>generate_predictor_file(model_dir, context)</code>","text":"<p>Generate the predictor_{model_name}.py file with inference logic.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_setup_file","title":"<code>generate_setup_file(model_dir, context)</code>","text":"<p>Generate setup.py for the external model.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.generate_types_file","title":"<code>generate_types_file(model_dir, context)</code>","text":"<p>Generate the types_{model_name}.py file with TypedDict definitions.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.update_xlm_models_file","title":"<code>update_xlm_models_file(model_name, xlm_models_path=Path('xlm_models.json'))</code>","text":"<p>Add the new model to the xlm_models.json file.</p>"},{"location":"reference/api/#xlm.commands.scaffold_model.validate_model_name","title":"<code>validate_model_name(name)</code>","text":"<p>Validate and normalize model name.</p>"},{"location":"reference/api/#modules","title":"Modules","text":"<p>Modules for simple transformer decoder that uses rotary embeddings for positional encoding.</p> <p>Adapted from MINGPT: https://github.com/karpathy/nanoGPT/blob/master/model.py</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayer","title":"<code>RotaryTransformerFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple unembedding layer with optional layer norm.</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayer.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayerForClassification","title":"<code>RotaryTransformerFinalLayerForClassification</code>","text":"<p>               Bases: <code>Module</code></p> <p>Feedforward layer with pre-norm and residual connection followed by a linear layer for classification.</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayerForClassification.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerLayer","title":"<code>RotaryTransformerLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with normalization and gating in between.</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerLayer.forward","title":"<code>forward(inp, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens. It can also be of shape (bsz, seq_len (query), seq_len (key-value)), where the mask indicates which tokens are valid in the context.</p> required"},{"location":"reference/api/#xlm.modules.rotary_transformer.RotaryTransformerLayerList","title":"<code>RotaryTransformerLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/api/#xlm.modules.rotary_transformer.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.gpt2_transformer.GPT","title":"<code>GPT</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.gpt2_transformer.GPT.estimate_mfu","title":"<code>estimate_mfu(fwdbwd_per_iter, dt)</code>","text":"<p>estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS</p>"},{"location":"reference/api/#xlm.modules.gpt2_transformer.GPT.get_num_params","title":"<code>get_num_params(non_embedding=True)</code>","text":"<p>Return the number of parameters in the model. For non-embedding count (default), the position embeddings get subtracted. The token embeddings would too, except due to the parameter sharing these params are actually used as weights in the final layer, so we include them.</p>"},{"location":"reference/api/#xlm.modules.gpt2_transformer.LayerNorm","title":"<code>LayerNorm</code>","text":"<p>               Bases: <code>Module</code></p> <p>LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False</p>"},{"location":"reference/api/#xlm.modules.encoder.DiffusionTransformerEncoder","title":"<code>DiffusionTransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.encoder.DiffusionTransformerEncoder.forward","title":"<code>forward(src_tokens, src_lengths)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src_tokens</code> <code>Tensor</code> <p>shape (B, T)</p> required <code>src_lengths</code> <code>Optional[Tensor]</code> <p>shape (B) of type LongTensor</p> required"},{"location":"reference/api/#xlm.modules.encoder.DiffusionTransformerEncoderLayer","title":"<code>DiffusionTransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.encoder.DiffusionTransformerEncoderLayer.forward","title":"<code>forward(src, src_mask=None, src_key_padding_mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src</code> <p>shape (B, T, d_model) for now we will assume T=query_seq_len=key_seq_len</p> required <code>src_mask</code> <p>shape (B*num_heads, T, T) or (T, T). True is attend, False is not attend Note that nn.TransformerEncoderLayer will allow float masks which are added to the attention scores. But we only support boolean masks here.</p> <code>None</code> <code>src_key_padding_mask</code> <p>shape (B, T) or (T). True is attend, False is masked</p> <code>None</code>"},{"location":"reference/api/#xlm.modules.ddit_simple.AdaLNModulations","title":"<code>AdaLNModulations</code>","text":"<p>               Bases: <code>Module</code></p> <p>Produces the modulation parameters for AdaLN.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.AdaLNModulations.__init__","title":"<code>__init__(cond_dim, dim, num_modulation_parameters=6)</code>","text":"<p>Initializes the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>cond_dim</code> <code>int</code> <p>The dimension of the conditioning input.</p> required <code>dim</code> <code>int</code> <p>The hidden size.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple.AdaLNModulations.ada_ln_modulate","title":"<code>ada_ln_modulate(x, shift, scale)</code>  <code>staticmethod</code>","text":"<p>Applies adaLN modulation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (bsz, seq_len, dim).</p> required <code>shift</code> <code>Tensor</code> <p>The shift parameter tensor of shape (bsz, 1, dim).</p> required <code>scale</code> <code>Tensor</code> <p>The scale parameter tensor of shape (bsz, 1, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The modulated output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.AdaLNModulations.forward","title":"<code>forward(c)</code>","text":"<p>Forward pass of the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The conditioning input tensor.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>Tuple[torch.Tensor]: The modulation parameters for AdaLN. Each tensor has shape (bsz, 1, dim). When num_modulation_paramters=6, these tensors stand for the shift and scale parameters for the MHA and MLP layers, and the gating parameters: shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDiTLayer","title":"<code>DDiTLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with adaLN and gating in between.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDiTLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, d_cond=None, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>d_cond</code> <code>Optional[int]</code> <p>the dimension of the conditioning input.</p> <code>None</code> <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDiTLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDiTLayer.forward","title":"<code>forward(x, c, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple.DDiTLayerList","title":"<code>DDiTLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDitFinalLayer","title":"<code>DDitFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.ddit_simple.DDitFinalLayer.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple.LabelEmbedder","title":"<code>LabelEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>cond_size</code> <code>int</code> <p>The size of the conditioning input.</p> required <code>label_dropout</code> <code>Optional[float]</code> <p>The dropout rate for class labels during training.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>embedding_table</code> <code>Embedding</code> <p>The embedding table for class labels.</p> <code>num_classes</code> <code>int</code> <p>The number of classes.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.LabelEmbedder.drop_labels","title":"<code>drop_labels(labels)</code>","text":"<p>Drop out class labels during training.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The modified class labels with some labels dropped by setting to the missing (last label).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.LabelEmbedder.forward","title":"<code>forward(labels)</code>","text":"<p>Forward pass of the LabelEmbedder module.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The embedded vector representations of the class labels.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.LayerNormAndScale","title":"<code>LayerNormAndScale</code>","text":"<p>               Bases: <code>Module</code></p> <p>Performs normalization and just scaling (no bias).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.LayerNormAndScale.__init__","title":"<code>__init__(dim, eps=1e-05)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple.LayerNormAndScale.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the normalized and scaled output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.TimestepEmbedder","title":"<code>TimestepEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.TimestepEmbedder.__init__","title":"<code>__init__(hidden_size, frequency_embedding_size=256, max_period=10000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer and the output of MLP.</p> required <code>frequency_embedding_size</code> <code>int</code> <p>The size of the frequency embedding layer.</p> <code>256</code>"},{"location":"reference/api/#xlm.modules.ddit_simple.TimestepEmbedder.forward","title":"<code>forward(t)</code>","text":"<p>Embeds scalar timesteps into vector representations.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>A 1-D Tensor of bsz indices, one per batch element. These may be fractional.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (bsz, hidden_size) Tensor of positional embeddings.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.AdaLNModulations","title":"<code>AdaLNModulations</code>","text":"<p>               Bases: <code>Module</code></p> <p>Produces the modulation parameters for AdaLN.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.AdaLNModulations.__init__","title":"<code>__init__(cond_dim, dim, num_modulation_parameters=6)</code>","text":"<p>Initializes the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>cond_dim</code> <code>int</code> <p>The dimension of the conditioning input.</p> required <code>dim</code> <code>int</code> <p>The hidden size.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.AdaLNModulations.ada_ln_modulate","title":"<code>ada_ln_modulate(x, shift, scale)</code>  <code>staticmethod</code>","text":"<p>Applies adaLN modulation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (bsz, seq_len, dim).</p> required <code>shift</code> <code>Tensor</code> <p>The shift parameter tensor of shape (bsz, 1, dim).</p> required <code>scale</code> <code>Tensor</code> <p>The scale parameter tensor of shape (bsz, 1, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The modulated output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.AdaLNModulations.forward","title":"<code>forward(c)</code>","text":"<p>Forward pass of the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The conditioning input tensor.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>Tuple[torch.Tensor]: The modulation parameters for AdaLN. Each tensor has shape (bsz, 1, dim). When num_modulation_paramters=6, these tensors stand for the shift and scale parameters for the MHA and MLP layers, and the gating parameters: shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDiTLayer","title":"<code>DDiTLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with adaLN and gating in between.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDiTLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, d_cond=None, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>d_cond</code> <code>Optional[int]</code> <p>the dimension of the conditioning input.</p> <code>None</code> <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDiTLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDiTLayer.forward","title":"<code>forward(x, c, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDiTLayerList","title":"<code>DDiTLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayer","title":"<code>DDitFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayer.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayerForClassification","title":"<code>DDitFinalLayerForClassification</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayerForClassification.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayerWithoutNormalization","title":"<code>DDitFinalLayerWithoutNormalization</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.DDitFinalLayerWithoutNormalization.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LabelEmbedder","title":"<code>LabelEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>cond_size</code> <code>int</code> <p>The size of the conditioning input.</p> required <code>label_dropout</code> <code>Optional[float]</code> <p>The dropout rate for class labels during training.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>embedding_table</code> <code>Embedding</code> <p>The embedding table for class labels.</p> <code>num_classes</code> <code>int</code> <p>The number of classes.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LabelEmbedder.drop_labels","title":"<code>drop_labels(labels)</code>","text":"<p>Drop out class labels during training.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The modified class labels with some labels dropped by setting to the missing (last label).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LabelEmbedder.forward","title":"<code>forward(labels)</code>","text":"<p>Forward pass of the LabelEmbedder module.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The embedded vector representations of the class labels.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LayerNormAndScale","title":"<code>LayerNormAndScale</code>","text":"<p>               Bases: <code>Module</code></p> <p>Performs normalization and just scaling (no bias).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LayerNormAndScale.__init__","title":"<code>__init__(dim, eps=1e-05)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.LayerNormAndScale.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the normalized and scaled output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.TimestepEmbedder","title":"<code>TimestepEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.TimestepEmbedder.__init__","title":"<code>__init__(hidden_size, frequency_embedding_size=256, max_period=10000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer and the output of MLP.</p> required <code>frequency_embedding_size</code> <code>int</code> <p>The size of the frequency embedding layer.</p> <code>256</code>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.TimestepEmbedder.forward","title":"<code>forward(t)</code>","text":"<p>Embeds scalar timesteps into vector representations.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>A 1-D Tensor of bsz indices, one per batch element. These may be fractional.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (bsz, hidden_size) Tensor of positional embeddings.</p>"},{"location":"reference/api/#xlm.modules.ddit_simple_v2.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/api/#xlm.modules.position.RotaryEmbedding","title":"<code>RotaryEmbedding</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/api/#xlm.modules.position.RotaryEmbedding.__init__","title":"<code>__init__(dim, head_first=True, cache_size=1024)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required <code>head_first</code> <code>bool</code> <p>if True, the input is assumed to be of shape (batch_size, seq_len, num_heads, dim)         if False, the input is assumed to be of shape (batch_size, num_heads, seq_len, dim)</p> <code>True</code> <code>cache_size</code> <code>int</code> <p>the maximum sequence length to cache the sine and cosine values for.</p> <code>1024</code>"},{"location":"reference/api/#xlm.modules.position.RotaryEmbedding.forward","title":"<code>forward(x, positions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>shape (batch_size, seq_len, num_heads, dim) if head_first is False shape (batch_size, num_heads, seq_len, dim) if head_first is True</p> required <code>positions</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>shape (batch_size, seq_len)</p> required"},{"location":"reference/api/#tasks","title":"Tasks","text":"<p>Molecule generation task utilities and metrics.</p> <p>This module provides: - Data preprocessing for SAFE molecular representations - Conversion utilities between SAFE and SMILES formats - Comprehensive metrics for evaluating molecular generation (diversity, QED, SA, validity, uniqueness)</p>"},{"location":"reference/api/#xlm.tasks.lm1b.detokenizer_lm1b_string","title":"<code>detokenizer_lm1b_string(x)</code>","text":"<p>Detokenizer for the LM1B dataset. Same as the one used in SEDD and MDLM.</p>"},{"location":"reference/api/#xlm.tasks.star.plot_graph","title":"<code>plot_graph(edge_list, path=None, source=None, goal=None)</code>","text":"<p>Example usage: graph = simple_star_graph(4, 4, 20) plot_graph(graph[\"edge_list\"], graph[\"path\"], graph[\"source\"], graph[\"goal\"])</p>"},{"location":"reference/api/#xlm.tasks.star.simple_star_graph","title":"<code>simple_star_graph(degree=3, pathlength=5, vocab_size=20)</code>","text":"<p>Generate a asymmetric star graph.</p>"},{"location":"reference/api/#xlm.tasks.star.simple_star_graph--adapted-from-httpsgithubcomgregorbachmannnext-token-failuresblobmaindatagraphspy","title":"Adapted from : https://github.com/gregorbachmann/Next-Token-Failures/blob/main/data/graphs.py","text":"Properties <ul> <li>The source is always the center of the star.</li> <li>The goal is always the end of one of the arms of the star.</li> <li>No nodes are repeated in the graph.     So pathlength * degree + 1  should be greater than or equal to vocab_size otherwise the     code will loop indefinitely.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>int, number of edges from source (center of the start) to other nodes</p> <code>3</code> <code>pathlength</code> <code>int</code> <p>int, length of the path (in terms of number of nodes)</p> <code>5</code> <code>vocab_size</code> <code>int</code> <p>int, number of nodes in the graph</p> <code>20</code> <p>Returns:     path: list, path from source to goal     edge_list: list, list of edges in the graph</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval","title":"<code>DeNovoEval</code>","text":"<p>Post-hoc evaluator for de novo molecule generation.</p> <p>Computes molecular properties on logged predictions at epoch end, matching GenMol's evaluation semantics. Computes: - Per-sample: QED, SA, SMILES (added to each prediction dict) - Global: Diversity, Validity, Uniqueness (aggregated across all samples)</p> <p>This approach enables: - Global metric computation (diversity/uniqueness on full generated set) - Exact match with GenMol's evaluation methodology - Reusable components for other tasks (frag, lead, pmo)</p> <p>Parameters:</p> Name Type Description Default <code>use_bracket_safe</code> <code>bool</code> <p>If True, decode from bracket SAFE format</p> <code>False</code> <code>compute_diversity</code> <code>bool</code> <p>If True, compute diversity metric</p> <code>True</code> <code>compute_validity</code> <code>bool</code> <p>If True, compute validity metric</p> <code>True</code> <code>compute_uniqueness</code> <code>bool</code> <p>If True, compute uniqueness metric</p> <code>True</code> <code>compute_qed</code> <code>bool</code> <p>If True, compute QED scores</p> <code>True</code> <code>compute_sa</code> <code>bool</code> <p>If True, compute SA scores</p> <code>True</code>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.evaluator_diversity","title":"<code>evaluator_diversity</code>  <code>property</code>","text":"<p>Lazy load diversity evaluator.</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.evaluator_uniqueness","title":"<code>evaluator_uniqueness</code>  <code>property</code>","text":"<p>Lazy load uniqueness evaluator.</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.evaluator_validity","title":"<code>evaluator_validity</code>  <code>property</code>","text":"<p>Lazy load validity evaluator.</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.oracle_qed","title":"<code>oracle_qed</code>  <code>property</code>","text":"<p>Lazy load QED oracle.</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.oracle_sa","title":"<code>oracle_sa</code>  <code>property</code>","text":"<p>Lazy load SA oracle.</p>"},{"location":"reference/api/#xlm.tasks.molgen.DeNovoEval.eval","title":"<code>eval(predictions, tokenizer=None)</code>","text":"<p>Evaluate predictions and return updated predictions + aggregated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dicts with 'text' field containing SAFE strings</p> required <code>tokenizer</code> <code>Any</code> <p>Optional tokenizer (not used for denovo, but kept for interface consistency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple of:</p> <code>Dict[str, Any]</code> <ul> <li>predictions: Updated list with per-sample metrics added (smiles, qed, sa)</li> </ul> <code>Tuple[List[Dict[str, Any]], Dict[str, Any]]</code> <ul> <li>aggregated_metrics: Dict of global metric values</li> </ul>"},{"location":"reference/api/#xlm.tasks.molgen.FragmentEval","title":"<code>FragmentEval</code>","text":"<p>               Bases: <code>DeNovoEval</code></p> <p>Post-hoc evaluator for fragment-constrained molecule generation.</p> <p>Extends DeNovoEval with fragment-specific metrics: - All de novo metrics (validity, uniqueness, quality, QED, SA, diversity) - Distance: Tanimoto distance between generated and target molecules</p> <p>Based on GenMol's fragment evaluation methodology. Computes: - Per-sample: QED, SA, SMILES, distance (if target available) - Global: Diversity, Validity, Uniqueness, Quality, Distance (mean)</p> <p>Parameters:</p> Name Type Description Default <code>use_bracket_safe</code> <code>bool</code> <p>If True, decode from bracket SAFE format</p> <code>False</code> <code>compute_diversity</code> <code>bool</code> <p>If True, compute diversity metric</p> <code>True</code> <code>compute_validity</code> <code>bool</code> <p>If True, compute validity metric</p> <code>True</code> <code>compute_uniqueness</code> <code>bool</code> <p>If True, compute uniqueness metric</p> <code>True</code> <code>compute_qed</code> <code>bool</code> <p>If True, compute QED scores</p> <code>True</code> <code>compute_sa</code> <code>bool</code> <p>If True, compute SA scores</p> <code>True</code> <code>compute_distance</code> <code>bool</code> <p>If True, compute Tanimoto distance to target</p> <code>True</code>"},{"location":"reference/api/#xlm.tasks.molgen.FragmentEval.eval","title":"<code>eval(predictions, tokenizer=None)</code>","text":"<p>Evaluate fragment generation predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dicts with: - 'text': Generated SAFE string (full molecule) - 'truth': Target SAFE string (full molecule, optional) - 'raw_input': Fragment prompt SAFE string (optional)</p> required <code>tokenizer</code> <code>Any</code> <p>Optional tokenizer (not used, kept for interface consistency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple of:</p> <code>Dict[str, Any]</code> <ul> <li>predictions: Updated list with per-sample metrics added</li> </ul> <code>Tuple[List[Dict[str, Any]], Dict[str, Any]]</code> <ul> <li>aggregated_metrics: Dict of global metric values</li> </ul>"},{"location":"reference/api/#xlm.tasks.molgen.SerializableSAFETokenizer","title":"<code>SerializableSAFETokenizer</code>","text":"<p>Wrapper around SAFE tokenizer that handles pickling/deepcopy.</p> <p>The underlying tokenizer from the safe library has a custom PreTokenizer that cannot be serialized. This wrapper provides dummy serialization by storing the model path and re-instantiating the tokenizer on unpickle.</p>"},{"location":"reference/api/#xlm.tasks.molgen.SerializableSAFETokenizer.__dir__","title":"<code>__dir__()</code>","text":"<p>Include both wrapper and tokenizer attributes in dir().</p>"},{"location":"reference/api/#xlm.tasks.molgen.SerializableSAFETokenizer.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate all attribute access to the underlying tokenizer.</p>"},{"location":"reference/api/#xlm.tasks.molgen.SerializableSAFETokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return state for pickling - only store the model path.</p>"},{"location":"reference/api/#xlm.tasks.molgen.SerializableSAFETokenizer.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickled state - re-instantiate tokenizer.</p>"},{"location":"reference/api/#xlm.tasks.molgen.bracketsafe2safe","title":"<code>bracketsafe2safe(safe_str)</code>","text":"<p>Convert bracket SAFE notation back to standard SAFE format.</p> <p>Removes angle brackets from interfragment attachment points and renumbers them to avoid conflicts with intrafragment attachment points.</p> <p>Based on genmol/src/genmol/utils/bracket_safe_converter.py:140-153</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE string in bracket notation</p> required <p>Returns:</p> Type Description <code>str</code> <p>SAFE string in standard notation</p>"},{"location":"reference/api/#xlm.tasks.molgen.genmol_fragment_preprocess_fn","title":"<code>genmol_fragment_preprocess_fn(example, tokenizer, *, fragment_column='linker_design')</code>","text":"<p>Preprocess GenMol fragment CSV data for fragment-constrained generation.</p> <p>Converts SMILES fragments and targets to SAFE format, then to bracket SAFE, and creates prompt_token_ids (fragment) and input_token_ids (full molecule).</p> <p>Based on GenMol's fragment evaluation dataset structure: - Input: Fragment SMILES (from <code>fragment_column</code>, default 'linker_design') - Target: Full molecule SMILES (from 'smiles' column)</p> <p>To use a different fragment column, pass <code>fragment_column</code> via <code>preprocess_function_kwargs</code> in the dataset config, or set <code>_fragment_column</code> in the example dict (overrides kwarg).</p> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>Dict[str, Any]</code> <p>Dataset example containing: - column named by <code>fragment_column</code>: SMILES with [n*] attachment points - 'smiles': Full target molecule SMILES - '_fragment_column' (optional): Overrides <code>fragment_column</code> if set</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer for encoding</p> required <code>fragment_column</code> <code>str</code> <p>CSV column to use as fragment input (default: 'linker_design'). Override via datamodule ... preprocess_function_kwargs.fragment_column.</p> <code>'linker_design'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Example with 'prompt_token_ids' (fragment) and 'input_token_ids' (full molecule)</p>"},{"location":"reference/api/#xlm.tasks.molgen.safe2bracketsafe","title":"<code>safe2bracketsafe(safe_str)</code>","text":"<p>Convert standard SAFE notation to bracket SAFE format.</p> <p>Bracket SAFE wraps interfragment attachment points in angle brackets. Example: \"1\" -&gt; \"&lt;1&gt;\", \"%10\" -&gt; \"&lt;%10&gt;\"</p> <p>Based on genmol/src/genmol/utils/bracket_safe_converter.py:133-137</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE string in standard notation</p> required <p>Returns:</p> Type Description <code>str</code> <p>SAFE string in bracket notation, or original string if conversion fails</p>"},{"location":"reference/api/#xlm.tasks.molgen.safe_bracket_on_the_fly_processor_combined","title":"<code>safe_bracket_on_the_fly_processor_combined(example, tokenizer, block_size=None)</code>","text":"<p>Works directly on the raw strings</p>"},{"location":"reference/api/#xlm.tasks.molgen.safe_strings_to_smiles","title":"<code>safe_strings_to_smiles(safe_strings, use_bracket_safe=False, fix=True)</code>","text":"<p>Convert batch of SAFE strings to SMILES strings.</p> <p>Based on genmol/src/genmol/sampler.py:81-89</p> <p>Parameters:</p> Name Type Description Default <code>safe_strings</code> <code>List[str]</code> <p>List of SAFE molecular representations</p> required <code>use_bracket_safe</code> <code>bool</code> <p>If True, convert from bracket SAFE first</p> <code>False</code> <code>fix</code> <code>bool</code> <p>If True, filter invalid fragments</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of SMILES strings (invalid conversions are skipped)</p>"},{"location":"reference/api/#xlm.tasks.molgen.safe_to_smiles","title":"<code>safe_to_smiles(safe_str, fix=True)</code>","text":"<p>Convert SAFE string to SMILES using safe library.</p> <p>Based on genmol/src/genmol/utils/utils_chem.py:26-30</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE molecular representation</p> required <code>fix</code> <code>bool</code> <p>If True, filter out invalid fragments before decoding</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>SMILES string, or None if conversion fails</p>"},{"location":"reference/api/#utils","title":"Utils","text":""},{"location":"reference/api/#xlm.utils.nn.add_exp_1_noise","title":"<code>add_exp_1_noise(probs, temperature=1.0)</code>","text":"<p>Sample from unnormalized probability using the exponential race method. Similar to using gumbel noise, trick but we require the probs to be positive (can be unnormalized). You can generate samples without repeatations from the output by taking argmax or topk, etc. Args:     probs: (batch, seq_len, vocab_size) can have any number of leading dimensions. Returns:     (batch, seq_len)</p>"},{"location":"reference/api/#xlm.utils.nn.add_gumbel_noise","title":"<code>add_gumbel_noise(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Add gumbel noise to logits which will result in samples from the distribtution if argmaxed. Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions. Assumed to be log of exponentiated scores.         That is, we assume logits are $l_i$ in $p_i = exp(l_i) / \\sum_i \\exp(l_i)$. Returns:     (batch, seq_len)</p>"},{"location":"reference/api/#xlm.utils.nn.dtype","title":"<code>dtype(string)</code>","text":"<p>Convert a string to a PyTorch data type.</p>"},{"location":"reference/api/#xlm.utils.nn.dtype--parameters","title":"Parameters","text":"<code>str</code> <p>The string to convert.</p>"},{"location":"reference/api/#xlm.utils.nn.dtype--returns","title":"Returns","text":"<p><code>torch.dtype</code>     The PyTorch data type.</p>"},{"location":"reference/api/#xlm.utils.nn.get_mask_from_sequence_lengths","title":"<code>get_mask_from_sequence_lengths(sequence_lengths, max_length)</code>","text":"<p>Given a variable of shape <code>(batch_size,)</code> that represents the sequence lengths of each batch element, this function returns a <code>(batch_size, max_length)</code> mask variable.  For example, if our input was <code>[2, 2, 3]</code>, with a <code>max_length</code> of 4, we'd return <code>[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]]</code>.</p> <p>We require <code>max_length</code> here instead of just computing it from the input <code>sequence_lengths</code> because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor.</p>"},{"location":"reference/api/#xlm.utils.nn.masked_mean","title":"<code>masked_mean(vector, mask, dim, keepdim=False)</code>","text":"<p>To calculate mean along certain dimensions on masked values</p>"},{"location":"reference/api/#xlm.utils.nn.masked_mean--parameters","title":"Parameters","text":"<code>torch.Tensor</code> <p>The vector to calculate mean.</p> <p>mask : <code>torch.BoolTensor</code>     The mask of the vector. It must be broadcastable with vector.     It must be 1 for non-masked values and 0 for masked values. dim : <code>int</code>     The dimension to calculate mean keepdim : <code>bool</code>     Whether to keep dimension</p>"},{"location":"reference/api/#xlm.utils.nn.masked_mean--returns","title":"Returns","text":"<p><code>torch.Tensor</code>     A <code>torch.Tensor</code> of including the mean values.</p>"},{"location":"reference/api/#xlm.utils.nn.masked_sum","title":"<code>masked_sum(vector, mask, dim, keepdim=False)</code>","text":"<p>To calculate sum along certain dimensions on masked values</p>"},{"location":"reference/api/#xlm.utils.nn.masked_sum--parameters","title":"Parameters","text":"<code>torch.Tensor</code> <p>The vector to calculate sum.</p> <p>mask : <code>torch.BoolTensor</code>     The mask of the vector. It must be broadcastable with vector. It must be 1 for non-masked values and 0 for masked out values. dim : <code>int</code>     The dimension to calculate sum keepdim : <code>bool</code>     Whether to keep dimension</p>"},{"location":"reference/api/#xlm.utils.nn.masked_sum--returns","title":"Returns","text":"<p><code>torch.Tensor</code>     A <code>torch.Tensor</code> of including the sum values.</p>"},{"location":"reference/api/#xlm.utils.nn.sample_categorical","title":"<code>sample_categorical(probs)</code>","text":"<p>Need this since torch.multinomial does not accept 3D input and cannot handle unnormalized probabilities.</p> <p>So we implement the \"exponential race method\" manually which can handle any number of leading dimensions and can handle unnormalized probabilities (not logits, )</p> <p>Note: This is not differentiable.. Use gumbel softmax for it. Args:     probs: (batch, seq_len, vocab_size) can have any number of leading dimensions.         Note: probs should be positive, can be unnormalized. Returns:     (batch, seq_len)</p>"},{"location":"reference/api/#xlm.utils.nn.sample_from_logits","title":"<code>sample_from_logits(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Sample from logits using the Gumbel-Max trick. Similar to sample_categorical, but works with logits (real valued). Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions. Returns:     (batch, seq_len)</p>"},{"location":"reference/api/#xlm.utils.nn.sample_from_top_k","title":"<code>sample_from_top_k(k, logits)</code>","text":"<p>Sample from the top-k logits using the Gumbel-Max trick. Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions.     k: The number of top logits to consider for sampling. Returns:     (batch, seq_len)</p>"},{"location":"reference/api/#xlm.utils.nn.sample_from_top_p","title":"<code>sample_from_top_p(p, logits)</code>","text":"<p>Sample from the top-p logits using the Gumbel-Max trick.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>The cumulative probability threshold. Must be between 0 and 1.</p> required <code>logits</code> <code>Tensor</code> <p>A tensor of shape (*batch, seq_len, vocab_size) representing                    the unnormalized log probabilities for each token.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of shape (*batch, seq_len) containing the sampled token indices.</p>"},{"location":"reference/api/#xlm.utils.nn.select_random_indices","title":"<code>select_random_indices(inp_shape, num_unmask, select_from_mask=None, selection_score=None, temperature=1.0, selection_mode='greedy', score_mode='logits')</code>","text":"<p>Select random indices from the last dimension using the selection_score. 1. If selection score is None then it is assumed to be uniform. 2. If score_mode = logits and selection_mode=sample, then temperature can be used to control the temperature of the distribution. 3. If select_from_mask is provided, indices only from these positions are sampled. Args:     inp_shape: torch.Size, typeically (batch, d)     num_unmask: (batch,) int tensor     select_from_mask: (batch, d) tensor, if provided, we only sample from the selected     selection_score: logit-like score for selection (can be negative). Should match inp_shape, so typically (batch, d)     score_mode:         \"logits\" =&gt; p_i = \\exp(s_i)/\\sum_j \\exp(s_j)         \"uprobs\" =&gt; p_i = s_i / \\sum_j s_j</p>"},{"location":"reference/api/#xlm.utils.nn.tiny_value_of_dtype","title":"<code>tiny_value_of_dtype(dtype)</code>","text":"<p>Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from <code>info_value_of_dtype(dtype).tiny</code> because it causes some NaN bugs. Only supports floating point dtypes.</p>"},{"location":"reference/api/#xlm.utils.ema.EMACallback","title":"<code>EMACallback</code>","text":"<p>               Bases: <code>Callback</code></p>"},{"location":"reference/api/#xlm.utils.ema.EMACallback.__init__","title":"<code>__init__(decay, use_num_updates=True, apply_ema_at_train_end=True)</code>","text":"<p>decay: The exponential decay. use_num_updates: Whether to use number of updates when computing     averages. apply_ema_at_train_end: If True, applies EMA weights to the model     at the end of training, so the final checkpoint contains     EMA-averaged weights in the model parameters.</p>"},{"location":"reference/api/#xlm.utils.ema.EMACallback.on_train_end","title":"<code>on_train_end(trainer, pl_module)</code>","text":"<p>Apply EMA weights to the model at the end of training.</p> <p>This makes the final checkpoint contain EMA-averaged weights directly in the model parameters, eliminating the need for manual EMA application during checkpoint extraction or inference.</p> <p>Note: After this is called, the model's parameters will contain EMA-averaged weights, and the original training weights will be lost (though they're still stored in collected_params if store() was called).</p>"}]}