{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"A Unified Framework for Non-Autoregressive Language Models <p>XLM is a modular, research-friendly framework for developing and comparing non-autoregressive language models. Built on PyTorch and PyTorch Lightning, with Hydra for configuration management, XLM makes it effortless to experiment with cutting-edge NAR architectures.</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description Modular Design Plug-and-play components\u2014swap models, losses, predictors, and collators independently Lightning-Powered Uses PyTorch Lightning for distributed training, mixed precision, and logging out of the box Hydra Configs Hierarchical configuration with runtime overrides\u2014no code changes needed Multiple Architectures Multiple model families ready to use as baselines Research-First Lightweight, and type annotated with <code>jaxtyping</code>, several debug for quick testing, and flexible code injection points for practially limitless customization Hub Integration Push trained models directly to Hugging Face Hub"},{"location":"#available-models","title":"Available Models","text":"Model Full Name Description <code>mlm</code> Masked Language Model Classic BERT-style masked prediction <code>ilm</code> Insertion Language Model Insertion-based generation <code>arlm</code> Autoregressive LM Standard left-to-right baseline <code>mdlm</code> Masked Diffusion LM Discrete diffusion with masking"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install xlm-core\n</code></pre> <p>For model implementations, also install:</p> <pre><code>pip install xlm-models\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>XLM uses a simple CLI with three main arguments:</p> <pre><code>xlm job_type=&lt;JOB&gt; job_name=&lt;NAME&gt; experiment=&lt;CONFIG&gt;\n</code></pre> Argument Description <code>job_type</code> One of <code>prepare_data</code>, <code>train</code>, <code>eval</code>, or <code>generate</code> <code>job_name</code> A descriptive name for your run <code>experiment</code> Path to your Hydra experiment config"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2013 Installation, CLI usage, and example workflow</li> <li>API Reference \u2013 xlm-core and xlm-models API documentation</li> <li>Contributing \u2013 Guidelines for adding new models and features</li> </ul>"},{"location":"TODOs/","title":"TODOs","text":"<ol> <li>How to add new commands?</li> <li>Separate page containing a complete list of all env vars and their uses.</li> </ol>"},{"location":"guide/contributing/","title":"Contributing","text":""},{"location":"guide/custom-commands/","title":"Custom Commands","text":"<p>Models can define custom commands that extend XLM's CLI by creating <code>configs/commands.yaml</code>:</p> <pre><code># configs/commands.yaml\nmy_custom_command: \"my_awesome_model.commands.my_function\"\npreprocess_data: \"my_awesome_model.commands.preprocess\"\n</code></pre> <p>Usage:</p> <pre><code>xlm command=my_custom_command arg1=value1 arg2=value2\n</code></pre> <p>The command functions should accept an <code>omegaconf.DictConfig</code> parameter containing the Hydra configuration.</p>"},{"location":"guide/data-pipeline/","title":"Data Pipeline","text":"<p><code>TextDataModule</code> is a Lightning <code>DataModule</code> that orchestrates the full data lifecycle -- download, preprocessing, caching, creation of dataloaders. It holds one or more <code>DatasetManager</code> instances per split (train, val, test, predict). Each manager is registered under a dataloader name (e.g. <code>\"lm\"</code>, <code>\"prediction\"</code>), and that same name is used by the <code>Harness</code> to look up the metric group that should be evaluated on the corresponding dataloader. Understanding this naming link is the single most important thing for working with the pipeline.</p>"},{"location":"guide/data-pipeline/#architecture-overview","title":"Architecture Overview","text":"<p>The following diagram shows a typical setup.  Each box on the left is a <code>DatasetManager</code> registered under a dataloader name inside <code>TextDataModule</code>. The arrows show how that same name is used by the <code>Harness</code> to find the matching metric group on the right.</p> <pre><code>flowchart TB\n    subgraph TDM [TextDataModule]\n        direction TB\n        train[\"train: #quot;lm#quot; -&gt; DatasetManager\"]\n        val_lm[\"val: #quot;lm#quot; -&gt; DatasetManager\"]\n        val_pred[\"val: #quot;prediction#quot; -&gt; DatasetManager\"]\n        test_lm[\"test: #quot;lm#quot; -&gt; DatasetManager\"]\n        test_pred[\"test: #quot;prediction#quot; -&gt; DatasetManager\"]\n        predict[\"predict: #quot;unconditional#quot; -&gt; UnconditionalGenerationDM\"]\n    end\n\n    subgraph H [Harness]\n        direction TB\n        rm_train[\"reported_metrics.train.lm\"]\n        rm_val_lm[\"reported_metrics.val.lm\"]\n        rm_val_pred[\"reported_metrics.val.prediction\"]\n        rm_test_lm[\"reported_metrics.test.lm\"]\n        rm_test_pred[\"reported_metrics.test.prediction\"]\n    end\n\n    train -- \"dataloader_name = #quot;lm#quot;\" --&gt; rm_train\n    val_lm -- \"dataloader_name = #quot;lm#quot;\" --&gt; rm_val_lm\n    val_pred -- \"dataloader_name = #quot;prediction#quot;\" --&gt; rm_val_pred\n    test_lm -- \"dataloader_name = #quot;lm#quot;\" --&gt; rm_test_lm\n    test_pred -- \"dataloader_name = #quot;prediction#quot;\" --&gt; rm_test_pred</code></pre> <p>Internally, <code>TextDataModule</code> stores its managers as a nested dictionary:</p> <pre><code>dataset_managers: Dict[\n    Literal[\"train\", \"val\", \"test\", \"predict\"],\n    Dict[str, DatasetManager],  # key = dataloader name\n]\n</code></pre> <p>The train split must contain exactly one <code>DatasetManager</code>; val / test / predict can each contain zero or more, yielding a list of dataloaders.</p> <p>On construction, <code>TextDataModule</code> assigns each dataloader name a numeric index (the iteration order).  During a training or evaluation step the <code>Harness</code> receives a <code>dataloader_idx</code> from the Lightning Trainer and resolves it back to the dataloader name to find the right metrics:</p> <pre><code>dataloader_idx  (from Lightning Trainer)\n      |\n      v\ndataloader_names[stage][idx]   --&gt;  dataloader_name  (e.g. \"lm\")\n      |\n      v\nreported_metrics[\"metrics_{stage}\"][dataloader_name]  --&gt;  list of MetricWrapper\n</code></pre> <p>Because the same name appears in both the datamodule config and the metrics config, adding a new evaluation dataset with its own metrics is just a matter of registering a new <code>DatasetManager</code> under a new name and adding a matching entry in the metrics config.  For details on how metrics themselves are structured, see the Metrics guide.</p> <p>The <code>TextDataModule</code> lifecycle follows the standard Lightning pattern:</p> <ol> <li><code>prepare_data()</code> -- rank 0 only; delegates to each manager's    <code>prepare_data()</code> (download + preprocess + cache).</li> <li><code>setup(stage)</code> -- all ranks; delegates to each manager's <code>setup()</code>    (load from cache, apply processors, DDP splitting).</li> <li><code>{train,val,test,predict}_dataloader()</code> -- returns the configured    <code>DataLoader</code>(s).</li> </ol> <p>The following config example shows how the pieces fit together in practice. Real configs live under <code>configs/lightning_train/datamodule/</code> and <code>configs/lightning_train/model_type/</code>.</p> <p>Datamodule config (<code>datamodule/lm1b.yaml</code>, simplified):</p> <pre><code>datamodule:\n  _target_: xlm.datamodule.TextDataModule\n  dataset_managers:\n    train: # &lt;-- split name\n      lm:                            # &lt;-- dataloader name\n        _target_: xlm.datamodule.DatasetManager\n        full_name: billion-word-benchmark/lm1b/train\n        preprocess_function: xlm.tasks.lm1b.preprocess_fn\n        on_the_fly_processor: xlm.datamodule.token_ids_to_input_ids\n        columns_to_remove: [text]\n        stages: [fit]\n        # ...\n    val: # &lt;-- split name\n      lm:                            # &lt;-- same dataloader name\n        _target_: xlm.datamodule.DatasetManager\n        full_name: billion-word-benchmark/lm1b/test\n        stages: [fit, validate]\n        # ...\n      unconditional_prediction:      # &lt;-- another dataloader name\n        _target_: xlm.datamodule.UnconditionalGenerationDatasetManager\n        # ...\n</code></pre> <p>Metrics config (<code>model_type/mdlm.yaml</code>, simplified):</p> <pre><code>reported_metrics:\n  train:\n    lm:                              # &lt;-- must match dataloader name above\n      accumulated_loss:\n        prefix: train/lm\n        update_fn: mdlm.metrics_mdlm.mean_metric_update_fn\n  val:\n    lm:                              # &lt;-- must match\n      accumulated_loss:\n        prefix: val/lm\n        update_fn: mdlm.metrics_mdlm.mean_metric_update_fn\n</code></pre> <p>The names <code>lm</code> and <code>unconditional_prediction</code> are the glue: they appear identically in the datamodule and metrics configs.</p>"},{"location":"guide/data-pipeline/#datasetmanager-lifecycle","title":"DatasetManager Lifecycle","text":"<p>Each <code>DatasetManager</code> owns a single dataset and drives it through a multi-stage pipeline:</p> <pre><code>flowchart LR\n    subgraph prepare_data [\"prepare_data() -- rank 0 only\"]\n        direction LR\n        A[Download] --&gt; B[Preprocess]\n        B --&gt; C[Cache]\n    end\n\n    subgraph setup [\"setup() -- all ranks\"]\n        direction LR\n        D[Load from cache] --&gt; E[\"On-the-fly\\nProcessors\"]\n        E --&gt; F[\"Group\\nProcessors\"]\n    end\n\n    prepare_data --&gt; setup\n    setup --&gt; G[\"Dataloader\\n+ Collator\"]</code></pre> Stage When What happens Download <code>prepare_data()</code> (rank 0) <code>datasets.load_dataset()</code> fetches from HuggingFace Hub. <code>LocalDatasetManager</code> overrides this to load local CSV files instead. Preprocess <code>prepare_data()</code> (rank 0) A configurable function (dotted-path string, e.g. a tokenization function) is applied via <code>dataset.map()</code>. Unwanted columns are dropped. Cache <code>prepare_data()</code> (rank 0) The preprocessed dataset is saved to disk (<code>manual_cache_dir / full_name</code>) with <code>save_to_disk()</code>. Subsequent runs skip download + preprocess. Setup <code>setup(stage)</code> (all ranks) Loads from cache. Optionally converts to <code>IterableDataset</code> (sharded). Applies on-the-fly and group processors. Splits by node in DDP. On-the-fly Processors Lazy, per-example Per-example transforms set via <code>dataset.set_transform()</code>. Example: <code>token_ids_to_input_ids</code> converts raw token IDs into <code>input_ids</code>, <code>attention_mask</code>, <code>token_type_ids</code>. Group Processors Lazy, batched Operates on large chunks of examples, e.g. sequence packing to <code>block_size</code> without padding. Collation DataLoader A <code>Collator</code> converts a list of examples into a <code>BaseBatch</code> tensor dict (<code>input_ids</code>, <code>attention_mask</code>, <code>token_type_ids</code>)."},{"location":"guide/data-pipeline/#dataloader-and-sampler-selection","title":"Dataloader and Sampler Selection","text":"<p><code>get_dataloader()</code> picks the loader class and sampler based on three axes: split, DDP, and dataset type.</p> Split DDP Iterable Loader Sampler Notes train yes yes <code>StatefulDataLoader</code> None Shuffling handled by the IterableDataset shuffle buffer; explicit <code>shuffle</code> kwarg is ignored. Shards split across workers during <code>setup()</code>. train yes no <code>StatefulDataLoader</code> <code>StatefulDistributedSampler</code> Partitions data across ranks and shuffles (disabled under <code>DEBUG_OVERFIT</code>). train no no <code>StatefulDataLoader</code> <code>RandomSampler</code> Single-GPU. Falls back to <code>SequentialSampler</code> under <code>DEBUG_OVERFIT</code>. train no yes <code>StatefulDataLoader</code> None Single-GPU with iterable dataset; no sampler needed. val / test / predict any any <code>DataLoader</code> None Never shuffled. Standard (non-stateful) <code>DataLoader</code>. <p>All train dataloaders use <code>StatefulDataLoader</code> (from <code>torchdata</code>) so that iteration state can be checkpointed and resumed mid-epoch.</p>"},{"location":"guide/external-models/","title":"External Language Models for XLM","text":"<p>XLM supports external language models that can be developed and maintained separately from the core framework. This allows researchers to keep their model code clean and self-contained, and share models without including the entire XLM codebase. The code follows a modular design with four main components that work together to provide a complete language modeling solution. You need to implement all of them in order to add a new working model.</p>"},{"location":"guide/external-models/#quick-start","title":"Quick Start","text":""},{"location":"guide/external-models/#1-scaffold-a-new-model","title":"1. Scaffold a New Model","text":"<p>Use the scaffolding script to create a complete model structure:</p> <pre><code>xlm-scaffold my_awesome_model\n</code></pre> <p>This creates:</p> <ul> <li>A complete Python package with skeleton implementations</li> <li>All necessary Hydra configuration files</li> <li>Registers the model in <code>xlm_models.json</code></li> </ul>"},{"location":"guide/external-models/#2-implement-your-model","title":"2. Implement Your Model","text":"<p>The scaffolded files contain detailed TODOs and docstrings. Key files to implement:</p> <ul> <li><code>my_awesome_model/types_my_awesome_model.py</code> - Type definitions</li> <li><code>my_awesome_model/model_my_awesome_model.py</code> - Neural network architecture</li> <li><code>my_awesome_model/loss_my_awesome_model.py</code> - Loss computation</li> <li><code>my_awesome_model/predictor_my_awesome_model.py</code> - Inference/generation logic</li> <li><code>my_awesome_model/datamodule_my_awesome_model.py</code> - Data preprocessing</li> <li><code>my_awesome_model/metrics_my_awesome_model.py</code> - Metrics computation</li> </ul>"},{"location":"guide/external-models/#3-test-your-model","title":"3. Test Your Model","text":"<pre><code>xlm job_type=train \\\n  job_name=my_model_test \\\n  experiment=star_easy_my_awesome_model \\\n  debug=overfit\n</code></pre>"},{"location":"guide/external-models/#main-components","title":"Main Components","text":""},{"location":"guide/external-models/#1-lossfunction","title":"1. LossFunction","text":"<p>The <code>LossFunction</code> is responsible for computing the training loss during model training, validation and optionally test time.</p> <p>Key Responsibilities: - Compute loss between model predictions and ground truth targets - Return a dictionary with \"loss\" key and any other additional values that you want to track.</p> <p>Interface: <pre><code>class LossFunction(Generic[T_in, T_out], Protocol):\n    model: Any\n    tokenizer: Tokenizer\n\n    def loss_fn(self, batch: T_in, ...) -&gt; T_out: ...\n    def configure(self, pl_module: \"Harness\"): ...\n        \"\"\"Converts scalar to tensor such that loss_fn becomes compile friendly. If you don't want to compile you don't need to implement this.\n        \"\"\"\n</code></pre></p> <p>Examples: <code>xlm-models/arlm/loss_arlm.py</code></p>"},{"location":"guide/external-models/#2-predictor","title":"2. Predictor","text":"<p>The <code>Predictor</code> handles generating output sequences from the model.</p> <p>Key Responsibilities: - Run the model (typically in a loop) to produce a sequence of tokens. - Convert generated token_ids to text.</p> <p>Interface: <pre><code>class Predictor(Generic[T_in, T_out_pred], Protocol):\n    tokenizer: Tokenizer\n    noise_schedule: NoiseSchedule\n    model: Any\n\n    def predict(self, batch: T_in, ...) -&gt; T_out_pred: ...\n    def to_dict(self, batch: T_in, preds: T_out_pred, ...) -&gt; List[Dict[str, Any]]: ...\n</code></pre></p> <p>Examples: <code>xlm-models/arlm/predictor_arlm.py</code></p>"},{"location":"guide/external-models/#3-collator","title":"3. Collator","text":"<p>The <code>Collator</code> prepares batches of data for training and inference. It handles data preprocessing, padding, and batching.</p> <p>Key Responsibilities: - It receives raw token_ids and converts them to a dict which is passed in as a batch to the model. - Handle padding and truncation.</p> <p>Interface: <pre><code>class Collator(Protocol):\n    \"\"\"For pre-training the model on language modeling.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n\nclass Seq2SeqCollator(Collator):\n    \"\"\"For training the model on seq2seq tasks.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n\nclass Seq2SeqCollatorPrediction(Collator):\n    \"\"\"For generating predictions for seq2seq tasks.\"\"\"\n    def __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, Any]: ...\n</code></pre></p> <p>Examples: <code>xlm-models/arlm/datamodule_arlm.py</code></p>"},{"location":"guide/external-models/#4-model","title":"4. Model","text":"<p>The <code>Model</code> is the bare neural network architecture for your LM. It defines the forward pass and model parameters.</p> <p>Key Responsibilities: - Define the neural network architecture. - Implement the forward pass.</p> <p>Interface: <pre><code>class Model:\n    def forward(self, input_ids: Tensor, attention_mask: Optional[Tensor] = None, ...) -&gt; Tensor: ...\n</code></pre></p> <p>Examples: <code>xlm-models/arlm/model_arlm.py</code></p> <p>All these four components are designed to be aware of each other, and are only expected to run with each other for the same LM and not with any other LM. This is a key design choice that allows one to implement really esoteric models without worrying about how to abstract them such that their dataflow becomes compatible with other LMs.</p>"},{"location":"guide/external-models/#model-structure","title":"Model Structure","text":"<p>Each external model follows this structure:</p> <pre><code>my_awesome_model/                    # Model root directory\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 types_my_awesome_model.py\n\u251c\u2500\u2500 model_my_awesome_model.py\n\u251c\u2500\u2500 loss_my_awesome_model.py\n\u251c\u2500\u2500 predictor_my_awesome_model.py\n\u251c\u2500\u2500 datamodule_my_awesome_model.py\n\u2514\u2500\u2500 metrics_my_awesome_model.py\n\u251c\u2500\u2500 configs/                         # Hydra configurations\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 model_type/\n\u2502   \u251c\u2500\u2500 collator/\n\u2502   \u251c\u2500\u2500 datamodule/\n\u2502   \u251c\u2500\u2500 experiment/\n\u2502   \u2514\u2500\u2500 commands.yaml                # Optional: see [Custom Commands](custom-commands.md)\n\u251c\u2500\u2500 setup.py                         # Package installation (optional)\n\u2514\u2500\u2500 README.md                        # Documentation\n</code></pre>"},{"location":"guide/external-models/#configuration","title":"Configuration","text":"<p>External models integrate with Hydra's configuration system. Use them in your experiments:</p> <pre><code># configs/experiment/my_experiment.yaml\ndefaults:\n  - override /model: my_awesome_model\n  - override /model_type: my_awesome_model\n  - override /datamodule: star_easy_my_awesome_model\n</code></pre> <p>Or via command line:</p> <pre><code>xlm job_type=train model=my_awesome_model model_type=my_awesome_model\n</code></pre> <p>Key config locations (paths may vary for external models in <code>xlm-models/</code>):</p> <ul> <li><code>configs/lightning_train/collator/</code> \u2013 Collator configs</li> <li><code>configs/lightning_train/datamodule/</code> \u2013 Datamodule configs</li> <li><code>configs/lightning_train/model/</code> \u2013 Model (neural network) configs</li> <li><code>configs/lightning_train/model_type/</code> \u2013 Loss, predictor, metrics</li> <li><code>configs/lightning_train/experiment/</code> \u2013 Experiment configs</li> </ul>"},{"location":"guide/external-models/#discovery-methods","title":"Discovery Methods","text":"<p>XLM discovers external models through two approaches:</p>"},{"location":"guide/external-models/#1-directory-based-discovery","title":"1. Directory-Based Discovery","text":"<p>Place your model directory in one of these locations:</p> <ul> <li>Current directory (<code>.</code>)</li> <li><code>xlm-models/</code> directory</li> <li>Directory specified by the <code>XLM_MODELS_PATH</code> environment variable</li> </ul> <p>Create a <code>xlm_models.json</code> file in the search directory:</p> <pre><code>{\n  \"my_awesome_model\": \"my_awesome_model\",\n  \"another_model\": \"path/to/another_model\"\n}\n</code></pre> <p>The paths are relative to the directory containing <code>xlm_models.json</code>.</p> <p>Example:</p> <pre><code># Project structure\n.\n\u251c\u2500\u2500 xlm_models.json          # {\"my_model\": \"my_model\"}\n\u251c\u2500\u2500 my_model/\n\u2502   \u251c\u2500\u2500 my_model/            # Python package\n\u2502   \u2514\u2500\u2500 configs/\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/external-models/#2-package-based-discovery","title":"2. Package-Based Discovery","text":"<p>Install your model as a Python package and register it via the <code>XLM_MODELS_PACKAGES</code> environment variable (colon-separated list of installed package names).</p> <p>Package structure requirements:</p> <p>Each model needs its own <code>setup.py</code> that packages the configs:</p> <pre><code># setup.py\nsetup(\n    name=\"my_awesome_model\",\n    packages=[\"my_awesome_model\"],\n    package_data={\n        \"my_awesome_model\": [\"configs/**/*.yaml\", \"configs/**/*.yml\"],\n    },\n    include_package_data=True,\n)\n</code></pre> <p>Installation and registration:</p> <p>Each model must be installed independently:</p> <pre><code># Install first model\npip install -e ./my_awesome_model\n\n# Install second model (separate setup.py)\npip install -e ./another_model\n\n# Register both installed packages\nexport XLM_MODELS_PACKAGES=\"my_awesome_model:another_model\"\n</code></pre> <p>Core models (<code>arlm</code>, <code>mlm</code>, <code>ilm</code>, <code>mdlm</code>) are automatically discovered and don't need to be added to <code>XLM_MODELS_PACKAGES</code>.</p>"},{"location":"guide/external-models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/external-models/#model-not-found","title":"Model Not Found","text":"<pre><code>Error: No module named 'my_model'\n</code></pre> <p>Check:</p> <ul> <li>Model is listed in <code>xlm_models.json</code> (directory-based)</li> <li>Model is installed and listed in <code>XLM_MODELS_PACKAGES</code> (package-based)</li> <li>Directory structure is correct</li> <li><code>__init__.py</code> exists in the Python package</li> </ul>"},{"location":"guide/external-models/#duplicate-model-names","title":"Duplicate Model Names","text":"<pre><code>ExternalModelConflictError: Duplicate model name: my_model\n</code></pre> <p>Solution: Each model must have a unique name. Check for duplicate entries in <code>xlm_models.json</code> files or conflicts with core XLM models.</p>"},{"location":"guide/external-models/#config-not-found","title":"Config Not Found","text":"<pre><code>hydra.errors.ConfigCompositionException: Could not find 'model/my_model'\n</code></pre> <p>Check:</p> <ul> <li><code>configs/model/my_model.yaml</code> exists</li> <li><code>configs/model_type/my_model.yaml</code> exists</li> <li>Config files have valid YAML syntax</li> <li><code>_target_</code> paths point to correct classes</li> </ul>"},{"location":"guide/external-models/#hydra-errors","title":"Hydra Errors","text":"<p>If you see <code>Unable to find or instantiate abc.xyz.MyClass</code>, try importing manually: <code>python -c \"from abc.xyz import MyClass\"</code>.</p>"},{"location":"guide/external-models/#unable-to-implement-a-component","title":"Unable to implement a component","text":"<p>Check existing models in <code>xlm-models/</code> (arlm, mlm, ilm, mdlm) for reference.</p>"},{"location":"guide/external-models/#examples","title":"Examples","text":"<p>See the core models in the <code>xlm-models</code> package for complete examples:</p> <ul> <li><code>arlm</code> - Auto-regressive language model</li> <li><code>mlm</code> - Masked language model</li> <li><code>ilm</code> - Infilling language model</li> <li><code>mdlm</code> - Masked diffusion language model</li> </ul>"},{"location":"guide/faq/","title":"FAQ","text":""},{"location":"guide/faq/#general","title":"General","text":"<p>What is XLM? XLM is a unified framework for developing and comparing non-autoregressive language models. It provides modular components for models, losses, predictors, and data collation.</p> <p>Which models are available? The framework includes MLM, ILM, ARLM, MDLM, and IDLM. See the Quick Start for the full list.</p>"},{"location":"guide/faq/#usage","title":"Usage","text":"<p>How do I train on a new dataset? Use the appropriate experiment config for your model and dataset. For example: <code>xlm job_type=train job_name=my_run experiment=lm1b_ilm</code>.</p> <p>How do I debug training? Add <code>debug=overfit</code> to overfit on a single batch, or use other debug configs from <code>configs/lightning_train/debug/</code>.</p>"},{"location":"guide/faq/#contributing","title":"Contributing","text":"<p>How do I add a new model? See the Contributing Guide for a complete walkthrough of the four components: Model, Loss, Predictor, and Collator.</p>"},{"location":"guide/metrics/","title":"Metrics","text":"<p>This guide covers how metrics are defined, configured, and evaluated.  For how metrics connect to the data pipeline via dataloader names, see the Data Pipeline guide.</p>"},{"location":"guide/metrics/#two-metric-categories","title":"Two Metric Categories","text":"<p>The <code>Harness</code> maintains two parallel metric dictionaries:</p> Category Purpose Example <code>diagnostic_metrics</code> Model-internal or debugging signals that may differ across model types. per-token CE, length loss <code>reported_metrics</code> Standardised task-level metrics that are comparable across models. accumulated loss, exact match, token accuracy <p>Both use the same nested structure and the same <code>MetricWrapper</code> class; the only difference is intent.  You can configure either (or both) as <code>null</code> if not needed.</p>"},{"location":"guide/metrics/#metricwrapper","title":"MetricWrapper","text":"<p>Every configured metric is a <code>MetricWrapper</code> instance (<code>xlm.metrics</code>).  It wraps a torchmetrics <code>Metric</code> and adds two pieces of glue:</p> <ul> <li><code>update_fn(batch, loss_dict, tokenizer, ...)</code> -- a plain function   (specified as a dotted-path string) that extracts the right inputs from the   batch and loss dictionary and returns a <code>dict</code> of kwargs for the underlying   <code>metric.update()</code>.</li> <li><code>log(pl_module, batch, loss_dict)</code> -- logs the metric value via   Lightning's <code>self.log()</code> with a configurable <code>prefix</code>, <code>on_step</code>, <code>on_epoch</code>,   and <code>prog_bar</code>.</li> </ul> <p>This separation means the <code>Metric</code> itself stays generic (e.g. <code>MeanMetric</code>, <code>ExactMatch</code>) while all task-specific and model-specific extraction logic lives in the <code>update_fn</code>.</p>"},{"location":"guide/metrics/#storage-structure","title":"Storage Structure","text":"<p>Metrics are stored as nested <code>ModuleDict</code>s inside the <code>Harness</code>:</p> <pre><code>reported_metrics\n  \u2514\u2500\u2500 metrics_{stage}          (e.g. \"metrics_val\")\n        \u2514\u2500\u2500 {dataloader_name}  (e.g. \"lm\", \"prediction\")\n              \u2514\u2500\u2500 ModuleList[MetricWrapper, ...]\n</code></pre> <p><code>diagnostic_metrics</code> follows the same layout.</p>"},{"location":"guide/metrics/#step-flow","title":"Step Flow","text":"<p>During every training / validation / test step, <code>Harness._step()</code>:</p> <ol> <li>Resolves <code>dataloader_idx</code> to <code>dataloader_name</code> (see Data Pipeline).</li> <li>Calls <code>compute_loss(batch, ...)</code>.</li> <li>Iterates over all <code>diagnostic_metrics</code> and <code>reported_metrics</code> for that    dataloader name, calling <code>metric.update(...)</code> then <code>metric.log(...)</code> on each.</li> </ol> <p>At epoch end, <code>on_validation_epoch_end</code> and <code>on_test_epoch_end</code> trigger additional aggregate computations for dataloaders whose name contains <code>\"prediction\"</code> -- specifically generative perplexity and any configured post-hoc metrics.</p>"},{"location":"guide/metrics/#configuration","title":"Configuration","text":"<p>Metric configs live in <code>configs/lightning_train/metrics/</code>.  Each file defines a single <code>MetricWrapper</code>:</p> <pre><code># configs/lightning_train/metrics/accumulated_loss.yaml\n_target_: xlm.metrics.MetricWrapper\nname: accumulated_loss\nmetric:\n  _target_: torchmetrics.MeanMetric\nprefix: ???   # e.g. train/lm, val/lm\nupdate_fn: ??? # e.g. mdlm.metrics_mdlm.mean_metric_update_fn\n</code></pre> <p>These are composed into the model-type config via Hydra defaults.  For example, a seq2seq model type might wire up both diagnostic and reported metrics like this:</p> <pre><code># model_type config (simplified)\ndefaults:\n  - /metrics@reported_metrics.train.lm.accumulated_loss: accumulated_loss\n  - /metrics@reported_metrics.val.lm.accumulated_loss: accumulated_loss\n  - /metrics@reported_metrics.val.prediction.exact_match: seq2seq_exact_match\n  - /metrics@reported_metrics.val.prediction.token_accuracy: seq2seq_token_accuracy\n  - /metrics@diagnostic_metrics.train.lm.length_loss: seq2seq_length_loss\n  - /metrics@diagnostic_metrics.train.lm.token_ce: seq2seq_token_ce\n\ndiagnostic_metrics:\n  train:\n    lm:\n      length_loss:\n        prefix: train/lm\n        update_fn: idlm.metrics.length_loss_metric_update_fn\n      token_ce:\n        prefix: train/lm\n        update_fn: idlm.metrics.token_ce_metric_update_fn\n\nreported_metrics:\n  train:\n    lm:\n      accumulated_loss:\n        prefix: train/lm\n        update_fn: idlm.metrics.mean_metric_update_fn\n  val:\n    lm:\n      accumulated_loss:\n        prefix: val/lm\n        update_fn: idlm.metrics.mean_metric_update_fn\n    prediction:\n      exact_match:\n        prefix: val/prediction\n        update_fn: idlm.metrics.seq2seq_exact_match_update_fn\n      token_accuracy:\n        prefix: val/prediction\n        update_fn: idlm.metrics.seq2seq_token_accuracy_update_fn\n</code></pre> <p>The top-level keys (<code>train</code>, <code>val</code>, <code>test</code>) and the second-level keys (<code>lm</code>, <code>prediction</code>) must match the dataloader names in the datamodule config -- that is how the <code>Harness</code> knows which metrics to apply to which dataloader.</p>"},{"location":"guide/metrics/#built-in-metrics-and-update-functions","title":"Built-in Metrics and Update Functions","text":"Metric class Module Description <code>MeanMetric</code> <code>torchmetrics</code> Tracks a running mean (used for loss). <code>ExactMatch</code> <code>xlm.metrics</code> Sequence-level exact match rate. <code>TokenAccuracy</code> <code>xlm.metrics</code> Per-token accuracy over predicted positions. Update function Module Extracts <code>mean_metric_update_fn</code> <code>xlm.metrics</code> <code>loss_dict[\"batch_loss\"]</code> <code>exact_match_update_fn</code> <code>xlm.metrics</code> <code>batch[\"target_ids\"]</code> vs <code>loss_dict[\"ids\"]</code> <code>seq2seq_exact_match_update_fn</code> <code>xlm.metrics</code> Concatenated input+target vs predicted ids <code>seq2seq_token_accuracy_update_fn</code> <code>xlm.metrics</code> Token-level accuracy on target positions <p>To add a custom metric, create a new <code>Metric</code> subclass (or use an existing torchmetrics metric), write an <code>update_fn</code> that extracts the right fields, add a YAML config under <code>configs/lightning_train/metrics/</code>, and wire it into your model-type config.</p>"},{"location":"guide/quickstart/","title":"Quick Start","text":""},{"location":"guide/quickstart/#installation","title":"Installation","text":"<pre><code>pip install xlm-core\n</code></pre> <p>For existing model implementations, also install:</p> <pre><code>pip install xlm-models\n</code></pre>"},{"location":"guide/quickstart/#cli-usage","title":"CLI Usage","text":"<p>XLM uses a simple CLI with three main arguments:</p> <pre><code>xlm job_type=&lt;JOB&gt; job_name=&lt;NAME&gt; experiment=&lt;CONFIG&gt;\n</code></pre> Argument Description <code>job_type</code> One of <code>prepare_data</code>, <code>train</code>, <code>eval</code>, or <code>generate</code> <code>job_name</code> A descriptive name for your run <code>experiment</code> Path to your Hydra experiment config"},{"location":"guide/quickstart/#example-ilm-on-lm1b","title":"Example: ILM on LM1B","text":"<p>A complete workflow demonstrating the Insertion Language Model on the LM1B dataset:</p>"},{"location":"guide/quickstart/#1-prepare-data","title":"1. Prepare Data","text":"<pre><code>xlm job_type=prepare_data job_name=lm1b_prepare experiment=lm1b_ilm\n</code></pre>"},{"location":"guide/quickstart/#2-train","title":"2. Train","text":"<pre><code># Quick debug run (overfit a single batch)\nxlm job_type=train job_name=lm1b_ilm experiment=lm1b_ilm debug=overfit\n\n# Full training\nxlm job_type=train job_name=lm1b_ilm experiment=lm1b_ilm\n</code></pre>"},{"location":"guide/quickstart/#3-evaluate","title":"3. Evaluate","text":"<pre><code>xlm job_type=eval job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +eval.ckpt_path=&lt;CHECKPOINT_PATH&gt;\n</code></pre>"},{"location":"guide/quickstart/#4-generate","title":"4. Generate","text":"<pre><code>xlm job_type=generate job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +generation.ckpt_path=&lt;CHECKPOINT_PATH&gt;\n</code></pre> <p>Tip: Add <code>debug=[overfit,print_predictions]</code> to print generated samples to the console:</p> <pre><code>xlm job_type=generate job_name=lm1b_ilm experiment=lm1b_ilm \\\n    +generation.ckpt_path=&lt;CHECKPOINT_PATH&gt; \\\n    debug=[overfit,print_predictions]\n</code></pre>"},{"location":"guide/quickstart/#5-push-to-hugging-face-hub","title":"5. Push to Hugging Face Hub","text":"<pre><code>xlm job_type=push_to_hub job_name=lm1b_ilm_hub experiment=lm1b_ilm \\\n    +hub_checkpoint_path=&lt;CHECKPOINT_PATH&gt; \\\n    +hub.repo_id=&lt;YOUR_REPO_ID&gt;\n</code></pre>"},{"location":"reference/xlm/","title":"xlm","text":""},{"location":"reference/xlm/#xlm","title":"<code>xlm</code>","text":"<p>xlm</p>"},{"location":"reference/xlm/commands/","title":"commands","text":""},{"location":"reference/xlm/commands/#xlm.commands","title":"<code>xlm.commands</code>","text":""},{"location":"reference/xlm/commands/cli_demo/","title":"cli_demo","text":""},{"location":"reference/xlm/commands/cli_demo/#xlm.commands.cli_demo","title":"<code>xlm.commands.cli_demo</code>","text":""},{"location":"reference/xlm/commands/cli_demo/#xlm.commands.cli_demo.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"<p>Instantiate a model from checkpoint or config.</p> Supports two modes <ol> <li>Load a model from full training checkpoint using <code>lightning_module.load_from_checkpoint(cfg.generation.ckpt_path)</code></li> <li>Load a model from model only checkpoint using <code>lightning_module.model.load_state_dict(torch.load(cfg.generation.model_only_checkpoint_path))</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule instance</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer instance</p> required <p>Returns:</p> Name Type Description <code>Harness</code> <code>Harness</code> <p>The instantiated model</p>"},{"location":"reference/xlm/commands/cli_demo/#xlm.commands.cli_demo.generate","title":"<code>generate(cfg)</code>","text":"<p>Generate text using the CLI demo interface.</p>"},{"location":"reference/xlm/commands/cli_demo/#xlm.commands.cli_demo.main","title":"<code>main(cfg)</code>","text":"<p>Main function for CLI demo.</p>"},{"location":"reference/xlm/commands/extract_model_state_dict/","title":"extract_model_state_dict","text":""},{"location":"reference/xlm/commands/extract_model_state_dict/#xlm.commands.extract_model_state_dict","title":"<code>xlm.commands.extract_model_state_dict</code>","text":""},{"location":"reference/xlm/commands/generate_star_graphs/","title":"generate_star_graphs","text":""},{"location":"reference/xlm/commands/generate_star_graphs/#xlm.commands.generate_star_graphs","title":"<code>xlm.commands.generate_star_graphs</code>","text":""},{"location":"reference/xlm/commands/hydra_callbacks/","title":"hydra_callbacks","text":""},{"location":"reference/xlm/commands/hydra_callbacks/#xlm.commands.hydra_callbacks","title":"<code>xlm.commands.hydra_callbacks</code>","text":"<p>Custom hydra callbacks for xlm. See https://hydra.cc/docs/experimental/callbacks/ for more information.</p>"},{"location":"reference/xlm/commands/hydra_callbacks/#xlm.commands.hydra_callbacks.LogComposeCallback","title":"<code>LogComposeCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Log compose call, result, and debug info</p>"},{"location":"reference/xlm/commands/hydra_callbacks/#xlm.commands.hydra_callbacks.LogComposeCallback.on_run_start","title":"<code>on_run_start(config, **kwargs)</code>","text":"<p>Called in RUN mode before job/application code starts. <code>config</code> is composed with overrides. Some <code>hydra.runtime</code> configs are not populated yet. See hydra.core.utils.run_job for more info.</p>"},{"location":"reference/xlm/commands/lightning_eval/","title":"lightning_eval","text":""},{"location":"reference/xlm/commands/lightning_eval/#xlm.commands.lightning_eval","title":"<code>xlm.commands.lightning_eval</code>","text":""},{"location":"reference/xlm/commands/lightning_eval/#xlm.commands.lightning_eval.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"<p>Instantiate and load a model for evaluation.</p> Supports two modes <ol> <li>Load a model from full training checkpoint using <code>lightning_module.load_from_checkpoint(cfg.eval.checkpoint_path)</code></li> <li>Load a model from model only checkpoint using <code>lightning_module.model.load_state_dict(torch.load(cfg.eval.model_only_checkpoint_path))</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer</p> required <p>Returns:</p> Type Description <code>Harness</code> <p>Tuple of (lightning_module, ckpt_path) where ckpt_path is the full checkpoint path</p> <code>Optional[str]</code> <p>(or None if using model-only checkpoint)</p>"},{"location":"reference/xlm/commands/lightning_generate/","title":"lightning_generate","text":""},{"location":"reference/xlm/commands/lightning_generate/#xlm.commands.lightning_generate","title":"<code>xlm.commands.lightning_generate</code>","text":""},{"location":"reference/xlm/commands/lightning_generate/#xlm.commands.lightning_generate.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"Supports two modes <ol> <li>Load a model from full training checkpoint using <code>lightning_module.load_from_checkpoint(cfg.generation.ckpt_path)</code></li> <li>Load a model from model only checkpoint using <code>lightning_module.model.load_state_dict(torch.load(cfg.generation.model_only_checkpoint_path))</code></li> </ol> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>generation_ckpt_path</code> <p>Path to the generation checkpoint</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule</p> required"},{"location":"reference/xlm/commands/lightning_main/","title":"lightning_main","text":""},{"location":"reference/xlm/commands/lightning_main/#xlm.commands.lightning_main","title":"<code>xlm.commands.lightning_main</code>","text":""},{"location":"reference/xlm/commands/lightning_prepare_data/","title":"lightning_prepare_data","text":""},{"location":"reference/xlm/commands/lightning_prepare_data/#xlm.commands.lightning_prepare_data","title":"<code>xlm.commands.lightning_prepare_data</code>","text":""},{"location":"reference/xlm/commands/lightning_train/","title":"lightning_train","text":""},{"location":"reference/xlm/commands/lightning_train/#xlm.commands.lightning_train","title":"<code>xlm.commands.lightning_train</code>","text":""},{"location":"reference/xlm/commands/push_to_hub/","title":"push_to_hub","text":""},{"location":"reference/xlm/commands/push_to_hub/#xlm.commands.push_to_hub","title":"<code>xlm.commands.push_to_hub</code>","text":""},{"location":"reference/xlm/commands/push_to_hub/#xlm.commands.push_to_hub.PushToHubConfig","title":"<code>PushToHubConfig</code>  <code>dataclass</code>","text":"<p>Configuration for pushing model to Hugging Face Hub.</p>"},{"location":"reference/xlm/commands/push_to_hub/#xlm.commands.push_to_hub.instantiate_model","title":"<code>instantiate_model(cfg, datamodule, tokenizer)</code>","text":"<p>Instantiate a model from checkpoint or config.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config</p> required <code>datamodule</code> <code>Any</code> <p>Datamodule instance</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer instance</p> required <p>Returns:</p> Name Type Description <code>Harness</code> <code>Harness</code> <p>The instantiated model</p>"},{"location":"reference/xlm/commands/push_to_hub/#xlm.commands.push_to_hub.main","title":"<code>main(cfg)</code>","text":"<p>Main function for Push to Hub.</p>"},{"location":"reference/xlm/commands/scaffold_model/","title":"scaffold_model","text":""},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model","title":"<code>xlm.commands.scaffold_model</code>","text":"<p>Script to scaffold a new external language model for the XLM framework.</p> <p>Usage: xlm-scaffold  [options] <p>This script creates a complete external model structure with: - Python package with skeleton implementations - Configuration files for all necessary components - Documentation and examples</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.validate_model_name","title":"<code>validate_model_name(name)</code>","text":"<p>Validate and normalize model name.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.create_template_context","title":"<code>create_template_context(model_name)</code>","text":"<p>Create template context with all necessary variables.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_types_file","title":"<code>generate_types_file(model_dir, context)</code>","text":"<p>Generate the types_{model_name}.py file with TypedDict definitions.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_model_file","title":"<code>generate_model_file(model_dir, context)</code>","text":"<p>Generate the model_{model_name}.py file with the neural network implementation.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_loss_file","title":"<code>generate_loss_file(model_dir, context)</code>","text":"<p>Generate the loss_{model_name}.py file with loss computation.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_predictor_file","title":"<code>generate_predictor_file(model_dir, context)</code>","text":"<p>Generate the predictor_{model_name}.py file with inference logic.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_datamodule_file","title":"<code>generate_datamodule_file(model_dir, context)</code>","text":"<p>Generate the datamodule_.py file with data processing logic."},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_metrics_file","title":"<code>generate_metrics_file(model_dir, context)</code>","text":"<p>Generate the metrics_{model_name}.py file with metric computation logic.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_init_file","title":"<code>generate_init_file(model_dir, context, is_core=False)</code>","text":"<p>Generate the init.py file.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_config_files","title":"<code>generate_config_files(config_dir, context, is_core=False)</code>","text":"<p>Generate all configuration files.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_setup_file","title":"<code>generate_setup_file(model_dir, context)</code>","text":"<p>Generate setup.py for the external model.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.generate_documentation","title":"<code>generate_documentation(model_dir, context)</code>","text":"<p>Generate README and documentation.</p>"},{"location":"reference/xlm/commands/scaffold_model/#xlm.commands.scaffold_model.update_xlm_models_file","title":"<code>update_xlm_models_file(model_name, xlm_models_path=Path('xlm_models.json'))</code>","text":"<p>Add the new model to the xlm_models.json file.</p>"},{"location":"reference/xlm/commands/split_owt/","title":"split_owt","text":""},{"location":"reference/xlm/commands/split_owt/#xlm.commands.split_owt","title":"<code>xlm.commands.split_owt</code>","text":""},{"location":"reference/xlm/commands/train_tokenizer/","title":"train_tokenizer","text":""},{"location":"reference/xlm/commands/train_tokenizer/#xlm.commands.train_tokenizer","title":"<code>xlm.commands.train_tokenizer</code>","text":""},{"location":"reference/xlm/datamodule/","title":"datamodule","text":""},{"location":"reference/xlm/datamodule/#xlm.datamodule","title":"<code>xlm.datamodule</code>","text":""},{"location":"reference/xlm/datamodule/#xlm.datamodule.BaseBatch","title":"<code>BaseBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with the keys that are present in input batches for all models.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Can depend on the model type. For ILM and IDLM: 0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.SimpleSpaceTokenizer","title":"<code>SimpleSpaceTokenizer</code>","text":"<p>               Bases: <code>PreTrainedTokenizer</code></p> <p>Splits on spaces</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.SimpleSpaceTokenizer.__init__","title":"<code>__init__(vocab, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Sequence[str]</code> <p>List of desired tokens. Following are list of all of the special tokens with their corresponding ids:     \"[PAD]\": 0,     \"[UNK]\": 1,     \"[MASK]\": 2,     \"[EOS]\": 3,     \"[BOS]\": 4, an id (starting at 5) will be assigned to each character.</p> required <code>model_max_length</code> <code>int</code> <p>Model maximum sequence length.</p> required"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager","title":"<code>DatasetManager</code>","text":"<p>Manages a single dataset through its lifecycle: download, preprocess, cache, setup, and dataloader creation.</p> <p>Used per split (train/val/test/predict) by :class:<code>TextDataModule</code>, which orchestrates multiple DatasetManager instances.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager.__init__","title":"<code>__init__(collator, full_name, full_name_debug, dataloader_kwargs, preprocess_function=None, preprocess_function_kwargs=None, on_the_fly_processor=None, on_the_fly_processor_kwargs=None, on_the_fly_group_processor=None, model_name=None, columns_to_remove=None, stages=None, iterable_dataset_shards=None, shuffle_buffer_size=None, shuffle_seed=42, split_by_node=True, rewrite_manual_cache=False, use_manual_cache=True)</code>","text":"<p>Initialize the dataset manager.</p> <p>Parameters:</p> Name Type Description Default <code>collator</code> <code>Collator</code> <p>Collates examples into batches; model-specific.</p> required <code>full_name</code> <code>str</code> <p>Full dataset path (e.g., \"repo/ds_name/split\").</p> required <code>full_name_debug</code> <code>str</code> <p>Used when DEBUG_OVERFIT is True; typically the train split path so val/test managers overfit on train data.</p> required <code>dataloader_kwargs</code> <code>DataLoaderKwargs</code> <p>batch_size, num_workers, shuffle, pin_memory.</p> required <code>preprocess_function</code> <code>Optional[str]</code> <p>Dotted path to preprocessing function (e.g., tokenization).</p> <code>None</code> <code>preprocess_function_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Kwargs passed to the preprocess function.</p> <code>None</code> <code>on_the_fly_processor</code> <code>Optional[str]</code> <p>Dotted path to per-example processor (e.g., ids_to_example_fn). For iterable (streaming) datasets, this is applied to each example on the fly, and before the group processor.</p> <code>None</code> <code>on_the_fly_processor_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Kwargs for the on-the-fly processor.</p> <code>None</code> <code>on_the_fly_group_processor</code> <code>Optional[str]</code> <p>Dotted path to the group processor function, which receives large batches of examples, and applies on-the-fly processors that require a big chunk of data, for example, packing sequences without padding.</p> <code>None</code> <code>model_name</code> <code>Optional[str]</code> <p>Used for model-specific cache subdirectories.</p> <code>None</code> <code>columns_to_remove</code> <code>Optional[List[str]]</code> <p>Columns to drop during preprocessing (e.g., [\"text\"]).</p> <code>None</code> <code>stages</code> <code>Optional[List[Literal['fit', 'validate', 'test', 'predict']]]</code> <p>Lightning stages this manager participates in.</p> <code>None</code> <code>iterable_dataset_shards</code> <code>Optional[int]</code> <p>If set, dataset is converted to IterableDataset with this many shards.</p> <code>None</code> <code>shuffle_buffer_size</code> <code>Optional[int]</code> <p>Buffer size for dataset.shuffle() (IterableDataset).</p> <code>None</code> <code>shuffle_seed</code> <code>Optional[int]</code> <p>Seed for shuffle.</p> <code>42</code> <code>split_by_node</code> <code>bool</code> <p>Whether to split IterableDataset by rank in DDP.</p> <code>True</code> <code>rewrite_manual_cache</code> <code>bool</code> <p>If True, re-download and overwrite cached data.</p> <code>False</code> <code>use_manual_cache</code> <code>bool</code> <p>If True, use manual cache dir created using save_to_disk(), else let HF Datasets do automatic caching.</p> <code>True</code>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager.set_epoch","title":"<code>set_epoch(epoch)</code>","text":"<p>Set epoch for IterableDataset shuffle buffer reproducibility.</p> <p>For IterableDataset, calls dataset.set_epoch(epoch). For map-style datasets with DistributedSampler, set_epoch must be called on the sampler separately.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current training epoch.</p> required Warning <p>Note for future extensions. For map-style datasets with DistributedSampler, set_epoch must be called on the sampler separately.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager.prepare_data","title":"<code>prepare_data(manual_cache_dir, tokenizer, num_proc=None, load=False)</code>","text":"<p>Download, preprocess, and cache the dataset.</p> <p>If use_manual_cache: checks cache first; downloads and caches (using save_to_disk()) if missing. Note this different from HF datasets' automatic caching. If rewrite_manual_cache: re-downloads and overwrites cache. Called before setup() by TextDataModule.prepare_data().</p> <p>Parameters:</p> Name Type Description Default <code>manual_cache_dir</code> <code>str</code> <p>Base directory for manual cache.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer for preprocessing.</p> required <code>num_proc</code> <code>Optional[int]</code> <p>Number of processes for parallel map operations.</p> <code>None</code> <code>load</code> <code>bool</code> <p>If True and cache exists, load and return the dataset; else return None.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Dataset]</code> <p>The dataset if load=True and cache was found/created; None otherwise.</p> Note <p>This method is only called on rank 0 and therefore does not set an state on the instance. setup() is called on all ranks.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager.setup","title":"<code>setup(stage, manual_cache_dir, tokenizer, block_size, is_ddp, rank, world_size, num_dataset_workers=None)</code>","text":"<p>Load dataset (should have already been downloaded and preprocessed by prepare_data()), optionally convert to IterableDataset, apply on-the-fly and group processors, and split by node if DDP.</p> calledBy <p>TextDataModule.setup().</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Literal['fit', 'validate', 'test', 'predict']</code> <p>Lightning stage (fit, validate, test, predict).</p> required <code>manual_cache_dir</code> <code>str</code> <p>Base directory for manual cache.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer for processors.</p> required <code>block_size</code> <code>int</code> <p>Max sequence length for group processors.</p> required <code>is_ddp</code> <code>bool</code> <p>Whether distributed training is used.</p> required <code>rank</code> <code>int</code> <p>Global rank of this process.</p> required <code>world_size</code> <code>int</code> <p>Total number of processes.</p> required <code>num_dataset_workers</code> <code>Optional[int]</code> <p>Number of workers for prepare_data if cache not used.</p> <code>None</code>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DatasetManager.get_dataloader","title":"<code>get_dataloader(type, is_ddp, rank, world_size)</code>","text":"<p>Return a DataLoader or StatefulDataLoader for the given split.</p> <p>Chooses loader and sampler based on: DDP vs single-GPU, IterableDataset vs map-style. Train: StatefulDataLoader with appropriate sampler. Val/test/predict: standard DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>Dataloader type (train, val, test, predict).</p> required <code>is_ddp</code> <code>bool</code> <p>Whether distributed training is used.</p> required <code>rank</code> <code>int</code> <p>Global rank of this process.</p> required <code>world_size</code> <code>int</code> <p>Total number of processes.</p> required <p>Returns:</p> Type Description <code>Union[DataLoader, StatefulDataLoader]</code> <p>Configured DataLoader or StatefulDataLoader.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.UnconditionalGenerationDatasetManager","title":"<code>UnconditionalGenerationDatasetManager</code>","text":"<p>This is used for unconditional generation, where we don't have any input text.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.BaseDataModule","title":"<code>BaseDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Base class for all datamodules.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.BaseDataModule.tokenizer","title":"<code>tokenizer</code>  <code>instance-attribute</code>","text":"<p>The tokenizer.[Required]</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.BaseDataModule.print_batch","title":"<code>print_batch(batch, split, dataloader_idx=None)</code>","text":"<p>Required to print train and validation batches at the beginning of the epoch.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.BaseCollatorInput","title":"<code>BaseCollatorInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with values that are lists of raw input_ids of variable length.</p> <p>This is the input to the collator for pre-training.</p> <p>The elements of the lists can be of different lengths.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>List[int]</code> <p>The input ids.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.Seq2SeqCollatorInput","title":"<code>Seq2SeqCollatorInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dict with values that are lists of raw input_ids, attention_mask, and token_type_ids.</p> <p>This is the input to the collator for pre-training.</p> <p>The elements of the lists can be of different lengths.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>List[int]</code> <p>The input ids.</p> <code>prompt_ids</code> <code>List[int]</code> <p>The target ids.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DefaultCollator","title":"<code>DefaultCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Simply stacks the input_ids, attention_mask, and token_type_ids and returns a batch.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DefaultCollatorWithPadding","title":"<code>DefaultCollatorWithPadding</code>","text":"<p>               Bases: <code>DefaultCollator</code></p> <p>Like DefaultCollator, but pads (truncates if needed) the input_ids, attention_mask, and token_type_ids to self.max_length.</p>"},{"location":"reference/xlm/datamodule/#xlm.datamodule.DefaultCollatorWithDynamicPadding","title":"<code>DefaultCollatorWithDynamicPadding</code>","text":"<p>               Bases: <code>DefaultCollatorWithPadding</code></p> <p>Like DefaultCollator, but pads to the max length in the batch.</p>"},{"location":"reference/xlm/external_models/","title":"external_models","text":""},{"location":"reference/xlm/external_models/#xlm.external_models","title":"<code>xlm.external_models</code>","text":""},{"location":"reference/xlm/external_models/#xlm.external_models.ExternalModelConflictError","title":"<code>ExternalModelConflictError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there are conflicts between external models or with core XLM.</p>"},{"location":"reference/xlm/external_models/#xlm.external_models.discover_external_models","title":"<code>discover_external_models()</code>","text":"<p>Discover external model directories.</p> <p>Each model must be packaged with its configs in the following structure:     |-- model.py    |-- loss.py    |-- predictor.py    |-- datamodule.py    |-- metrics.py    |-- ..    |-- setup.py (optional)    |-- README.md (optional)    |-- configs/    |   |-- datamodule/    |   |-- experiment/    |   |-- ..    |   |-- .. <p>The model can be installed as a python package (recommended for sharing) or simply kept as a directory (during development). When installed as a python package, make sure to package the configs as well like so in the setup.py: package_dir={     \"\": \"\", }, package_data={     \"\": [\"configs//*.yaml\", \"configs//*.yml\"], } <p>Discovery:</p> <ol> <li>search_dirs: We look for directories that may contain .     By default these are the \".\" (current directory), \"xlm-models\" (standard xlm-models directory), and the directory specified in the XLM_MODELS_PATH environment variable. <li>Each search_dir may contain multiple  directories, hence multiple models. Therefore, we look for a xlm_models.json file in each search_dir that     has the following structure:     <pre><code>{\n    \"&lt;model_name_1&gt;\": \"&lt;model_root_dir_1&gt;\",\n    \"&lt;model_name_2&gt;\": \"&lt;model_root_dir_2&gt;\",\n    ...\n}\n</code></pre>     The model root dir path is relative to the search_dir. <li>installed python packages: We also allow discovering models from installed python packages. The package names must be specified in the XLM_MODELS_PACKAGES environment variable as a colon-separated list (e.g., arlm:mlm:ilm:mdlm). The installed package much follow the same structure as the  and package the configs as shown above. For the case of python packages, we expect only one model per package. <p>Parameters:</p> Name Type Description Default <code>validate</code> <p>Whether to run validation on discovered models.</p> required <code>strict_validation</code> <p>If True, raise errors for validation failures. If False, just warn.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Path], Dict[str, Path]]</code> <p>dict of model_name -&gt; model_root_dir</p> <p>Raises:</p> Type Description <code>ExternalModelConflictError</code> <p>If validation fails and strict_validation=True</p>"},{"location":"reference/xlm/external_models/#xlm.external_models.setup_external_models","title":"<code>setup_external_models()</code>","text":"<p>Auto-discover and register external models.</p> <p>Parameters:</p> Name Type Description Default <code>validate</code> <p>Whether to run validation on discovered models.</p> required <code>strict_validation</code> <p>If True, raise errors for validation failures. If False, just warn.</p> required <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of discovered external model directories.</p> <p>Raises:</p> Type Description <code>ExternalModelConflictError</code> <p>If validation fails and strict_validation=True</p>"},{"location":"reference/xlm/flags/","title":"flags","text":""},{"location":"reference/xlm/flags/#xlm.flags","title":"<code>xlm.flags</code>","text":""},{"location":"reference/xlm/generative_perplexity/","title":"generative_perplexity","text":""},{"location":"reference/xlm/generative_perplexity/#xlm.generative_perplexity","title":"<code>xlm.generative_perplexity</code>","text":""},{"location":"reference/xlm/generative_perplexity/#xlm.generative_perplexity.AutoModelForCausalLMGenerativePerplexityEvaluator","title":"<code>AutoModelForCausalLMGenerativePerplexityEvaluator</code>","text":"<p>               Bases: <code>GenerativePerplexityEvaluator</code></p>"},{"location":"reference/xlm/generative_perplexity/#xlm.generative_perplexity.AutoModelForCausalLMGenerativePerplexityEvaluator.__init__","title":"<code>__init__(name, batch_size=64, device=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pretrained model. Currently supported models: gpt2-*</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the evaluator. The default should work on A100. (default: 64)</p> <code>64</code>"},{"location":"reference/xlm/generative_perplexity/#xlm.generative_perplexity.AutoModelForCausalLMGenerativePerplexityEvaluator.load","title":"<code>load(generator_tokenizer, device)</code>","text":"<p>Load the pretrained model and tokenizer.</p> <ol> <li>EOS     We need to make sure that if the generator generates EOS token. Then the evaluator     can evaluate it correctly. That is, the eval tokenizer recognizes it as single EOS token and the eval     model is trained to predict it.</li> <li>PAD     We need PAD token in eval tokenizer because retokenization will produce sequences of varying lengths.     Some models like GPT2 only have EOS token. In those cases, we will not be able to evaluate EOS generation     because PAD will be set to EOS and the EOS token will be lost.</li> </ol>"},{"location":"reference/xlm/harness/","title":"harness","text":""},{"location":"reference/xlm/harness/#xlm.harness","title":"<code>xlm.harness</code>","text":""},{"location":"reference/xlm/harness/#xlm.harness.LRSchedulerWithConfig","title":"<code>LRSchedulerWithConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>We follow the same structure as the one in LightningModule. lr_scheduler_config = {             # REQUIRED: The lr_scheduler instance             \"scheduler\": lr_scheduler,             # The unit of the lr_scheduler's step size, could also be 'step'.             # 'epoch' updates the lr_scheduler on epoch end whereas 'step'             # updates it after a optimizer update.             \"interval\": \"epoch\",             # How many epochs/steps should pass between calls to             # <code>lr_scheduler.step()</code>. 1 corresponds to updating the learning             # rate after every epoch/step.             \"frequency\": 1,             # Metric to to monitor for schedulers like <code>ReduceLROnPlateau</code>             \"monitor\": \"val_loss\",             # If set to <code>True</code>, will enforce that the value specified 'monitor'             # is available when the lr_scheduler is updated, thus stopping             # training if not found. If set to <code>False</code>, it will only produce a warning             \"strict\": True,         }</p>"},{"location":"reference/xlm/harness/#xlm.harness.Predictor","title":"<code>Predictor</code>","text":"<p>               Bases: <code>Generic[T_in, T_out_pred]</code>, <code>Protocol</code></p>"},{"location":"reference/xlm/harness/#xlm.harness.Predictor.to_dict","title":"<code>to_dict(batch, preds, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Create json lines from the predictions batch.</p>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin","title":"<code>PredictorHistoryMixin</code>","text":"<p>Mixin class for adding history tracking to predictors.</p> <p>This mixin provides generic history tracking capabilities that can be used by any predictor that implements iterative generation. History is stored as a list of tuples: (decoded_text, confidence_score, step_number).</p> Usage <p>class MyPredictor(torch.nn.Module, PredictorHistoryMixin, Predictor[...]):     def init(self, ..., return_history: bool = False):         super().init()         self.init_history(return_history=return_history)         ...</p> <pre><code>def predict(self, batch):\n    history = self.create_history(batch_size)\n    for step in range(steps):\n        # ... generation logic ...\n        history = self.update_history_from_state(history, state, step)\n    return {\"text\": ..., \"history\": history}\n</code></pre>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin.init_history","title":"<code>init_history(return_history=False, decode_fn=None)</code>","text":"<p>Initialize history tracking.</p> <p>Parameters:</p> Name Type Description Default <code>return_history</code> <code>bool</code> <p>Whether to track history during generation.</p> <code>False</code> <code>decode_fn</code> <code>Optional[Callable]</code> <p>Optional custom decode function. If None, will use self.decode.</p> <code>None</code>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin.create_history","title":"<code>create_history(batch_size)</code>","text":"<p>Create empty history for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of sequences in the batch.</p> required <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Empty history list for each batch element.</p>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin.update_history_from_state","title":"<code>update_history_from_state(history, state, step, confidence_key='confidence', active_mask_key=None)</code>","text":"<p>Update history from a state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Current history list.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the current state (must be decodable).</p> required <code>step</code> <code>int</code> <p>Current step number.</p> required <code>confidence_key</code> <code>str</code> <p>Key in state dict for confidence values (default: \"confidence\").</p> <code>'confidence'</code> <code>active_mask_key</code> <code>Optional[str]</code> <p>Optional key for mask indicating which samples are still active.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Updated history.</p>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin.update_history_explicit","title":"<code>update_history_explicit(history, texts, confidences, step, active_mask=None)</code>","text":"<p>Update history with explicit values.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Current history list.</p> required <code>texts</code> <code>List[str]</code> <p>Decoded text for each batch element.</p> required <code>confidences</code> <code>Union[List[float], Tensor]</code> <p>Confidence/score for each batch element.</p> required <code>step</code> <code>int</code> <p>Current step number.</p> required <code>active_mask</code> <code>Optional[Union[List[bool], Tensor]]</code> <p>Optional mask indicating which samples are still active.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[str, float, int]]]</code> <p>Updated history.</p>"},{"location":"reference/xlm/harness/#xlm.harness.PredictorHistoryMixin.format_history_for_output","title":"<code>format_history_for_output(history, round_precision=4)</code>","text":"<p>Format history for output in to_dict methods.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>Raw history list.</p> required <code>round_precision</code> <code>int</code> <p>Number of decimal places to round confidence values.</p> <code>4</code> <p>Returns:</p> Type Description <code>List[List[List[Any]]]</code> <p>Formatted history with rounded confidence values.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness","title":"<code>Harness</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>PyTorchModelHubMixin</code></p> <p>Main module that provides the scaffolding for the codebase.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.tokenizer","title":"<code>tokenizer</code>  <code>instance-attribute</code>","text":"<p>Task Metrics usually consist of two types of metrics:     1. diagnostic metrics: These are typically different for different models as well as different tasks.     2. reported metrics: These are the same for all the models but different for different tasks. What we want too do is avoid a full blown (task x model) setup whenever we can but provide it as a last resort. The best case scenario is complete decopling. This happens when all the models adhere to the same output signature. But this never works for diagnostic metrics. In some cases, different tasks can share base metrics of both types. In these cases, we can use inheritance to avoid some code duplication. We would still have (task x model) number of classes though.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.__init__","title":"<code>__init__(cfg, tokenizer=None, datamodule=None, write_per_sample_metrics=False, **kwargs)</code>","text":"<p>Initialize the Harness module.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration dictionary.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Optional tokenizer instance.</p> <code>None</code> <code>datamodule</code> <code>Optional[BaseDataModule]</code> <p>Optional datamodule instance.</p> <code>None</code> <code>write_per_sample_metrics</code> <code>bool</code> <p>Whether to write per-sample metrics.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.setup_metrics","title":"<code>setup_metrics(cfg)</code>","text":"<p>Attache metrics as modules</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.setup_post_hoc_evaluator","title":"<code>setup_post_hoc_evaluator(cfg)</code>","text":"<p>Setup post-hoc evaluator. Can be use for tasks like molecule generation.</p> <p>The post-hoc evaluator computes metrics on logged predictions at epoch end, enabling global metric computation (e.g., diversity on full generated set).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration dictionary</p> required"},{"location":"reference/xlm/harness/#xlm.harness.Harness.create_lr_scheduler","title":"<code>create_lr_scheduler(optimizer, name, num_warmup_steps=None, fraction_warmup_steps=None, num_training_steps=None, interval='step', frequency=1, monitor='train_loss', strict=True, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a learning rate noise_schedule with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Huggingface name of the learning rate noise_schedule. https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_scheduler</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use with the noise_schedule</p> required <code>num_training_steps</code> <code>Optional[int]</code> <p>The total number of training steps.</p> <code>None</code> <code>num_warmup_steps</code> <code>Optional[int]</code> <p>The number of warmup steps.</p> <code>None</code> <code>fraction_warmup_steps</code> <code>Optional[float]</code> <p>The fraction of training steps to use for warmup.</p> <code>None</code> <code>interval</code> <code>Literal['step', 'epoch']</code> <p>The interval at which to update the learning rate.</p> <code>'step'</code> <code>frequency</code> <code>int</code> <p>The frequency of the learning rate updates.</p> <code>1</code> <code>monitor</code> <code>Optional[str]</code> <p>The metric to monitor for the learning rate noise_schedule.</p> <code>'train_loss'</code> <code>strict</code> <code>bool</code> <p>Whether to strictly follow the learning rate schedule.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the learning rate noise_schedule.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LRSchedulerWithConfig</code> <code>LRSchedulerWithConfig</code> <p>The configured learning rate scheduler.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.prepare_batch_for_prediction","title":"<code>prepare_batch_for_prediction(batch)</code>","text":"<p>We need this for some tasks even if we have task sepecific collator, mainly because we want to clone some elements of the batch useful for computing metrics. TODO: Get rid of this method by cloning in the collator itself.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.compute_loss","title":"<code>compute_loss(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Computes loss based on the dataloader name.</p> <p>For 'lm', the loss function is applied. For 'prediction', the predictor's predict_step is used.</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.compute_post_hoc_metrics","title":"<code>compute_post_hoc_metrics(split, dataloader_name, epoch, step, update_logged_predictions=True)</code>","text":"<p>Compute post-hoc metrics on logged predictions.</p> <p>Similar to compute_generative_perplexity, but for arbitrary post-hoc metrics. Loads predictions from jsonl, computes per-sample and global metrics, and logs aggregated results.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>train/val/test/predict</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader</p> required <code>epoch</code> <code>int</code> <p>Current epoch</p> required <code>step</code> <code>int</code> <p>Current step</p> required <code>update_logged_predictions</code> <code>bool</code> <p>If True, update predictions jsonl with per-sample metrics</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary of aggregated metrics, or None if no evaluator</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.extract_model_weights","title":"<code>extract_model_weights()</code>","text":"<p>Extract current model state dict.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Model state dict (self.model.state_dict())</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.save_model_weights","title":"<code>save_model_weights(path, overwrite=False)</code>","text":"<p>Save current model weights to local file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to save the model weights</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing file</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file exists and overwrite is False</p>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.load_model_weights","title":"<code>load_model_weights(path, strict=True)</code>","text":"<p>Load model weights from local file into self.model.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the model weights file</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys match</p> <code>True</code>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.load_model_from_hub","title":"<code>load_model_from_hub(repo_id, revision=None, cache_dir=None, force_download=False, token=None, strict=True, **kwargs)</code>","text":"<p>Download and load model weights from HuggingFace Hub into self.model.</p> <p>This method downloads the model weights from the hub and loads them into the existing model. It does NOT reconstruct a new Harness instance.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace Hub repository ID (e.g., \"username/model\")</p> required <code>revision</code> <code>Optional[str]</code> <p>Git revision (branch, tag, or commit)</p> <code>None</code> <code>cache_dir</code> <code>Optional[Union[str, Path]]</code> <p>Directory to cache downloaded files</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Force re-download even if cached</p> <code>False</code> <code>token</code> <code>Optional[Union[str, bool]]</code> <p>HuggingFace Hub token for private repos</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys match</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments for hf_hub_download</p> <code>{}</code>"},{"location":"reference/xlm/harness/#xlm.harness.Harness.from_checkpoint","title":"<code>from_checkpoint(checkpoint_path, cfg=None, tokenizer=None, datamodule=None, apply_ema=False, map_location='cpu', **kwargs)</code>  <code>classmethod</code>","text":"<p>Load Harness from Lightning checkpoint with optional EMA application.</p> <p>This is the ONLY method that can apply EMA weights, as it has direct access to the checkpoint file containing the EMA state.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Union[str, Path]</code> <p>Path to the Lightning checkpoint file</p> required <code>cfg</code> <code>Optional[DictConfig]</code> <p>Optional config to override checkpoint config</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Optional tokenizer instance</p> <code>None</code> <code>datamodule</code> <code>Optional[BaseDataModule]</code> <p>Optional datamodule instance</p> <code>None</code> <code>apply_ema</code> <code>bool</code> <p>Whether to apply EMA weights from checkpoint</p> <code>False</code> <code>map_location</code> <code>str</code> <p>Device to load checkpoint to</p> <code>'cpu'</code> <code>**kwargs</code> <p>Additional arguments for load_from_checkpoint</p> <code>{}</code> <p>Returns:</p> Type Description <code>Harness</code> <p>Harness instance with loaded weights (and EMA applied if requested)</p> Example"},{"location":"reference/xlm/harness/#xlm.harness.Harness.from_checkpoint--load-with-ema-weights-applied","title":"Load with EMA weights applied","text":"<p>harness = Harness.from_checkpoint(     \"checkpoint.ckpt\",     apply_ema=True,     cfg=cfg,     tokenizer=tokenizer,     datamodule=datamodule )</p>"},{"location":"reference/xlm/log_predictions/","title":"log_predictions","text":""},{"location":"reference/xlm/log_predictions/#xlm.log_predictions","title":"<code>xlm.log_predictions</code>","text":""},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.FilePredictionWriter","title":"<code>FilePredictionWriter</code>","text":"<p>               Bases: <code>_PredictionWriter</code></p> <p>Writer that outputs predictions to JSONL files.</p>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.FilePredictionWriter.__init__","title":"<code>__init__(fields_to_keep_in_output=None, file_path_='from_pl_module')</code>","text":"<p>Initialize FilePredictionWriter.</p> <p>Parameters:</p> Name Type Description Default <code>fields_to_keep_in_output</code> <code>Optional[List[str]]</code> <p>List of fields to keep in the output. If None, all fields are kept.</p> <code>None</code> <code>file_path_</code> <code>Union[Path, Literal['none', 'from_pl_module']]</code> <p>Path to the file or special values. if \"from_pl_module\", query the pl_module for the predictions_file for the step and epoch set to \"none\" to disable file writing</p> <code>'from_pl_module'</code>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.FilePredictionWriter.write","title":"<code>write(predictions, ground_truth_text, step, epoch, split, dataloader_name, pl_module, trainer)</code>","text":"<p>Write predictions to a JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dictionaries.</p> required <code>ground_truth_text</code> <code>Optional[List[str]]</code> <p>List of ground truth text strings.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>epoch</code> <code>int</code> <p>Current epoch.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name (train/val/test/predict).</p> required <code>dataloader_name</code> <code>str</code> <p>The dataloader name.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module.</p> required <code>trainer</code> <code>Optional[Trainer]</code> <p>The Lightning trainer.</p> required"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.FilePredictionWriter.read","title":"<code>read(step, epoch, split, dataloader_name, pl_module)</code>","text":"<p>Read predictions from a JSONL file.</p>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LoggerPredictionWriter","title":"<code>LoggerPredictionWriter</code>","text":"<p>               Bases: <code>_PredictionWriter</code></p> <p>Writer that outputs predictions to Lightning loggers.</p>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LoggerPredictionWriter.__init__","title":"<code>__init__(n_rows=10, logger_=None, fields_to_keep_in_output=None)</code>","text":"<p>Initialize LoggerPredictionWriter.</p> <p>Parameters:</p> Name Type Description Default <code>n_rows</code> <code>int</code> <p>Number of rows to log to the logger.</p> <code>10</code> <code>logger_</code> <code>Optional[List[Logger]]</code> <p>List of loggers to use. If None, uses pl_module.trainer.loggers.</p> <code>None</code> <code>fields_to_keep_in_output</code> <code>Optional[List[str]]</code> <p>List of fields to keep in the output. If None, all fields are kept.</p> <code>None</code>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LoggerPredictionWriter.write","title":"<code>write(predictions, ground_truth_text, step, epoch, split, dataloader_name, pl_module, trainer)</code>","text":"<p>Write predictions to Lightning loggers.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dictionaries.</p> required <code>ground_truth_text</code> <code>List[str]</code> <p>List of ground truth text strings.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>epoch</code> <code>int</code> <p>Current epoch.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name (train/val/test/predict).</p> required <code>dataloader_name</code> <code>str</code> <p>The dataloader name.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module.</p> required <code>trainer</code> <code>Optional[Trainer]</code> <p>The Lightning trainer.</p> required"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.ConsolePredictionWriter","title":"<code>ConsolePredictionWriter</code>","text":"<p>               Bases: <code>_PredictionWriter</code></p> <p>Writer that outputs predictions to console.</p>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.ConsolePredictionWriter.__init__","title":"<code>__init__(fields_to_keep_in_output=None)</code>","text":"<p>Initialize ConsolePredictionWriter.</p> <p>Parameters:</p> Name Type Description Default <code>fields_to_keep_in_output</code> <code>Optional[List[str]]</code> <p>List of fields to keep in the output. If None, all fields are kept.</p> <code>None</code>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.ConsolePredictionWriter.write","title":"<code>write(predictions, ground_truth_text, step, epoch, split, dataloader_name, pl_module, trainer)</code>","text":"<p>Write predictions to console.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dictionaries.</p> required <code>ground_truth_text</code> <code>List[str]</code> <p>List of ground truth text strings.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>epoch</code> <code>int</code> <p>Current epoch.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name (train/val/test/predict).</p> required <code>dataloader_name</code> <code>str</code> <p>The dataloader name.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module.</p> required <code>trainer</code> <code>Optional[Trainer]</code> <p>The Lightning trainer.</p> required"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LogPredictions","title":"<code>LogPredictions</code>","text":"<p>Main logging class that handles the shared pipeline and delegates to writers.</p>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LogPredictions.__init__","title":"<code>__init__(writers=None, inject_target=None, additional_fields_from_batch=None, fields_to_keep_in_output=None)</code>","text":"<p>Initialize LogPredictions.</p> <p>Parameters:</p> Name Type Description Default <code>writers</code> <code>Optional[Union[List[_PredictionWriter], List[Literal['file', 'logger', 'console']]]]</code> <p>List of prediction writers. If None, creates default writers for backward compatibility.</p> <code>None</code> <code>inject_target</code> <code>Optional[str]</code> <p>Key in batch to use as ground truth. If None, empty strings are used.</p> <code>None</code>"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LogPredictions.__call__","title":"<code>__call__(pl_module, trainer, batch, preds, split, dataloader_name)</code>","text":"<p>Log predictions using the shared pipeline and delegate to writers.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module.</p> required <code>trainer</code> <code>Optional[Trainer]</code> <p>The Lightning trainer.</p> required <code>batch</code> <code>Dict[str, Any]</code> <p>The input batch.</p> required <code>preds</code> <code>Dict[str, Any]</code> <p>The predictions.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name (train/val/test/predict).</p> required <code>dataloader_name</code> <code>str</code> <p>The dataloader name.</p> required"},{"location":"reference/xlm/log_predictions/#xlm.log_predictions.LogPredictions.read","title":"<code>read(step, epoch, split, dataloader_name, pl_module)</code>","text":"<p>Read predictions from the writers.</p>"},{"location":"reference/xlm/metrics/","title":"metrics","text":""},{"location":"reference/xlm/metrics/#xlm.metrics","title":"<code>xlm.metrics</code>","text":""},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper","title":"<code>MetricWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unified metric wrapper that works with both Lightning trainer and Fabric.</p> <p>Sends the raw <code>batch</code> and <code>loss_dict</code> output to the <code>update_fn</code> which transforms it into a dict of kwargs for the <code>metric</code>. The <code>update_fn</code> can contain task specific and model specific logic.</p> <p>For Lightning: Use the <code>log</code> method to log metrics via LightningModule. For Fabric: Use <code>compute</code> and <code>get_log_dict</code> methods for manual logging.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.full_name","title":"<code>full_name</code>  <code>property</code>","text":"<p>Get the full metric name with prefix.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.update","title":"<code>update(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update the metric with the current batch and loss_dict.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.log","title":"<code>log(pl_module, batch, metrics)</code>","text":"<p>Log the metric using Lightning's logging mechanism.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.compute","title":"<code>compute()</code>","text":"<p>Compute the current metric value. Useful for Fabric-based training.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.get_log_dict","title":"<code>get_log_dict()</code>","text":"<p>Get a dictionary with the metric name and computed value for logging. Useful for Fabric-based training.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.MetricWrapper.reset","title":"<code>reset()</code>","text":"<p>Reset the metric state.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.ExactMatch","title":"<code>ExactMatch</code>","text":"<p>               Bases: <code>MeanMetric</code></p>"},{"location":"reference/xlm/metrics/#xlm.metrics.ExactMatch.update","title":"<code>update(pred, target, pred_length=None, target_length=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>predicted tokens</p> required <code>target</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>target tokens</p> required <code>pred_length</code> <code>Optional[Integer[Tensor, ' *batch']]</code> <p>length of the predicted tokens</p> <code>None</code> <code>target_length</code> <code>Optional[Integer[Tensor, ' *batch']]</code> <p>length of the target tokens</p> <code>None</code>"},{"location":"reference/xlm/metrics/#xlm.metrics.TokenAccuracy","title":"<code>TokenAccuracy</code>","text":"<p>               Bases: <code>MeanMetric</code></p>"},{"location":"reference/xlm/metrics/#xlm.metrics.TokenAccuracy.update","title":"<code>update(pred, target, pred_mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>predicted tokens</p> required <code>target</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>target tokens</p> required <code>pred_mask</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>True for positions that predicted.</p> <code>None</code>"},{"location":"reference/xlm/metrics/#xlm.metrics.exact_match_update_fn","title":"<code>exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" *batch target_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/xlm/metrics/#xlm.metrics.seq2seq_exact_match_update_fn","title":"<code>seq2seq_exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required <p>Note: We rely on having same number right pads in target and pred, which may not be true for ARLM.</p>"},{"location":"reference/xlm/metrics/#xlm.metrics.seq2seq_token_accuracy_update_fn","title":"<code>seq2seq_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/xlm/model/","title":"model","text":""},{"location":"reference/xlm/model/#xlm.model","title":"<code>xlm.model</code>","text":""},{"location":"reference/xlm/modules/","title":"modules","text":""},{"location":"reference/xlm/modules/#xlm.modules","title":"<code>xlm.modules</code>","text":""},{"location":"reference/xlm/modules/ddit_simple/","title":"ddit_simple","text":""},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple","title":"<code>xlm.modules.ddit_simple</code>","text":""},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LayerNormAndScale","title":"<code>LayerNormAndScale</code>","text":"<p>               Bases: <code>Module</code></p> <p>Performs normalization and just scaling (no bias).</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LayerNormAndScale.__init__","title":"<code>__init__(dim, eps=1e-05)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LayerNormAndScale.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the normalized and scaled output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.TimestepEmbedder","title":"<code>TimestepEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.TimestepEmbedder.__init__","title":"<code>__init__(hidden_size, frequency_embedding_size=256, max_period=10000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer and the output of MLP.</p> required <code>frequency_embedding_size</code> <code>int</code> <p>The size of the frequency embedding layer.</p> <code>256</code>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.TimestepEmbedder.forward","title":"<code>forward(t)</code>","text":"<p>Embeds scalar timesteps into vector representations.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>A 1-D Tensor of bsz indices, one per batch element. These may be fractional.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (bsz, hidden_size) Tensor of positional embeddings.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LabelEmbedder","title":"<code>LabelEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>cond_size</code> <code>int</code> <p>The size of the conditioning input.</p> required <code>label_dropout</code> <code>Optional[float]</code> <p>The dropout rate for class labels during training.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>embedding_table</code> <code>Embedding</code> <p>The embedding table for class labels.</p> <code>num_classes</code> <code>int</code> <p>The number of classes.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LabelEmbedder.drop_labels","title":"<code>drop_labels(labels)</code>","text":"<p>Drop out class labels during training.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The modified class labels with some labels dropped by setting to the missing (last label).</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.LabelEmbedder.forward","title":"<code>forward(labels)</code>","text":"<p>Forward pass of the LabelEmbedder module.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The embedded vector representations of the class labels.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.AdaLNModulations","title":"<code>AdaLNModulations</code>","text":"<p>               Bases: <code>Module</code></p> <p>Produces the modulation parameters for AdaLN.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.AdaLNModulations.__init__","title":"<code>__init__(cond_dim, dim, num_modulation_parameters=6)</code>","text":"<p>Initializes the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>cond_dim</code> <code>int</code> <p>The dimension of the conditioning input.</p> required <code>dim</code> <code>int</code> <p>The hidden size.</p> required"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.AdaLNModulations.forward","title":"<code>forward(c)</code>","text":"<p>Forward pass of the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The conditioning input tensor.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>Tuple[torch.Tensor]: The modulation parameters for AdaLN. Each tensor has shape (bsz, 1, dim). When num_modulation_paramters=6, these tensors stand for the shift and scale parameters for the MHA and MLP layers, and the gating parameters: shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.AdaLNModulations.ada_ln_modulate","title":"<code>ada_ln_modulate(x, shift, scale)</code>  <code>staticmethod</code>","text":"<p>Applies adaLN modulation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (bsz, seq_len, dim).</p> required <code>shift</code> <code>Tensor</code> <p>The shift parameter tensor of shape (bsz, 1, dim).</p> required <code>scale</code> <code>Tensor</code> <p>The scale parameter tensor of shape (bsz, 1, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The modulated output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDiTLayer","title":"<code>DDiTLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with adaLN and gating in between.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDiTLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, d_cond=None, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>d_cond</code> <code>Optional[int]</code> <p>the dimension of the conditioning input.</p> <code>None</code> <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDiTLayer.forward","title":"<code>forward(x, c, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens.</p> required"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDiTLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDiTLayerList","title":"<code>DDiTLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDitFinalLayer","title":"<code>DDitFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.DDitFinalLayer.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/xlm/modules/ddit_simple/#xlm.modules.ddit_simple.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/","title":"ddit_simple_v2","text":""},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2","title":"<code>xlm.modules.ddit_simple_v2</code>","text":""},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LayerNormAndScale","title":"<code>LayerNormAndScale</code>","text":"<p>               Bases: <code>Module</code></p> <p>Performs normalization and just scaling (no bias).</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LayerNormAndScale.__init__","title":"<code>__init__(dim, eps=1e-05)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LayerNormAndScale.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the normalized and scaled output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.TimestepEmbedder","title":"<code>TimestepEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.TimestepEmbedder.__init__","title":"<code>__init__(hidden_size, frequency_embedding_size=256, max_period=10000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer and the output of MLP.</p> required <code>frequency_embedding_size</code> <code>int</code> <p>The size of the frequency embedding layer.</p> <code>256</code>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.TimestepEmbedder.forward","title":"<code>forward(t)</code>","text":"<p>Embeds scalar timesteps into vector representations.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>A 1-D Tensor of bsz indices, one per batch element. These may be fractional.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An (bsz, hidden_size) Tensor of positional embeddings.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LabelEmbedder","title":"<code>LabelEmbedder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>cond_size</code> <code>int</code> <p>The size of the conditioning input.</p> required <code>label_dropout</code> <code>Optional[float]</code> <p>The dropout rate for class labels during training.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>embedding_table</code> <code>Embedding</code> <p>The embedding table for class labels.</p> <code>num_classes</code> <code>int</code> <p>The number of classes.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LabelEmbedder.drop_labels","title":"<code>drop_labels(labels)</code>","text":"<p>Drop out class labels during training.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The modified class labels with some labels dropped by setting to the missing (last label).</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.LabelEmbedder.forward","title":"<code>forward(labels)</code>","text":"<p>Forward pass of the LabelEmbedder module.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>The input tensor of class labels of shape (bsz,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The embedded vector representations of the class labels.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.AdaLNModulations","title":"<code>AdaLNModulations</code>","text":"<p>               Bases: <code>Module</code></p> <p>Produces the modulation parameters for AdaLN.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.AdaLNModulations.__init__","title":"<code>__init__(cond_dim, dim, num_modulation_parameters=6)</code>","text":"<p>Initializes the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>cond_dim</code> <code>int</code> <p>The dimension of the conditioning input.</p> required <code>dim</code> <code>int</code> <p>The hidden size.</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.AdaLNModulations.forward","title":"<code>forward(c)</code>","text":"<p>Forward pass of the AdaLNModulations module.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Tensor</code> <p>The conditioning input tensor.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>Tuple[torch.Tensor]: The modulation parameters for AdaLN. Each tensor has shape (bsz, 1, dim). When num_modulation_paramters=6, these tensors stand for the shift and scale parameters for the MHA and MLP layers, and the gating parameters: shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.AdaLNModulations.ada_ln_modulate","title":"<code>ada_ln_modulate(x, shift, scale)</code>  <code>staticmethod</code>","text":"<p>Applies adaLN modulation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (bsz, seq_len, dim).</p> required <code>shift</code> <code>Tensor</code> <p>The shift parameter tensor of shape (bsz, 1, dim).</p> required <code>scale</code> <code>Tensor</code> <p>The scale parameter tensor of shape (bsz, 1, dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The modulated output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDiTLayer","title":"<code>DDiTLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with adaLN and gating in between.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDiTLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, d_cond=None, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>d_cond</code> <code>Optional[int]</code> <p>the dimension of the conditioning input.</p> <code>None</code> <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDiTLayer.forward","title":"<code>forward(x, c, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens.</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDiTLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDiTLayerList","title":"<code>DDiTLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayer","title":"<code>DDitFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayer.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayerWithoutNormalization","title":"<code>DDitFinalLayerWithoutNormalization</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayerWithoutNormalization.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayerForClassification","title":"<code>DDitFinalLayerForClassification</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.DDitFinalLayerForClassification.forward","title":"<code>forward(x, c)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>c</code> <code>Tensor</code> <p>the conditioning input of shape (bsz, cond_dim).</p> required"},{"location":"reference/xlm/modules/ddit_simple_v2/#xlm.modules.ddit_simple_v2.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/modules/encoder/","title":"encoder","text":""},{"location":"reference/xlm/modules/encoder/#xlm.modules.encoder","title":"<code>xlm.modules.encoder</code>","text":""},{"location":"reference/xlm/modules/encoder/#xlm.modules.encoder.DiffusionTransformerEncoderLayer","title":"<code>DiffusionTransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/encoder/#xlm.modules.encoder.DiffusionTransformerEncoderLayer.forward","title":"<code>forward(src, src_mask=None, src_key_padding_mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src</code> <p>shape (B, T, d_model) for now we will assume T=query_seq_len=key_seq_len</p> required <code>src_mask</code> <p>shape (B*num_heads, T, T) or (T, T). True is attend, False is not attend Note that nn.TransformerEncoderLayer will allow float masks which are added to the attention scores. But we only support boolean masks here.</p> <code>None</code> <code>src_key_padding_mask</code> <p>shape (B, T) or (T). True is attend, False is masked</p> <code>None</code>"},{"location":"reference/xlm/modules/encoder/#xlm.modules.encoder.DiffusionTransformerEncoder","title":"<code>DiffusionTransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/encoder/#xlm.modules.encoder.DiffusionTransformerEncoder.forward","title":"<code>forward(src_tokens, src_lengths)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src_tokens</code> <code>Tensor</code> <p>shape (B, T)</p> required <code>src_lengths</code> <code>Optional[Tensor]</code> <p>shape (B) of type LongTensor</p> required"},{"location":"reference/xlm/modules/gpt2_transformer/","title":"gpt2_transformer","text":""},{"location":"reference/xlm/modules/gpt2_transformer/#xlm.modules.gpt2_transformer","title":"<code>xlm.modules.gpt2_transformer</code>","text":"<p>Adapted from MINGPT: https://github.com/karpathy/nanoGPT/blob/master/model.py</p>"},{"location":"reference/xlm/modules/gpt2_transformer/#xlm.modules.gpt2_transformer.LayerNorm","title":"<code>LayerNorm</code>","text":"<p>               Bases: <code>Module</code></p> <p>LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False</p>"},{"location":"reference/xlm/modules/gpt2_transformer/#xlm.modules.gpt2_transformer.GPT","title":"<code>GPT</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/gpt2_transformer/#xlm.modules.gpt2_transformer.GPT.get_num_params","title":"<code>get_num_params(non_embedding=True)</code>","text":"<p>Return the number of parameters in the model. For non-embedding count (default), the position embeddings get subtracted. The token embeddings would too, except due to the parameter sharing these params are actually used as weights in the final layer, so we include them.</p>"},{"location":"reference/xlm/modules/gpt2_transformer/#xlm.modules.gpt2_transformer.GPT.estimate_mfu","title":"<code>estimate_mfu(fwdbwd_per_iter, dt)</code>","text":"<p>estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS</p>"},{"location":"reference/xlm/modules/position/","title":"position","text":""},{"location":"reference/xlm/modules/position/#xlm.modules.position","title":"<code>xlm.modules.position</code>","text":""},{"location":"reference/xlm/modules/position/#xlm.modules.position.RotaryEmbedding","title":"<code>RotaryEmbedding</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/xlm/modules/position/#xlm.modules.position.RotaryEmbedding.__init__","title":"<code>__init__(dim, head_first=True, cache_size=1024)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the dimension of the input.</p> required <code>head_first</code> <code>bool</code> <p>if True, the input is assumed to be of shape (batch_size, seq_len, num_heads, dim)         if False, the input is assumed to be of shape (batch_size, num_heads, seq_len, dim)</p> <code>True</code> <code>cache_size</code> <code>int</code> <p>the maximum sequence length to cache the sine and cosine values for.</p> <code>1024</code>"},{"location":"reference/xlm/modules/position/#xlm.modules.position.RotaryEmbedding.forward","title":"<code>forward(x, positions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>shape (batch_size, seq_len, num_heads, dim) if head_first is False shape (batch_size, num_heads, seq_len, dim) if head_first is True</p> required <code>positions</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>shape (batch_size, seq_len)</p> required"},{"location":"reference/xlm/modules/rotary_transformer/","title":"rotary_transformer","text":""},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer","title":"<code>xlm.modules.rotary_transformer</code>","text":"<p>Modules for simple transformer decoder that uses rotary embeddings for positional encoding.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerLayer","title":"<code>RotaryTransformerLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>One layer of DDiT.</p> <p>It consists of a multi-head self-attention layer followed by a feedforward layer with normalization and gating in between.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerLayer.__init__","title":"<code>__init__(d_model, nhead, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, force_flash_attn=False)</code>","text":"<p>Initialize the DDiTBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the dimension of the input.</p> required <code>nhead</code> <code>int</code> <p>the number of attention heads.</p> required <code>mlp_ratio</code> <p>the ratio of the hidden size of the MLP/feedforward layer to the input size.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate.</p> <code>0.1</code>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerLayer.forward","title":"<code>forward(inp, attention_mask, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required <code>attention_mask</code> <code>Tensor</code> <p>the attention mask of shape (bsz, seq_len), which is True for non-padding tokens. It can also be of shape (bsz, seq_len (query), seq_len (key-value)), where the mask indicates which tokens are valid in the context.</p> required"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerLayer.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(x, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>the input tensor of shape (batch_size, seq_len, num_heads, dim).</p> required <p>Returns:</p> Type Description <p>The tensor with rotary position embeddings applied to the first dim/2 of the last dimension.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerLayerList","title":"<code>RotaryTransformerLayerList</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>A module list of DDiT blocks that share the rotary cache for the rotary embeddings.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayer","title":"<code>RotaryTransformerFinalLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple unembedding layer with optional layer norm.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayer.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayerForClassification","title":"<code>RotaryTransformerFinalLayerForClassification</code>","text":"<p>               Bases: <code>Module</code></p> <p>Feedforward layer with pre-norm and residual connection followed by a linear layer for classification.</p>"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.RotaryTransformerFinalLayerForClassification.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor of shape (bsz, seq_len, dim).</p> required"},{"location":"reference/xlm/modules/rotary_transformer/#xlm.modules.rotary_transformer.add_bias_apply_dropout_scale","title":"<code>add_bias_apply_dropout_scale(x, bias=None, dropout=0.0, scale=None, residual=None, training=True)</code>","text":"<p>Adds bias, applies dropout, scales, and adds residual.</p> <p>TODO: Consider creating fused implementation using jit and two wrappers Args:     x: The input tensor of shape (bsz, seq_len, dim).     bias: The bias tensor of shape (bsz, 1, dim).     dropout: The dropout rate.     scale: The scale tensor of shape (bsz, 1, dim).     residual: The residual tensor of shape (bsz, seq_len, dim).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of shape (bsz, seq_len, dim).</p>"},{"location":"reference/xlm/noise/","title":"noise","text":""},{"location":"reference/xlm/noise/#xlm.noise","title":"<code>xlm.noise</code>","text":""},{"location":"reference/xlm/tasks/","title":"tasks","text":""},{"location":"reference/xlm/tasks/#xlm.tasks","title":"<code>xlm.tasks</code>","text":""},{"location":"reference/xlm/tasks/lm1b/","title":"lm1b","text":""},{"location":"reference/xlm/tasks/lm1b/#xlm.tasks.lm1b","title":"<code>xlm.tasks.lm1b</code>","text":""},{"location":"reference/xlm/tasks/lm1b/#xlm.tasks.lm1b.detokenizer_lm1b_string","title":"<code>detokenizer_lm1b_string(x)</code>","text":"<p>Detokenizer for the LM1B dataset. Same as the one used in SEDD and MDLM.</p>"},{"location":"reference/xlm/tasks/molgen/","title":"molgen","text":""},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen","title":"<code>xlm.tasks.molgen</code>","text":"<p>Molecule generation task utilities and metrics.</p> <p>This module provides: - Data preprocessing for SAFE molecular representations - Conversion utilities between SAFE and SMILES formats - Comprehensive metrics for evaluating molecular generation (diversity, QED, SA, validity, uniqueness)</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.SerializableSAFETokenizer","title":"<code>SerializableSAFETokenizer</code>","text":"<p>Wrapper around SAFE tokenizer that handles pickling/deepcopy.</p> <p>The underlying tokenizer from the safe library has a custom PreTokenizer that cannot be serialized. This wrapper provides dummy serialization by storing the model path and re-instantiating the tokenizer on unpickle.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.SerializableSAFETokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return state for pickling - only store the model path.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.SerializableSAFETokenizer.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickled state - re-instantiate tokenizer.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.SerializableSAFETokenizer.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate all attribute access to the underlying tokenizer.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.SerializableSAFETokenizer.__dir__","title":"<code>__dir__()</code>","text":"<p>Include both wrapper and tokenizer attributes in dir().</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval","title":"<code>DeNovoEval</code>","text":"<p>Post-hoc evaluator for de novo molecule generation.</p> <p>Computes molecular properties on logged predictions at epoch end, matching GenMol's evaluation semantics. Computes: - Per-sample: QED, SA, SMILES (added to each prediction dict) - Global: Diversity, Validity, Uniqueness (aggregated across all samples)</p> <p>This approach enables: - Global metric computation (diversity/uniqueness on full generated set) - Exact match with GenMol's evaluation methodology - Reusable components for other tasks (frag, lead, pmo)</p> <p>Parameters:</p> Name Type Description Default <code>use_bracket_safe</code> <code>bool</code> <p>If True, decode from bracket SAFE format</p> <code>False</code> <code>compute_diversity</code> <code>bool</code> <p>If True, compute diversity metric</p> <code>True</code> <code>compute_validity</code> <code>bool</code> <p>If True, compute validity metric</p> <code>True</code> <code>compute_uniqueness</code> <code>bool</code> <p>If True, compute uniqueness metric</p> <code>True</code> <code>compute_qed</code> <code>bool</code> <p>If True, compute QED scores</p> <code>True</code> <code>compute_sa</code> <code>bool</code> <p>If True, compute SA scores</p> <code>True</code>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.oracle_qed","title":"<code>oracle_qed</code>  <code>property</code>","text":"<p>Lazy load QED oracle.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.oracle_sa","title":"<code>oracle_sa</code>  <code>property</code>","text":"<p>Lazy load SA oracle.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.evaluator_diversity","title":"<code>evaluator_diversity</code>  <code>property</code>","text":"<p>Lazy load diversity evaluator.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.evaluator_validity","title":"<code>evaluator_validity</code>  <code>property</code>","text":"<p>Lazy load validity evaluator.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.evaluator_uniqueness","title":"<code>evaluator_uniqueness</code>  <code>property</code>","text":"<p>Lazy load uniqueness evaluator.</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.DeNovoEval.eval","title":"<code>eval(predictions, tokenizer=None)</code>","text":"<p>Evaluate predictions and return updated predictions + aggregated metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dicts with 'text' field containing SAFE strings</p> required <code>tokenizer</code> <code>Any</code> <p>Optional tokenizer (not used for denovo, but kept for interface consistency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple of:</p> <code>Dict[str, Any]</code> <ul> <li>predictions: Updated list with per-sample metrics added (smiles, qed, sa)</li> </ul> <code>Tuple[List[Dict[str, Any]], Dict[str, Any]]</code> <ul> <li>aggregated_metrics: Dict of global metric values</li> </ul>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.FragmentEval","title":"<code>FragmentEval</code>","text":"<p>               Bases: <code>DeNovoEval</code></p> <p>Post-hoc evaluator for fragment-constrained molecule generation.</p> <p>Extends DeNovoEval with fragment-specific metrics: - All de novo metrics (validity, uniqueness, quality, QED, SA, diversity) - Distance: Tanimoto distance between generated and target molecules</p> <p>Based on GenMol's fragment evaluation methodology. Computes: - Per-sample: QED, SA, SMILES, distance (if target available) - Global: Diversity, Validity, Uniqueness, Quality, Distance (mean)</p> <p>Parameters:</p> Name Type Description Default <code>use_bracket_safe</code> <code>bool</code> <p>If True, decode from bracket SAFE format</p> <code>False</code> <code>compute_diversity</code> <code>bool</code> <p>If True, compute diversity metric</p> <code>True</code> <code>compute_validity</code> <code>bool</code> <p>If True, compute validity metric</p> <code>True</code> <code>compute_uniqueness</code> <code>bool</code> <p>If True, compute uniqueness metric</p> <code>True</code> <code>compute_qed</code> <code>bool</code> <p>If True, compute QED scores</p> <code>True</code> <code>compute_sa</code> <code>bool</code> <p>If True, compute SA scores</p> <code>True</code> <code>compute_distance</code> <code>bool</code> <p>If True, compute Tanimoto distance to target</p> <code>True</code>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.FragmentEval.eval","title":"<code>eval(predictions, tokenizer=None)</code>","text":"<p>Evaluate fragment generation predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Dict[str, Any]]</code> <p>List of prediction dicts with: - 'text': Generated SAFE string (full molecule) - 'truth': Target SAFE string (full molecule, optional) - 'raw_input': Fragment prompt SAFE string (optional)</p> required <code>tokenizer</code> <code>Any</code> <p>Optional tokenizer (not used, kept for interface consistency)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple of:</p> <code>Dict[str, Any]</code> <ul> <li>predictions: Updated list with per-sample metrics added</li> </ul> <code>Tuple[List[Dict[str, Any]], Dict[str, Any]]</code> <ul> <li>aggregated_metrics: Dict of global metric values</li> </ul>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.safe2bracketsafe","title":"<code>safe2bracketsafe(safe_str)</code>","text":"<p>Convert standard SAFE notation to bracket SAFE format.</p> <p>Bracket SAFE wraps interfragment attachment points in angle brackets. Example: \"1\" -&gt; \"&lt;1&gt;\", \"%10\" -&gt; \"&lt;%10&gt;\"</p> <p>Based on genmol/src/genmol/utils/bracket_safe_converter.py:133-137</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE string in standard notation</p> required <p>Returns:</p> Type Description <code>str</code> <p>SAFE string in bracket notation, or original string if conversion fails</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.bracketsafe2safe","title":"<code>bracketsafe2safe(safe_str)</code>","text":"<p>Convert bracket SAFE notation back to standard SAFE format.</p> <p>Removes angle brackets from interfragment attachment points and renumbers them to avoid conflicts with intrafragment attachment points.</p> <p>Based on genmol/src/genmol/utils/bracket_safe_converter.py:140-153</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE string in bracket notation</p> required <p>Returns:</p> Type Description <code>str</code> <p>SAFE string in standard notation</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.safe_to_smiles","title":"<code>safe_to_smiles(safe_str, fix=True)</code>","text":"<p>Convert SAFE string to SMILES using safe library.</p> <p>Based on genmol/src/genmol/utils/utils_chem.py:26-30</p> <p>Parameters:</p> Name Type Description Default <code>safe_str</code> <code>str</code> <p>SAFE molecular representation</p> required <code>fix</code> <code>bool</code> <p>If True, filter out invalid fragments before decoding</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>SMILES string, or None if conversion fails</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.safe_strings_to_smiles","title":"<code>safe_strings_to_smiles(safe_strings, use_bracket_safe=False, fix=True)</code>","text":"<p>Convert batch of SAFE strings to SMILES strings.</p> <p>Based on genmol/src/genmol/sampler.py:81-89</p> <p>Parameters:</p> Name Type Description Default <code>safe_strings</code> <code>List[str]</code> <p>List of SAFE molecular representations</p> required <code>use_bracket_safe</code> <code>bool</code> <p>If True, convert from bracket SAFE first</p> <code>False</code> <code>fix</code> <code>bool</code> <p>If True, filter invalid fragments</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of SMILES strings (invalid conversions are skipped)</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.safe_bracket_on_the_fly_processor_combined","title":"<code>safe_bracket_on_the_fly_processor_combined(example, tokenizer, block_size=None)</code>","text":"<p>Works directly on the raw strings</p>"},{"location":"reference/xlm/tasks/molgen/#xlm.tasks.molgen.genmol_fragment_preprocess_fn","title":"<code>genmol_fragment_preprocess_fn(example, tokenizer, *, fragment_column='linker_design')</code>","text":"<p>Preprocess GenMol fragment CSV data for fragment-constrained generation.</p> <p>Converts SMILES fragments and targets to SAFE format, then to bracket SAFE, and creates prompt_token_ids (fragment) and input_token_ids (full molecule).</p> <p>Based on GenMol's fragment evaluation dataset structure: - Input: Fragment SMILES (from <code>fragment_column</code>, default 'linker_design') - Target: Full molecule SMILES (from 'smiles' column)</p> <p>To use a different fragment column, pass <code>fragment_column</code> via <code>preprocess_function_kwargs</code> in the dataset config, or set <code>_fragment_column</code> in the example dict (overrides kwarg).</p> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>Dict[str, Any]</code> <p>Dataset example containing: - column named by <code>fragment_column</code>: SMILES with [n*] attachment points - 'smiles': Full target molecule SMILES - '_fragment_column' (optional): Overrides <code>fragment_column</code> if set</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer for encoding</p> required <code>fragment_column</code> <code>str</code> <p>CSV column to use as fragment input (default: 'linker_design'). Override via datamodule ... preprocess_function_kwargs.fragment_column.</p> <code>'linker_design'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Example with 'prompt_token_ids' (fragment) and 'input_token_ids' (full molecule)</p>"},{"location":"reference/xlm/tasks/owt/","title":"owt","text":""},{"location":"reference/xlm/tasks/owt/#xlm.tasks.owt","title":"<code>xlm.tasks.owt</code>","text":""},{"location":"reference/xlm/tasks/star/","title":"star","text":""},{"location":"reference/xlm/tasks/star/#xlm.tasks.star","title":"<code>xlm.tasks.star</code>","text":""},{"location":"reference/xlm/tasks/star/#xlm.tasks.star.simple_star_graph","title":"<code>simple_star_graph(degree=3, pathlength=5, vocab_size=20)</code>","text":"<p>Generate a asymmetric star graph.</p>"},{"location":"reference/xlm/tasks/star/#xlm.tasks.star.simple_star_graph--adapted-from-httpsgithubcomgregorbachmannnext-token-failuresblobmaindatagraphspy","title":"Adapted from : https://github.com/gregorbachmann/Next-Token-Failures/blob/main/data/graphs.py","text":"Properties <ul> <li>The source is always the center of the star.</li> <li>The goal is always the end of one of the arms of the star.</li> <li>No nodes are repeated in the graph.     So pathlength * degree + 1  should be greater than or equal to vocab_size otherwise the     code will loop indefinitely.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>int, number of edges from source (center of the start) to other nodes</p> <code>3</code> <code>pathlength</code> <code>int</code> <p>int, length of the path (in terms of number of nodes)</p> <code>5</code> <code>vocab_size</code> <code>int</code> <p>int, number of nodes in the graph</p> <code>20</code> <p>Returns:     path: list, path from source to goal     edge_list: list, list of edges in the graph</p>"},{"location":"reference/xlm/tasks/star/#xlm.tasks.star.plot_graph","title":"<code>plot_graph(edge_list, path=None, source=None, goal=None)</code>","text":"<p>Example usage: graph = simple_star_graph(4, 4, 20) plot_graph(graph[\"edge_list\"], graph[\"path\"], graph[\"source\"], graph[\"goal\"])</p>"},{"location":"reference/xlm/tasks/star_old/","title":"star_old","text":""},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old","title":"<code>xlm.tasks.star_old</code>","text":""},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old.StarGraphIDLMCollator","title":"<code>StarGraphIDLMCollator</code>","text":"<p>               Bases: <code>DefaultIDLMCollator</code></p> <p>Adds the <code>constraint</code>  using the <code>token_type_ids</code></p>"},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old.StarGraphDataModule","title":"<code>StarGraphDataModule</code>","text":"<p>               Bases: <code>BaseDataModule</code></p>"},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old.simple_star_graph","title":"<code>simple_star_graph(degree=3, pathlength=5, vocab_size=20)</code>","text":"<p>Generate a asymmetric star graph.</p>"},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old.simple_star_graph--adapted-from-httpsgithubcomgregorbachmannnext-token-failuresblobmaindatagraphspy","title":"Adapted from : https://github.com/gregorbachmann/Next-Token-Failures/blob/main/data/graphs.py","text":"Properties <ul> <li>The source is always the center of the star.</li> <li>The goal is always the end of one of the arms of the star.</li> <li>No nodes are repeated in the graph.     So pathlength * degree + 1  should be greater than or equal to vocab_size otherwise the     code will loop indefinitely.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>int, number of edges from source (center of the start) to other nodes</p> <code>3</code> <code>pathlength</code> <code>int</code> <p>int, length of the path (in terms of number of nodes)</p> <code>5</code> <code>vocab_size</code> <code>int</code> <p>int, number of nodes in the graph</p> <code>20</code> <p>Returns:     path: list, path from source to goal     edge_list: list, list of edges in the graph</p>"},{"location":"reference/xlm/tasks/star_old/#xlm.tasks.star_old.plot_graph","title":"<code>plot_graph(edge_list, path=None, source=None, goal=None)</code>","text":"<p>Example usage: graph = simple_star_graph(4, 4, 20) plot_graph(graph[\"edge_list\"], graph[\"path\"], graph[\"source\"], graph[\"goal\"])</p>"},{"location":"reference/xlm/tasks/sudoku/","title":"sudoku","text":""},{"location":"reference/xlm/tasks/sudoku/#xlm.tasks.sudoku","title":"<code>xlm.tasks.sudoku</code>","text":""},{"location":"reference/xlm/tasks/sudoku_extreme/","title":"sudoku_extreme","text":""},{"location":"reference/xlm/tasks/sudoku_extreme/#xlm.tasks.sudoku_extreme","title":"<code>xlm.tasks.sudoku_extreme</code>","text":"<p>Preprocessing for brozonoyer/sapientinc-sudoku-extreme-timvink-sudoku-solver.</p> <p>Dataset has \"question\" (puzzle, \".\" for blanks) and \"answer\" (solution). We convert \".\" -&gt; \"0\" to match the tokenizer convention (vocab 0-9) and produce input_token_ids / prompt_token_ids like the standard sudoku task.</p>"},{"location":"reference/xlm/tasks/sudoku_extreme/#xlm.tasks.sudoku_extreme.sudoku_extreme_preprocess_fn","title":"<code>sudoku_extreme_preprocess_fn(example, tokenizer)</code>","text":"<p>Preprocess sapientinc-sudoku-extreme examples.</p> <p>Uses \"question\" (puzzle) and \"answer\" (solution). Blanks are \".\" in the dataset; we convert to \"0\" before tokenizing.</p> <p>Also processes \"trajectory\" field which contains a list of strings representing step-by-step board configurations from question to solution.</p>"},{"location":"reference/xlm/utils/","title":"utils","text":""},{"location":"reference/xlm/utils/#xlm.utils","title":"<code>xlm.utils</code>","text":""},{"location":"reference/xlm/utils/batch_size/","title":"batch_size","text":""},{"location":"reference/xlm/utils/batch_size/#xlm.utils.batch_size","title":"<code>xlm.utils.batch_size</code>","text":""},{"location":"reference/xlm/utils/checkpoint_with_thinning/","title":"checkpoint_with_thinning","text":""},{"location":"reference/xlm/utils/checkpoint_with_thinning/#xlm.utils.checkpoint_with_thinning","title":"<code>xlm.utils.checkpoint_with_thinning</code>","text":""},{"location":"reference/xlm/utils/checkpoint_with_thinning/#xlm.utils.checkpoint_with_thinning.ThinningCheckpoint","title":"<code>ThinningCheckpoint</code>","text":"<p>               Bases: <code>ModelCheckpoint</code></p> <p>Checkpoint callback that saves after every N steps and keeps checkpoints at K*N step intervals.</p> <p>Parameters:</p> Name Type Description Default <code>keep_multiple</code> <code>int</code> <p>Keep checkpoints for every keep_multiple * every_n_train_steps</p> <code>1</code> <code>dirpath</code> <code>Optional[Union[Path, str]]</code> <p>directory to save the model file</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>checkpoint filename. Must contain {step} in the pattern</p> <code>None</code> <code>every_n_train_steps</code> <code>Optional[int]</code> <p>Number of training steps between checkpoints</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to ModelCheckpoint</p> required <p>Example::     &gt;&gt;&gt; callback = StepBasedCheckpoint(     ...     dirpath='checkpoints',     ...     filename='model-{epoch}-{step}',     ...     every_n_train_steps=1000,     ...     keep_multiple=10     ... )     # This will:     # - Save a checkpoint every 1000 steps     # - Keep checkpoints at steps 10000, 20000, etc.     # - Delete intermediate checkpoints</p>"},{"location":"reference/xlm/utils/data/","title":"data","text":""},{"location":"reference/xlm/utils/data/#xlm.utils.data","title":"<code>xlm.utils.data</code>","text":""},{"location":"reference/xlm/utils/ddp/","title":"ddp","text":""},{"location":"reference/xlm/utils/ddp/#xlm.utils.ddp","title":"<code>xlm.utils.ddp</code>","text":""},{"location":"reference/xlm/utils/debug/","title":"debug","text":""},{"location":"reference/xlm/utils/debug/#xlm.utils.debug","title":"<code>xlm.utils.debug</code>","text":""},{"location":"reference/xlm/utils/ema/","title":"ema","text":""},{"location":"reference/xlm/utils/ema/#xlm.utils.ema","title":"<code>xlm.utils.ema</code>","text":""},{"location":"reference/xlm/utils/ema/#xlm.utils.ema.EMACallback","title":"<code>EMACallback</code>","text":"<p>               Bases: <code>Callback</code></p>"},{"location":"reference/xlm/utils/ema/#xlm.utils.ema.EMACallback.__init__","title":"<code>__init__(decay, use_num_updates=True, apply_ema_at_train_end=True)</code>","text":"<p>decay: The exponential decay. use_num_updates: Whether to use number of updates when computing     averages. apply_ema_at_train_end: If True, applies EMA weights to the model     at the end of training, so the final checkpoint contains     EMA-averaged weights in the model parameters.</p>"},{"location":"reference/xlm/utils/ema/#xlm.utils.ema.EMACallback.on_train_end","title":"<code>on_train_end(trainer, pl_module)</code>","text":"<p>Apply EMA weights to the model at the end of training.</p> <p>This makes the final checkpoint contain EMA-averaged weights directly in the model parameters, eliminating the need for manual EMA application during checkpoint extraction or inference.</p> <p>Note: After this is called, the model's parameters will contain EMA-averaged weights, and the original training weights will be lost (though they're still stored in collected_params if store() was called).</p>"},{"location":"reference/xlm/utils/file_friendly_progress_callback/","title":"file_friendly_progress_callback","text":""},{"location":"reference/xlm/utils/file_friendly_progress_callback/#xlm.utils.file_friendly_progress_callback","title":"<code>xlm.utils.file_friendly_progress_callback</code>","text":""},{"location":"reference/xlm/utils/file_friendly_progress_callback/#xlm.utils.file_friendly_progress_callback.FileFriendlyTQDMProgressBar","title":"<code>FileFriendlyTQDMProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p>"},{"location":"reference/xlm/utils/file_friendly_progress_callback/#xlm.utils.file_friendly_progress_callback.FileFriendlyTQDMProgressBar.init_validation_tqdm","title":"<code>init_validation_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for validation.</p>"},{"location":"reference/xlm/utils/file_friendly_progress_callback/#xlm.utils.file_friendly_progress_callback.FileFriendlyTQDMProgressBar.init_test_tqdm","title":"<code>init_test_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for testing.</p>"},{"location":"reference/xlm/utils/file_friendly_progress_callback/#xlm.utils.file_friendly_progress_callback.replace_cr_with_newline","title":"<code>replace_cr_with_newline(message)</code>","text":"<p>TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line.</p>"},{"location":"reference/xlm/utils/imports/","title":"imports","text":""},{"location":"reference/xlm/utils/imports/#xlm.utils.imports","title":"<code>xlm.utils.imports</code>","text":""},{"location":"reference/xlm/utils/imports/#xlm.utils.imports.get_function","title":"<code>get_function(path)</code>  <code>cached</code>","text":"<p>Quicky way to import a function from a string.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the function.</p> required <p>Returns:</p> Type Description <p>The function.</p> Example <p>get_function(\"xlm.datamodule.ids_to_example_fn\")  Limitations <ol> <li>Must include at least one \u201c.\u201d</li> </ol> <p><code>get_function(\"foo\")        # \u2192 ValueError: not enough values to unpack</code> You need a dot to split module vs. attribute.</p> <ol> <li> <p>No nested\u2010attribute support     It only splits on the last dot. If you wrote \"pkg.mod.Class.method\", you\u2019d get pkg.mod.Class as the module and then .method on that, which gives you an unbound function, not a bound method of an instance.</p> </li> <li> <p>Import failures     If the module name is wrong or missing from your PYTHONPATH, you\u2019ll get an ImportError or ModuleNotFoundError at lookup time.</p> </li> <li> <p>Attribute errors     If the attribute isn\u2019t on the module (typo, private name, etc.), you\u2019ll see an AttributeError. You might want to catch and rewrap these with a friendlier message.</p> </li> <li> <p>Dynamic\u2010reload blind spot     Because it caches the object, if you later hot-reload the module or monkey-patch it, get_function will keep returning the old reference. (You can clear the cache with get_function.cache_clear() if you really need to.)</p> </li> <li> <p>Built-ins &amp; extensions     For truly built-in callables (e.g. len, open) you\u2019d have to use \"builtins.len\" or \"io.open\"\u2014plain \"len\" won\u2019t work.</p> </li> </ol>"},{"location":"reference/xlm/utils/nn/","title":"nn","text":""},{"location":"reference/xlm/utils/nn/#xlm.utils.nn","title":"<code>xlm.utils.nn</code>","text":""},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.tiny_value_of_dtype","title":"<code>tiny_value_of_dtype(dtype)</code>","text":"<p>Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical issues such as division by zero. This is different from <code>info_value_of_dtype(dtype).tiny</code> because it causes some NaN bugs. Only supports floating point dtypes.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_mean","title":"<code>masked_mean(vector, mask, dim, keepdim=False)</code>","text":"<p>To calculate mean along certain dimensions on masked values</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_mean--parameters","title":"Parameters","text":"<code>torch.Tensor</code> <p>The vector to calculate mean.</p> <p>mask : <code>torch.BoolTensor</code>     The mask of the vector. It must be broadcastable with vector.     It must be 1 for non-masked values and 0 for masked values. dim : <code>int</code>     The dimension to calculate mean keepdim : <code>bool</code>     Whether to keep dimension</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_mean--returns","title":"Returns","text":"<p><code>torch.Tensor</code>     A <code>torch.Tensor</code> of including the mean values.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_sum","title":"<code>masked_sum(vector, mask, dim, keepdim=False)</code>","text":"<p>To calculate sum along certain dimensions on masked values</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_sum--parameters","title":"Parameters","text":"<code>torch.Tensor</code> <p>The vector to calculate sum.</p> <p>mask : <code>torch.BoolTensor</code>     The mask of the vector. It must be broadcastable with vector. It must be 1 for non-masked values and 0 for masked out values. dim : <code>int</code>     The dimension to calculate sum keepdim : <code>bool</code>     Whether to keep dimension</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.masked_sum--returns","title":"Returns","text":"<p><code>torch.Tensor</code>     A <code>torch.Tensor</code> of including the sum values.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.get_mask_from_sequence_lengths","title":"<code>get_mask_from_sequence_lengths(sequence_lengths, max_length)</code>","text":"<p>Given a variable of shape <code>(batch_size,)</code> that represents the sequence lengths of each batch element, this function returns a <code>(batch_size, max_length)</code> mask variable.  For example, if our input was <code>[2, 2, 3]</code>, with a <code>max_length</code> of 4, we'd return <code>[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]]</code>.</p> <p>We require <code>max_length</code> here instead of just computing it from the input <code>sequence_lengths</code> because it lets us avoid finding the max, then copying that value from the GPU to the CPU so that we can use it to construct a new tensor.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.dtype","title":"<code>dtype(string)</code>","text":"<p>Convert a string to a PyTorch data type.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.dtype--parameters","title":"Parameters","text":"<code>str</code> <p>The string to convert.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.dtype--returns","title":"Returns","text":"<p><code>torch.dtype</code>     The PyTorch data type.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.sample_categorical","title":"<code>sample_categorical(probs)</code>","text":"<p>Need this since torch.multinomial does not accept 3D input and cannot handle unnormalized probabilities.</p> <p>So we implement the \"exponential race method\" manually which can handle any number of leading dimensions and can handle unnormalized probabilities (not logits, )</p> <p>Note: This is not differentiable.. Use gumbel softmax for it. Args:     probs: (batch, seq_len, vocab_size) can have any number of leading dimensions.         Note: probs should be positive, can be unnormalized. Returns:     (batch, seq_len)</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.sample_from_logits","title":"<code>sample_from_logits(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Sample from logits using the Gumbel-Max trick. Similar to sample_categorical, but works with logits (real valued). Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions. Returns:     (batch, seq_len)</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.sample_from_top_k","title":"<code>sample_from_top_k(k, logits)</code>","text":"<p>Sample from the top-k logits using the Gumbel-Max trick. Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions.     k: The number of top logits to consider for sampling. Returns:     (batch, seq_len)</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.sample_from_top_p","title":"<code>sample_from_top_p(p, logits)</code>","text":"<p>Sample from the top-p logits using the Gumbel-Max trick.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>The cumulative probability threshold. Must be between 0 and 1.</p> required <code>logits</code> <code>Tensor</code> <p>A tensor of shape (*batch, seq_len, vocab_size) representing                    the unnormalized log probabilities for each token.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of shape (*batch, seq_len) containing the sampled token indices.</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.add_gumbel_noise","title":"<code>add_gumbel_noise(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Add gumbel noise to logits which will result in samples from the distribtution if argmaxed. Args:     logits: (batch, seq_len, vocab_size) can have any number of leading dimensions. Assumed to be log of exponentiated scores.         That is, we assume logits are $l_i$ in $p_i = exp(l_i) / \\sum_i \\exp(l_i)$. Returns:     (batch, seq_len)</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.add_exp_1_noise","title":"<code>add_exp_1_noise(probs, temperature=1.0)</code>","text":"<p>Sample from unnormalized probability using the exponential race method. Similar to using gumbel noise, trick but we require the probs to be positive (can be unnormalized). You can generate samples without repeatations from the output by taking argmax or topk, etc. Args:     probs: (batch, seq_len, vocab_size) can have any number of leading dimensions. Returns:     (batch, seq_len)</p>"},{"location":"reference/xlm/utils/nn/#xlm.utils.nn.select_random_indices","title":"<code>select_random_indices(inp_shape, num_unmask, select_from_mask=None, selection_score=None, temperature=1.0, selection_mode='greedy', score_mode='logits')</code>","text":"<p>Select random indices from the last dimension using the selection_score. 1. If selection score is None then it is assumed to be uniform. 2. If score_mode = logits and selection_mode=sample, then temperature can be used to control the temperature of the distribution. 3. If select_from_mask is provided, indices only from these positions are sampled. Args:     inp_shape: torch.Size, typeically (batch, d)     num_unmask: (batch,) int tensor     select_from_mask: (batch, d) tensor, if provided, we only sample from the selected     selection_score: logit-like score for selection (can be negative). Should match inp_shape, so typically (batch, d)     score_mode:         \"logits\" =&gt; p_i = \\exp(s_i)/\\sum_j \\exp(s_j)         \"uprobs\" =&gt; p_i = s_i / \\sum_j s_j</p>"},{"location":"reference/xlm/utils/omegaconf_resolvers/","title":"omegaconf_resolvers","text":""},{"location":"reference/xlm/utils/omegaconf_resolvers/#xlm.utils.omegaconf_resolvers","title":"<code>xlm.utils.omegaconf_resolvers</code>","text":""},{"location":"reference/xlm/utils/omegaconf_resolvers/#xlm.utils.omegaconf_resolvers.dictconfig_filter_key","title":"<code>dictconfig_filter_key(d, fn)</code>","text":"<p>Only keep keys where fn(key) is True. Support nested DictConfig.</p>"},{"location":"reference/xlm/utils/omegaconf_resolvers/#xlm.utils.omegaconf_resolvers.remove_keys_with_double_underscores","title":"<code>remove_keys_with_double_underscores(d)</code>","text":"<p>Remove keys with double underscores. This can be used to remove keys that are only present for computational purposes and are not part of the final config.</p>"},{"location":"reference/xlm/utils/on_exception_checkpoint/","title":"on_exception_checkpoint","text":""},{"location":"reference/xlm/utils/on_exception_checkpoint/#xlm.utils.on_exception_checkpoint","title":"<code>xlm.utils.on_exception_checkpoint</code>","text":""},{"location":"reference/xlm/utils/on_exception_checkpoint/#xlm.utils.on_exception_checkpoint.OnExceptionCheckpoint","title":"<code>OnExceptionCheckpoint</code>","text":"<p>               Bases: <code>OnExceptionCheckpoint</code></p> <p>Same as the base class, but skips saving when exceptions are raised during sanity checking.</p>"},{"location":"reference/xlm/utils/os/","title":"os","text":""},{"location":"reference/xlm/utils/os/#xlm.utils.os","title":"<code>xlm.utils.os</code>","text":""},{"location":"reference/xlm/utils/rank_zero/","title":"rank_zero","text":""},{"location":"reference/xlm/utils/rank_zero/#xlm.utils.rank_zero","title":"<code>xlm.utils.rank_zero</code>","text":""},{"location":"reference/xlm/utils/rank_zero/#xlm.utils.rank_zero.RankedLogger","title":"<code>RankedLogger</code>","text":"<p>               Bases: <code>LoggerAdapter</code></p> <p>A multi-GPU-friendly python command line logger.</p>"},{"location":"reference/xlm/utils/rank_zero/#xlm.utils.rank_zero.RankedLogger.__init__","title":"<code>__init__(name=__name__, rank_zero_only=False, extra=None, stacklevel=2)</code>","text":"<p>Initializes a multi-GPU-friendly python command line logger that logs on all processes with their rank prefixed in the log message.</p> <p>:param name: The name of the logger. Default is <code>__name__</code>. :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is <code>False</code>. :param extra: (Optional) A dict-like object which provides contextual information. See <code>logging.LoggerAdapter</code>. :param stacklevel: The stack level to use for finding the caller. Default is 2.</p>"},{"location":"reference/xlm/utils/rank_zero/#xlm.utils.rank_zero.RankedLogger.log","title":"<code>log(level, msg, *args, rank=None, **kwargs)</code>","text":"<p>Delegate a log call to the underlying logger, after prefixing its message with the rank of the process it's being logged from. If <code>'rank'</code> is provided, then the log will only occur on that rank/process.</p> <p>:param level: The level to log at. Look at <code>logging.__init__.py</code> for more information. :param msg: The message to log. :param rank: The rank to log at. :param args: Additional args to pass to the underlying logging function. :param kwargs: Any additional keyword args to pass to the underlying logging function.</p>"},{"location":"reference/xlm/utils/rank_zero/#xlm.utils.rank_zero.get_rank_zero_logger","title":"<code>get_rank_zero_logger(name)</code>","text":"<p>Get a multi-GPU-friendly python command line logger.</p> <p>:param name: The name of the logger. :return: A multi-GPU-friendly python command line logger.</p>"},{"location":"reference/xlm/utils/rich_utils/","title":"rich_utils","text":""},{"location":"reference/xlm/utils/rich_utils/#xlm.utils.rich_utils","title":"<code>xlm.utils.rich_utils</code>","text":""},{"location":"reference/xlm/utils/rich_utils/#xlm.utils.rich_utils.print_config_tree","title":"<code>print_config_tree(cfg, print_order=[], resolve=True, save_to_file=False)</code>","text":"<p>Prints content of DictConfig using Rich library and its tree structure.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration composed by Hydra.</p> required <code>print_order</code> <code>Sequence[str]</code> <p>Determines what top level fields to print and in what order.</p> <code>[]</code> <code>resolve</code> <code>bool</code> <p>Whether to resolve reference fields of DictConfig.</p> <code>True</code> <code>save_to_file</code> <code>bool</code> <p>Whether to export config to the hydra output folder.</p> <code>False</code>"},{"location":"reference/xlm/utils/rich_utils/#xlm.utils.rich_utils.enforce_tags","title":"<code>enforce_tags(cfg, save_to_file=False)</code>","text":"<p>Prompts user to input tags from command line if no tags are provided in config.</p>"},{"location":"reference/xlm/utils/saving_utils/","title":"saving_utils","text":""},{"location":"reference/xlm/utils/saving_utils/#xlm.utils.saving_utils","title":"<code>xlm.utils.saving_utils</code>","text":""},{"location":"reference/xlm/utils/saving_utils/#xlm.utils.saving_utils.mkdir_rank_zero_only","title":"<code>mkdir_rank_zero_only(dir, exist_ok=True)</code>","text":"<p>Create directory only on rank 0.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>Path</code> <p>Directory path.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, do not raise an exception if the directory already exists. Default to True.</p> <code>True</code>"},{"location":"reference/xlm/utils/saving_utils/#xlm.utils.saving_utils.process_state_dict","title":"<code>process_state_dict(state_dict, symbols=0, exceptions=None)</code>","text":"<p>Filter and map model state dict keys.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Union[OrderedDict, dict]</code> <p>State dict.</p> required <code>symbols</code> <code>int</code> <p>Determines how many symbols should be cut in the beginning of state dict keys. Default to 0.</p> <code>0</code> <code>exceptions</code> <code>Union[str, List[str]]</code> <p>Determines exceptions, i.e. substrings, which keys should not contain.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>Filtered state dict.</p>"},{"location":"reference/xlm/utils/saving_utils/#xlm.utils.saving_utils.save_predictions_from_dataloader","title":"<code>save_predictions_from_dataloader(predictions, path)</code>","text":"<p>Save predictions returned by <code>Trainer.predict</code> method for single dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Any]</code> <p>Predictions returned by <code>Trainer.predict</code> method.</p> required <code>path</code> <code>Path</code> <p>Path to predictions.</p> required"},{"location":"reference/xlm/utils/saving_utils/#xlm.utils.saving_utils.save_predictions","title":"<code>save_predictions(predictions, dirname, output_format='json')</code>","text":"<p>Save predictions returned by <code>Trainer.predict</code> method.</p> <p>Due to <code>LightningDataModule.predict_dataloader</code> return type is Union[DataLoader, List[DataLoader]], so <code>Trainer.predict</code> method can return a list of dictionaries, one for each provided batch containing their respective predictions, or a list of lists, one for each provided dataloader containing their respective predictions, where each list contains dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[Any]</code> <p>Predictions returned by <code>Trainer.predict</code> method.</p> required <code>dirname</code> <code>str</code> <p>Dirname for predictions.</p> required <code>output_format</code> <code>str</code> <p>Output file format. It could be <code>json</code> or <code>csv</code>. Default to <code>json</code>.</p> <code>'json'</code>"},{"location":"reference/xlm/utils/seed/","title":"seed","text":""},{"location":"reference/xlm/utils/seed/#xlm.utils.seed","title":"<code>xlm.utils.seed</code>","text":""},{"location":"reference/xlm/utils/seed/#xlm.utils.seed.seed_everything","title":"<code>seed_everything(seed=None, workers=False)</code>","text":"<p>Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module. In addition, sets the following environment variables:</p> <ul> <li><code>PL_GLOBAL_SEED</code>: will be passed to spawned subprocesses (e.g. ddp_spawn backend).</li> <li><code>PL_SEED_WORKERS</code>: (optional) is set to 1 if <code>workers=True</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Optional[int]</code> <p>the integer value seed for global random state in Lightning. If <code>None</code>, it will read the seed from <code>PL_GLOBAL_SEED</code> env variable. If <code>None</code> and the <code>PL_GLOBAL_SEED</code> env variable is not set, then the seed defaults to 0.</p> <code>None</code> <code>workers</code> <code>bool</code> <p>if set to <code>True</code>, will properly configure all dataloaders passed to the Trainer with a <code>worker_init_fn</code>. If the user already provides such a function for their dataloaders, setting this argument will have no influence. See also: :func:<code>~lightning.fabric.utilities.seed.pl_worker_init_function</code>.</p> <code>False</code>"},{"location":"reference/xlm/utils/seed/#xlm.utils.seed.reset_seed","title":"<code>reset_seed()</code>","text":"<p>Reset the seed to the value that :func:<code>~lightning.fabric.utilities.seed.seed_everything</code> previously set.</p> <p>If :func:<code>~lightning.fabric.utilities.seed.seed_everything</code> is unused, this function will do nothing.</p>"},{"location":"reference/xlm/utils/seed/#xlm.utils.seed.pl_worker_init_function","title":"<code>pl_worker_init_function(worker_id, rank=None)</code>","text":"<p>The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with <code>seed_everything(seed, workers=True)</code>.</p> <p>See also the PyTorch documentation on <code>randomness in DataLoaders &lt;https://pytorch.org/docs/stable/notes/randomness.html#dataloader&gt;</code>_.</p>"},{"location":"reference/xlm/utils/signal/","title":"signal","text":""},{"location":"reference/xlm/utils/signal/#xlm.utils.signal","title":"<code>xlm.utils.signal</code>","text":""},{"location":"reference/xlm/utils/signal/#xlm.utils.signal.print_signal_handlers","title":"<code>print_signal_handlers(prefix='')</code>","text":"<p>Just print information about existing handlers for SIGTERM, SIGINT, SIGCONT, USR1 and USR2 signals.</p>"},{"location":"reference/xlm/utils/slurm/","title":"slurm","text":""},{"location":"reference/xlm/utils/slurm/#xlm.utils.slurm","title":"<code>xlm.utils.slurm</code>","text":""},{"location":"reference/xlm/utils/slurm/#xlm.utils.slurm.print_slurm_info","title":"<code>print_slurm_info()</code>","text":"<p>Typical output: SLURM INFO</p> <p>SLURM_STEP_NUM_TASKS: 1 SLURM_JOB_USER: dhruveshpate_umass_edu SLURM_TASKS_PER_NODE: 1 SLURM_JOB_UID: 31803 SLURM_TASK_PID: 3169823 SLURM_JOB_GPUS: 0 SLURM_LOCALID: 0 # not needed SLURM_SUBMIT_DIR: /home/dhruveshpate_umass_edu # not needed SLURM_JOB_START_TIME: 1752936994 # not needed SLURM_STEP_NODELIST: gpu027 SLURM_SPACK_ROOT: /usr # not needed SLURM_CLUSTER_NAME: unity # not needed SLURM_JOB_END_TIME: 1752951394 # not needed SLURM_PMI2_SRUN_PORT: 41735 SLURM_CPUS_ON_NODE: 12 SLURM_JOB_CPUS_PER_NODE: 12 SLURM_GPUS_ON_NODE: 1 SLURM_GTIDS: 0 # not needed SLURM_JOB_PARTITION: gpu SLURM_TRES_PER_TASK: cpu=12 SLURM_OOM_KILL_STEP: 0 # not needed SLURM_JOB_NUM_NODES: 1 SLURM_STEPID: 4294967290 SLURM_JOBID: 40112524 SLURM_GPUS: 1 SLURM_PTY_PORT: 43041 # not needed SLURM_LAUNCH_NODE_IPADDR: 10.100.1.2 SLURM_JOB_QOS: short SLURM_PTY_WIN_ROW: 46 # not needed SLURM_PMI2_PROC_MAPPING: (vector,(0,1,1)) # not needed SLURM_PROCID: 0 # not needed SLURM_CPUS_PER_TASK: 12 SLURM_TOPOLOGY_ADDR: .ib-core.tors-r1pac16.gpu027 # not needed SLURM_STEPMGR: gpu027 SLURM_TOPOLOGY_ADDR_PATTERN: switch.switch.switch.node # not needed SLURM_SRUN_COMM_HOST: 10.100.1.2 SLURM_SCRIPT_CONTEXT: prolog_task # not needed SLURM_MEM_PER_NODE: 40960 SLURM_PTY_WIN_COL: 157 # not needed SLURM_NODELIST: gpu027 SLURM_SRUN_COMM_PORT: 40067 SLURM_STEP_ID: 4294967290 SLURM_JOB_ACCOUNT: pi_mccallum_umass_edu # not needed SLURM_PRIO_PROCESS: 0 # not needed SLURM_NNODES: 1 SLURM_SUBMIT_HOST: login2 SLURM_JOB_ID: 40112524 SLURM_NODEID: 0 SLURM_STEP_NUM_NODES: 1 SLURM_INCLUDE_DIR: /usr/include # not needed SLURM_STEP_TASKS_PER_NODE: 1 SLURM_MPI_TYPE: pmi2 # not needed SLURM_PMI2_STEP_NODES: gpu027 # not needed SLURM_CONF: /var/spool/slurm/slurmd/conf-cache/slurm.conf # not needed SLURM_JOB_NAME: interactive SLURM_LIB_DIR: /usr/lib/x86_64-linux-gnu # not needed SLURM_STEP_LAUNCHER_PORT: 40067 SLURM_JOB_GID: 31803 SLURM_JOB_NODELIST: gpu027 SLURM_JOB_OUT: None</p>"},{"location":"reference/xlm/utils/speed_monitor_callback/","title":"speed_monitor_callback","text":""},{"location":"reference/xlm/utils/speed_monitor_callback/#xlm.utils.speed_monitor_callback","title":"<code>xlm.utils.speed_monitor_callback</code>","text":""},{"location":"reference/xlm/utils/speed_monitor_callback/#xlm.utils.speed_monitor_callback.SpeedMonitorCallback","title":"<code>SpeedMonitorCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Monitor the speed of each step and each epoch.</p>"},{"location":"reference/xlm/utils/speed_monitor_callback/#xlm.utils.speed_monitor_callback.SpeedMonitorCallback.__init__","title":"<code>__init__(window=50)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>The window size for the running mean.</p> <code>50</code>"},{"location":"reference/xlm/utils/tensorboard_logging/","title":"tensorboard_logging","text":""},{"location":"reference/xlm/utils/tensorboard_logging/#xlm.utils.tensorboard_logging","title":"<code>xlm.utils.tensorboard_logging</code>","text":""},{"location":"reference/xlm/utils/tqdm/","title":"tqdm","text":""},{"location":"reference/xlm/utils/tqdm/#xlm.utils.tqdm","title":"<code>xlm.utils.tqdm</code>","text":""},{"location":"reference/xlm/utils/tqdm/#xlm.utils.tqdm.replace_cr_with_newline","title":"<code>replace_cr_with_newline(message)</code>","text":"<p>TQDM and requests use carriage returns to get the training line to update for each batch without adding more lines to the terminal output. Displaying those in a file won't work correctly, so we'll just make sure that each batch shows up on its one line.</p>"},{"location":"reference/xlm/utils/wandb/","title":"wandb","text":""},{"location":"reference/xlm/utils/wandb/#xlm.utils.wandb","title":"<code>xlm.utils.wandb</code>","text":""},{"location":"reference/xlm/version/","title":"version","text":""},{"location":"reference/xlm/version/#xlm.version","title":"<code>xlm.version</code>","text":""},{"location":"reference/arlm/","title":"arlm","text":""},{"location":"reference/arlm/#arlm","title":"<code>arlm</code>","text":"<p>ARLM - Auto-Regressive Language Model for XLM Framework</p> <p>This package implements the ARLM model with all necessary components: - Model architecture (model_arlm.py) - Loss function (loss_arlm.py) - Predictor for inference (predictor_arlm.py) - Data module (datamodule_arlm.py) - Metrics computation (metrics_arlm.py) - Type definitions (types_arlm.py)</p> <p>This model was migrated from xlm.lm.arlm to be an external model.</p>"},{"location":"reference/arlm/#arlm.RotaryTransformerARLMModel","title":"<code>RotaryTransformerARLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder for auto-regressive language modeling.</p>"},{"location":"reference/arlm/#arlm.RotaryTransformerARLMModel.__init__","title":"<code>__init__(num_embeddings, d_model, num_layers, nhead, padding_idx=0, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, rotary_emb_dim=64, max_length=1024, force_flash_attn=False, final_layer_without_normalization=False)</code>","text":"<p>Initialize the ARLM transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the vocabulary.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers.</p> required <code>nhead</code> <code>int</code> <p>Number of attention heads.</p> required <code>padding_idx</code> <code>int</code> <p>Index of the padding token.</p> <code>0</code> <code>dim_feedforward</code> <code>Optional[int]</code> <p>Dimension of the feedforward network.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function.</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>Epsilon for layer normalization.</p> <code>1e-05</code> <code>rotary_emb_dim</code> <code>int</code> <p>Dimension of rotary embeddings.</p> <code>64</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> <code>1024</code> <code>force_flash_attn</code> <code>bool</code> <p>Whether to force flash attention.</p> <code>False</code> <code>final_layer_without_normalization</code> <code>bool</code> <p>Whether to use final layer without normalization.</p> <code>False</code>"},{"location":"reference/arlm/#arlm.RotaryTransformerARLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Forward pass of the ARLM model.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len seq_len']]</code> <p>The attention mask of shape (batch, seq_len, seq_len) for full attention matrix,           or (batch, seq_len) for simple mask. True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The token type ids of shape (*batch, seq_len)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>vocab_logits</code> <code>Float[Tensor, ' *batch seq_len vocab_size']</code> <p>The vocabulary logits of shape (*batch, seq_len, vocab_size)</p>"},{"location":"reference/arlm/#arlm.RotaryTransformerARLMModel.get_named_params_for_weight_decay","title":"<code>get_named_params_for_weight_decay()</code>","text":"<p>Get parameters for weight decay (all parameters except biases and layer-norm parameters).</p>"},{"location":"reference/arlm/#arlm.RotaryTransformerARLMModel.get_named_params_for_no_weight_decay","title":"<code>get_named_params_for_no_weight_decay()</code>","text":"<p>Get parameters for no weight decay (biases and layer-norm parameters).</p>"},{"location":"reference/arlm/#arlm.ARLMLoss","title":"<code>ARLMLoss</code>","text":"<p>               Bases: <code>LossFunction[ARLMBatch, ARLMLossDict]</code></p> <p>Loss function for Auto-Regressive Language Modeling (ARLM).</p> <p>This loss function implements causal language modeling where the model predicts the next token given the previous tokens. The loss is computed using cross-entropy on the target sequence (which is already shifted in the batch).</p> <p>For seq2seq tasks, loss is only computed on suffix tokens (non-prompt tokens).</p>"},{"location":"reference/arlm/#arlm.ARLMLoss.__init__","title":"<code>__init__(model=None, tokenizer=None)</code>","text":"<p>Initialize the ARLM loss function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer for processing tokens.</p> <code>None</code>"},{"location":"reference/arlm/#arlm.ARLMLoss.configure","title":"<code>configure(pl_module)</code>","text":"<p>Configure the loss function with the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>Harness</code> <p>The lightning module instance.</p> required"},{"location":"reference/arlm/#arlm.ARLMLoss.__call__","title":"<code>__call__(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the loss for the given batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch containing input_ids, attention_mask, and target_ids.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/arlm/#arlm.ARLMLoss.loss_fn","title":"<code>loss_fn(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the causal language modeling loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/arlm/#arlm.ARLMPredictor","title":"<code>ARLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[Dict[str, Any], ARLMPredictionDict]</code></p>"},{"location":"reference/arlm/#arlm.ARLMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, sampling_method='sample', top=1000, p=0.9, model=None)</code>","text":"<p>Constructor for ARLMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer to use.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> <code>None</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>Sampling method to use.</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>Top-k parameter for sampling.</p> <code>1000</code> <code>p</code> <code>float</code> <p>Top-p parameter for sampling.</p> <code>0.9</code> <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/arlm/#arlm.ARLMPredictor.predict_single_step","title":"<code>predict_single_step(step_results, current_step)</code>","text":"<p>Predict the next token in the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results containing x, attention_mask, and logits.</p> required <code>current_step</code> <code>int</code> <p>Current prediction step.</p> required <code>final_step</code> <p>Whether this is the final step.</p> required <p>Returns:</p> Type Description <code>ARLMStepResults</code> <p>Updated step results with the next token predicted.</p>"},{"location":"reference/arlm/#arlm.ARLMPredictor.stop","title":"<code>stop(step_results, current_length)</code>","text":"<p>Check if prediction should stop.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results.</p> required <code>current_length</code> <code>int</code> <p>Current sequence length.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if prediction should stop, False otherwise.</p>"},{"location":"reference/arlm/#arlm.ARLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Decode the predicted sequence.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ARLMStepResults</code> <p>Step results containing the predicted sequence.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str], Integer[Tensor, ' batch seq_len']]</code> <p>Tuple of (decoded_text, decoded_text_with_special_tokens, token_ids).</p>"},{"location":"reference/arlm/#arlm.ARLMPredictor.predict","title":"<code>predict(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None, max_len=0)</code>","text":"<p>Predict the complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch containing input_ids and attention_mask.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <code>max_len</code> <code>int</code> <p>Maximum length for prediction.</p> <code>0</code> <p>Returns:</p> Type Description <code>ARLMPredictionDict</code> <p>Prediction results containing text, token IDs, and attention mask.</p>"},{"location":"reference/arlm/#arlm.ARLMPredictor.to_dict","title":"<code>to_dict(batch, preds, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Convert predictions to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>preds</code> <code>ARLMPredictionDict</code> <p>Prediction results.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing prediction results.</p>"},{"location":"reference/arlm/#arlm.DefaultARLMCollator","title":"<code>DefaultARLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/arlm/#arlm.DefaultARLMCollator.__init__","title":"<code>__init__(tokenizer, block_size, noise_schedule, truncate='block', add_eos=False)</code>","text":"<p>Initialize the ARLM collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>block_size</code> <code>int</code> <p>Maximum sequence length.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the sequence.</p> <code>False</code>"},{"location":"reference/arlm/#arlm.DefaultARLMCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[BaseCollatorInput]</code> <p>List of examples with input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMBatch</code> <p>ARLMBatch with input_ids, attention_mask, and target_ids.</p>"},{"location":"reference/arlm/#arlm.ARLMSeq2SeqCollator","title":"<code>ARLMSeq2SeqCollator</code>","text":""},{"location":"reference/arlm/#arlm.ARLMSeq2SeqCollator.__init__","title":"<code>__init__(tokenizer, noise_schedule, block_size=None, input_block_size=None, add_bos=None, add_eos=False, truncate='block')</code>","text":"<p>Initialize the ARLM sequence-to-sequence collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the target.</p> <code>None</code> <code>input_block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the input.</p> <code>None</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the suffix.</p> <code>False</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code>"},{"location":"reference/arlm/#arlm.ARLMSeq2SeqCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/arlm/#arlm.ARLMSeq2SeqPredCollator","title":"<code>ARLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ARLMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/arlm/#arlm.ARLMSeq2SeqPredCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence prediction.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/arlm/#arlm.ARLMBatch","title":"<code>ARLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ARLM.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The target ids for language modeling (shifted by 1). Positions with -100 are ignored during loss computation (prompt tokens or padding).</p>"},{"location":"reference/arlm/#arlm.ARLMSeq2SeqBatch","title":"<code>ARLMSeq2SeqBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ARLM for sequence-to-sequence training.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model (prompt + target).</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Token type ids (not used in ARLM but kept for interface consistency).</p> <code>target_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The target ids for language modeling (shifted by 1). Positions with -100 are ignored during loss computation (prompt tokens or padding).</p>"},{"location":"reference/arlm/#arlm.ARLMLossDict","title":"<code>ARLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p> <code>batch_loss</code> <code>Float[Tensor, ' batch']</code> <p>Loss value for each example in the batch.</p> <code>nlls</code> <code>Float[Tensor, ' num_tokens']</code> <p>The negative log likelihoods of the real predicted tokens (non-pad, and masked in input).</p>"},{"location":"reference/arlm/#arlm.ARLMPredictionDict","title":"<code>ARLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for ARLM.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>The batch of generated text without special tokens.</p> <code>text_with_spl_tokens</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Attention mask accompanying the generated ids.</p> <code>positions</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of positions of the generated tokens accompanying the ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>int</code> <p>The index of the first output token.</p>"},{"location":"reference/arlm/datamodule_arlm/","title":"datamodule_arlm","text":""},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm","title":"<code>arlm.datamodule_arlm</code>","text":""},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMEmptyDataset","title":"<code>ARLMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples)</code>","text":"<p>Initialize the ARLM empty dataset.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>num_examples</code> <code>int</code> <p>Number of empty examples to generate.</p> required"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMEmptyDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Generate empty examples for ARLM training.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.DefaultARLMCollator","title":"<code>DefaultARLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.DefaultARLMCollator.__init__","title":"<code>__init__(tokenizer, block_size, noise_schedule, truncate='block', add_eos=False)</code>","text":"<p>Initialize the ARLM collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>block_size</code> <code>int</code> <p>Maximum sequence length.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the sequence.</p> <code>False</code>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.DefaultARLMCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[BaseCollatorInput]</code> <p>List of examples with input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMBatch</code> <p>ARLMBatch with input_ids, attention_mask, and target_ids.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMSeq2SeqCollator","title":"<code>ARLMSeq2SeqCollator</code>","text":""},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMSeq2SeqCollator.__init__","title":"<code>__init__(tokenizer, noise_schedule, block_size=None, input_block_size=None, add_bos=None, add_eos=False, truncate='block')</code>","text":"<p>Initialize the ARLM sequence-to-sequence collator.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to use.</p> required <code>noise_schedule</code> <code>NoiseSchedule</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> required <code>block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the target.</p> <code>None</code> <code>input_block_size</code> <code>Optional[int]</code> <p>Maximum sequence length for the input.</p> <code>None</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the suffix.</p> <code>False</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMSeq2SeqCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence training.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMSeq2SeqPredCollator","title":"<code>ARLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ARLMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.ARLMSeq2SeqPredCollator.__call__","title":"<code>__call__(examples)</code>","text":"<p>Collate examples into a batch for ARLM sequence-to-sequence prediction.</p> <p>Parameters:</p> Name Type Description Default <code>examples</code> <code>List[Seq2SeqCollatorInput]</code> <p>List of examples with prompt_ids and input_ids.</p> required <p>Returns:</p> Type Description <code>ARLMSeq2SeqBatch</code> <p>ARLMSeq2SeqBatch with input_ids, attention_mask, target_ids.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.prepare_prefix_ids_arlm","title":"<code>prepare_prefix_ids_arlm(prefix_ids, pad_token_id, bos_token_id=None, eos_token_id=None, max_seq_len=None, truncate='block', add_bos=None, add_eos=False)</code>","text":"<p>Prepare prefix ids for ARLM seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List of prefix token sequences.</p> required <code>pad_token_id</code> <code>int</code> <p>Padding token ID.</p> required <code>bos_token_id</code> <code>Optional[int]</code> <p>BOS token ID.</p> <code>None</code> <code>eos_token_id</code> <code>Optional[int]</code> <p>EOS token ID.</p> <code>None</code> <code>max_seq_len</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the prefix.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, List[List[int]]]</code> <p>Dictionary with input_ids and attention_mask as lists.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.prepare_suffix_ids_arlm","title":"<code>prepare_suffix_ids_arlm(suffix_ids, pad_token_id, bos_token_id=None, eos_token_id=None, max_seq_len=None, truncate='block', add_bos=None, add_eos=False)</code>","text":"<p>Prepare suffix ids for ARLM seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>suffix_ids</code> <code>List[List[int]]</code> <p>List of suffix token sequences.</p> required <code>pad_token_id</code> <code>int</code> <p>Padding token ID.</p> required <code>bos_token_id</code> <code>Optional[int]</code> <p>BOS token ID.</p> <code>None</code> <code>eos_token_id</code> <code>Optional[int]</code> <p>EOS token ID.</p> <code>None</code> <code>max_seq_len</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <p>Truncation strategy.</p> <code>'block'</code> <code>add_bos</code> <code>Optional[str]</code> <p>Where to add BOS token (\"input\" for prefix, \"output\" for after prefix, None for no BOS).</p> <code>None</code> <code>add_eos</code> <code>bool</code> <p>Whether to add EOS token at the end of the suffix.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, List[List[int]]]</code> <p>Dictionary with input_ids, attention_mask, and target_ids as lists.</p>"},{"location":"reference/arlm/datamodule_arlm/#arlm.datamodule_arlm.print_batch_arlm","title":"<code>print_batch_arlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging ARLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/arlm/loss_arlm/","title":"loss_arlm","text":""},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm","title":"<code>arlm.loss_arlm</code>","text":""},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm.ARLMLoss","title":"<code>ARLMLoss</code>","text":"<p>               Bases: <code>LossFunction[ARLMBatch, ARLMLossDict]</code></p> <p>Loss function for Auto-Regressive Language Modeling (ARLM).</p> <p>This loss function implements causal language modeling where the model predicts the next token given the previous tokens. The loss is computed using cross-entropy on the target sequence (which is already shifted in the batch).</p> <p>For seq2seq tasks, loss is only computed on suffix tokens (non-prompt tokens).</p>"},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm.ARLMLoss.__init__","title":"<code>__init__(model=None, tokenizer=None)</code>","text":"<p>Initialize the ARLM loss function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code> <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer for processing tokens.</p> <code>None</code>"},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm.ARLMLoss.configure","title":"<code>configure(pl_module)</code>","text":"<p>Configure the loss function with the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>pl_module</code> <code>Harness</code> <p>The lightning module instance.</p> required"},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm.ARLMLoss.__call__","title":"<code>__call__(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the loss for the given batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch containing input_ids, attention_mask, and target_ids.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/arlm/loss_arlm/#arlm.loss_arlm.ARLMLoss.loss_fn","title":"<code>loss_fn(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Compute the causal language modeling loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ARLMBatch</code> <p>The input batch.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>The dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>The dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>ARLMLossDict</code> <p>Dictionary containing the loss, batch_loss, and nlls.</p>"},{"location":"reference/arlm/metrics_arlm/","title":"metrics_arlm","text":""},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm","title":"<code>arlm.metrics_arlm</code>","text":""},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.seq2seq_exact_match_update_fn","title":"<code>seq2seq_exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required <p>Note: We rely on having same number right pads in target and pred, which may not be true for ARLM.</p>"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.seq2seq_token_accuracy_update_fn","title":"<code>seq2seq_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.mean_metric_update_fn","title":"<code>mean_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for mean loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary containing loss (since we don't use batch_loss for ARLM).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with mean loss value.</p>"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.perplexity_metric_update_fn","title":"<code>perplexity_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for perplexity metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary containing nlls.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with perplexity value.</p>"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.token_nll_metric_update_fn","title":"<code>token_nll_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for token-level negative log likelihood metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary containing nlls.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with token-level NLL values.</p>"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.sequence_length_metric_update_fn","title":"<code>sequence_length_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for sequence length metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with sequence length values.</p>"},{"location":"reference/arlm/metrics_arlm/#arlm.metrics_arlm.valid_tokens_metric_update_fn","title":"<code>valid_tokens_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for valid tokens count metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with valid tokens count.</p>"},{"location":"reference/arlm/model_arlm/","title":"model_arlm","text":""},{"location":"reference/arlm/model_arlm/#arlm.model_arlm","title":"<code>arlm.model_arlm</code>","text":""},{"location":"reference/arlm/model_arlm/#arlm.model_arlm.RotaryTransformerARLMModel","title":"<code>RotaryTransformerARLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder for auto-regressive language modeling.</p>"},{"location":"reference/arlm/model_arlm/#arlm.model_arlm.RotaryTransformerARLMModel.__init__","title":"<code>__init__(num_embeddings, d_model, num_layers, nhead, padding_idx=0, dim_feedforward=None, dropout=0.1, activation='relu', layer_norm_eps=1e-05, rotary_emb_dim=64, max_length=1024, force_flash_attn=False, final_layer_without_normalization=False)</code>","text":"<p>Initialize the ARLM transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the vocabulary.</p> required <code>d_model</code> <code>int</code> <p>Dimension of the model.</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers.</p> required <code>nhead</code> <code>int</code> <p>Number of attention heads.</p> required <code>padding_idx</code> <code>int</code> <p>Index of the padding token.</p> <code>0</code> <code>dim_feedforward</code> <code>Optional[int]</code> <p>Dimension of the feedforward network.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>activation</code> <code>str</code> <p>Activation function.</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>Epsilon for layer normalization.</p> <code>1e-05</code> <code>rotary_emb_dim</code> <code>int</code> <p>Dimension of rotary embeddings.</p> <code>64</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> <code>1024</code> <code>force_flash_attn</code> <code>bool</code> <p>Whether to force flash attention.</p> <code>False</code> <code>final_layer_without_normalization</code> <code>bool</code> <p>Whether to use final layer without normalization.</p> <code>False</code>"},{"location":"reference/arlm/model_arlm/#arlm.model_arlm.RotaryTransformerARLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Forward pass of the ARLM model.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len seq_len']]</code> <p>The attention mask of shape (batch, seq_len, seq_len) for full attention matrix,           or (batch, seq_len) for simple mask. True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The token type ids of shape (*batch, seq_len)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>vocab_logits</code> <code>Float[Tensor, ' *batch seq_len vocab_size']</code> <p>The vocabulary logits of shape (*batch, seq_len, vocab_size)</p>"},{"location":"reference/arlm/model_arlm/#arlm.model_arlm.RotaryTransformerARLMModel.get_named_params_for_weight_decay","title":"<code>get_named_params_for_weight_decay()</code>","text":"<p>Get parameters for weight decay (all parameters except biases and layer-norm parameters).</p>"},{"location":"reference/arlm/model_arlm/#arlm.model_arlm.RotaryTransformerARLMModel.get_named_params_for_no_weight_decay","title":"<code>get_named_params_for_no_weight_decay()</code>","text":"<p>Get parameters for no weight decay (biases and layer-norm parameters).</p>"},{"location":"reference/arlm/predictor_arlm/","title":"predictor_arlm","text":""},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm","title":"<code>arlm.predictor_arlm</code>","text":""},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMStepResults","title":"<code>ARLMStepResults</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Step results for ARLM prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Bool[TT, \" batch seq_len\"] Mask of the current sequence.</p> <code>logits</code> <code>Optional[Float[Tensor, ' batch seq_len vocab_size']]</code> <p>Float[TT, \" batch seq_len vocab_size\"] Logits of the current sequence.</p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor","title":"<code>ARLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[Dict[str, Any], ARLMPredictionDict]</code></p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, sampling_method='sample', top=1000, p=0.9, model=None)</code>","text":"<p>Constructor for ARLMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>max_length</code> <code>int</code> <p>Maximum sequence length.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>The tokenizer to use.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule (not used in ARLM but kept for interface consistency).</p> <code>None</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>Sampling method to use.</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>Top-k parameter for sampling.</p> <code>1000</code> <code>p</code> <code>float</code> <p>Top-p parameter for sampling.</p> <code>0.9</code> <code>model</code> <code>Optional[ARLMModel]</code> <p>The ARLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.predict_single_step","title":"<code>predict_single_step(step_results, current_step)</code>","text":"<p>Predict the next token in the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results containing x, attention_mask, and logits.</p> required <code>current_step</code> <code>int</code> <p>Current prediction step.</p> required <code>final_step</code> <p>Whether this is the final step.</p> required <p>Returns:</p> Type Description <code>ARLMStepResults</code> <p>Updated step results with the next token predicted.</p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.stop","title":"<code>stop(step_results, current_length)</code>","text":"<p>Check if prediction should stop.</p> <p>Parameters:</p> Name Type Description Default <code>step_results</code> <code>ARLMStepResults</code> <p>Current step results.</p> required <code>current_length</code> <code>int</code> <p>Current sequence length.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if prediction should stop, False otherwise.</p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Decode the predicted sequence.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>ARLMStepResults</code> <p>Step results containing the predicted sequence.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str], Integer[Tensor, ' batch seq_len']]</code> <p>Tuple of (decoded_text, decoded_text_with_special_tokens, token_ids).</p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.predict","title":"<code>predict(batch, batch_idx=None, dataloader_idx=None, dataloader_name=None, max_len=0)</code>","text":"<p>Predict the complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch containing input_ids and attention_mask.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <code>max_len</code> <code>int</code> <p>Maximum length for prediction.</p> <code>0</code> <p>Returns:</p> Type Description <code>ARLMPredictionDict</code> <p>Prediction results containing text, token IDs, and attention mask.</p>"},{"location":"reference/arlm/predictor_arlm/#arlm.predictor_arlm.ARLMPredictor.to_dict","title":"<code>to_dict(batch, preds, batch_idx=None, dataloader_idx=None, dataloader_name=None)</code>","text":"<p>Convert predictions to dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>preds</code> <code>ARLMPredictionDict</code> <p>Prediction results.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>Batch index.</p> <code>None</code> <code>dataloader_idx</code> <code>Optional[int]</code> <p>Dataloader index.</p> <code>None</code> <code>dataloader_name</code> <code>Optional[str]</code> <p>Dataloader name.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing prediction results.</p>"},{"location":"reference/arlm/types_arlm/","title":"types_arlm","text":""},{"location":"reference/arlm/types_arlm/#arlm.types_arlm","title":"<code>arlm.types_arlm</code>","text":""},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMBatch","title":"<code>ARLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ARLM.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The target ids for language modeling (shifted by 1). Positions with -100 are ignored during loss computation (prompt tokens or padding).</p>"},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMSeq2SeqPredictionBatch","title":"<code>ARLMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ARLM for predicting suffix given the prefix.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch suffix_seq_len']</code> <p>The target ids to the model.</p>"},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMSeq2SeqBatch","title":"<code>ARLMSeq2SeqBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ARLM for sequence-to-sequence training.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model (prompt + target).</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>Token type ids (not used in ARLM but kept for interface consistency).</p> <code>target_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The target ids for language modeling (shifted by 1). Positions with -100 are ignored during loss computation (prompt tokens or padding).</p>"},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMUnconditionalPredictionBatch","title":"<code>ARLMUnconditionalPredictionBatch</code>","text":"<p>Input to the ARLM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch 1']</code> <p>The input ids to the model (just BOS token).</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch 1']</code> <p>1 for tokens that are not padding.</p>"},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMLossDict","title":"<code>ARLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p> <code>batch_loss</code> <code>Float[Tensor, ' batch']</code> <p>Loss value for each example in the batch.</p> <code>nlls</code> <code>Float[Tensor, ' num_tokens']</code> <p>The negative log likelihoods of the real predicted tokens (non-pad, and masked in input).</p>"},{"location":"reference/arlm/types_arlm/#arlm.types_arlm.ARLMPredictionDict","title":"<code>ARLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for ARLM.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>The batch of generated text without special tokens.</p> <code>text_with_spl_tokens</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Attention mask accompanying the generated ids.</p> <code>positions</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of positions of the generated tokens accompanying the ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>int</code> <p>The index of the first output token.</p>"},{"location":"reference/mlm/","title":"mlm","text":""},{"location":"reference/mlm/#mlm","title":"<code>mlm</code>","text":"<p>MLM - Masked Language Model for XLM Framework</p> <p>This package implements the MLM model with all necessary components: - Model architecture (model_mlm.py) - Loss function (loss_mlm.py) - Predictor for inference (predictor_mlm.py) - Data module (datamodule_mlm.py) - Metrics computation (metrics_mlm.py) - Type definitions (types_mlm.py) - History tracking (history_mlm.py)</p> <p>This model was migrated from xlm.lm.mlm to be an external model.</p>"},{"location":"reference/mlm/#mlm.RotaryTransformerMLMModel","title":"<code>RotaryTransformerMLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/mlm/#mlm.RotaryTransformerMLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/mlm/#mlm.MLMPredictor","title":"<code>MLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MLMBatch, MLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/mlm/#mlm.MLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None, skip_special_tokens=True)</code>","text":"<p>Initialize MLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MLMModel]</code> <p>The MLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/mlm/#mlm.MLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/mlm/#mlm.DefaultMLMCollator","title":"<code>DefaultMLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mlm/#mlm.MLMSeq2SeqTrainCollator","title":"<code>MLMSeq2SeqTrainCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mlm/#mlm.MLMSeq2SeqCollator","title":"<code>MLMSeq2SeqCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position. TODO (efficiency): This is not ideal for seq2seq training as we will be wasting a lot of tokens in padding. For training, we should only pad on one side.</li> </ul>"},{"location":"reference/mlm/#mlm.MLMSeq2SeqPredCollator","title":"<code>MLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>MLMSeq2SeqCollator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p>"},{"location":"reference/mlm/#mlm.MLMBatch","title":"<code>MLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM. Attributes:     input_ids (Integer[TT, \" batch seq_len\"]): The input ids to the model.     attention_mask (Integer[TT, \" batch seq_len\"]): 1 for tokens that are not padding.     target_ids (Optional[Integer[TT, \" batch seq_len\"]]): The target ids to the model.</p>"},{"location":"reference/mlm/#mlm.MLMSeq2SeqPredictionBatch","title":"<code>MLMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM for predicting suffix given the prefix.</p>"},{"location":"reference/mlm/#mlm.MLMUncondtionalPredictionBatch","title":"<code>MLMUncondtionalPredictionBatch</code>","text":"<p>Input to the MLM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model. All masks.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p>"},{"location":"reference/mlm/#mlm.MLMLossDict","title":"<code>MLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p>"},{"location":"reference/mlm/#mlm.MLMPredictionDict","title":"<code>MLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for MLM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>Integer[Tensor, ' batch']</code> <p>The index of the first token in the output.</p>"},{"location":"reference/mlm/#mlm.HistoryTopKPlugin","title":"<code>HistoryTopKPlugin</code>","text":"<p>               Bases: <code>HistoryPluginBase</code></p> <p>We will dump the top k tokens and probs as tensors in separate files for each step.</p>"},{"location":"reference/mlm/datamodule_mlm/","title":"datamodule_mlm","text":""},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm","title":"<code>mlm.datamodule_mlm</code>","text":""},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.DefaultMLMCollator","title":"<code>DefaultMLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.MLMSeq2SeqTrainCollator","title":"<code>MLMSeq2SeqTrainCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.MLMSeq2SeqCollator","title":"<code>MLMSeq2SeqCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position. TODO (efficiency): This is not ideal for seq2seq training as we will be wasting a lot of tokens in padding. For training, we should only pad on one side.</li> </ul>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.MLMSeq2SeqPredCollator","title":"<code>MLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>MLMSeq2SeqCollator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.MLMInfillWithExactTargetPredCollator","title":"<code>MLMInfillWithExactTargetPredCollator</code>","text":"<p>               Bases: <code>DefaultMLMCollator</code></p> <p>Identical to DefaultMLMCollator but expects the prompt_ids to already contain masks.</p>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.prepare_prefix_ids","title":"<code>prepare_prefix_ids(prefix_ids, pad_token_id, max_seq_len=None, truncate='block')</code>","text":"<p>Prepare prefix ids for seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List[List[int]]</p> required <code>pad_token_id</code> <code>int</code> <p>int</p> required <code>max_seq_len</code> <code>Optional[int]</code> <p>Optional[int]</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <ul> <li>\"max\": Truncate to max(max_seq_len, max_in_batch).<ul> <li>when max_seq_len is not provided, it is the max in the batch.</li> </ul> </li> <li>\"block\": Pad-Truncate to max_seq_len.</li> <li>None: Pad to max in the batch.</li> </ul> <code>'block'</code> <p>Note: Prefixes if truncated will be truncated from the left. Returns:     Dict[str, TT]:         input_ids: Integer[TT, \" batch seq_len\"]         attention_mask: Integer[TT, \" batch seq_len\"]</p>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.prepare_prefix_suffix_ids","title":"<code>prepare_prefix_suffix_ids(prefix_ids, suffix_ids, pad_token_id, mask_token_id, eos_token_id=None, bos_token_id=None, max_seq_len=None, truncate='block', loss_on_padding=True)</code>","text":"<p>Prepare concatenated prefix and suffix ids for seq2seq tasks with padding on the right only</p> <p>Parameters:</p> Name Type Description Default <code>loss_on_padding</code> <code>bool</code> <p>bool - If true, pad token is treated as a normal token: it has attention on it, it is predicted as a target token. - If false, it has no attention on it, it is not predicted as a target token (-100)</p> <code>True</code>"},{"location":"reference/mlm/datamodule_mlm/#mlm.datamodule_mlm.print_batch_mlm","title":"<code>print_batch_mlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging MLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/mlm/history_mlm/","title":"history_mlm","text":""},{"location":"reference/mlm/history_mlm/#mlm.history_mlm","title":"<code>mlm.history_mlm</code>","text":""},{"location":"reference/mlm/history_mlm/#mlm.history_mlm.StepResults","title":"<code>StepResults</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The input token ids.</p> required <code>attention_mask</code> <p>The mask for the input token ids.</p> required <code>position_ids</code> <p>The ids of the positions in the sequence.</p> required <code>logits</code> <p>The logits for the next token.</p> required <code>t</code> <p>The time step.</p> required <code>current_step</code> <p>The current step.</p> required <code>change</code> <p>The change in the logits.</p> required <code>input_end_positions</code> <p>The end positions of the input tokens.</p> required"},{"location":"reference/mlm/history_mlm/#mlm.history_mlm.HistoryPluginBase","title":"<code>HistoryPluginBase</code>","text":""},{"location":"reference/mlm/history_mlm/#mlm.history_mlm.HistoryPluginBase.reset","title":"<code>reset()</code>","text":"<p>Called between examples.</p>"},{"location":"reference/mlm/history_mlm/#mlm.history_mlm.HistoryTopKPlugin","title":"<code>HistoryTopKPlugin</code>","text":"<p>               Bases: <code>HistoryPluginBase</code></p> <p>We will dump the top k tokens and probs as tensors in separate files for each step.</p>"},{"location":"reference/mlm/loss_mlm/","title":"loss_mlm","text":""},{"location":"reference/mlm/loss_mlm/#mlm.loss_mlm","title":"<code>mlm.loss_mlm</code>","text":""},{"location":"reference/mlm/metrics_mlm/","title":"metrics_mlm","text":""},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm","title":"<code>mlm.metrics_mlm</code>","text":""},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm.exact_match_update_fn","title":"<code>exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" *batch target_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm.infill_token_accuracy_update_fn","title":"<code>infill_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm.seq2seq_exact_match_update_fn","title":"<code>seq2seq_exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm.seq2seq_token_accuracy_update_fn","title":"<code>seq2seq_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mlm/metrics_mlm/#mlm.metrics_mlm.mean_metric_update_fn","title":"<code>mean_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for mean loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary containing loss (since we don't use batch_loss for ARLM).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with mean loss value.</p>"},{"location":"reference/mlm/model_mlm/","title":"model_mlm","text":""},{"location":"reference/mlm/model_mlm/#mlm.model_mlm","title":"<code>mlm.model_mlm</code>","text":""},{"location":"reference/mlm/model_mlm/#mlm.model_mlm.RotaryTransformerMLMModel","title":"<code>RotaryTransformerMLMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/mlm/model_mlm/#mlm.model_mlm.RotaryTransformerMLMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/mlm/predictor_mlm/","title":"predictor_mlm","text":""},{"location":"reference/mlm/predictor_mlm/#mlm.predictor_mlm","title":"<code>mlm.predictor_mlm</code>","text":""},{"location":"reference/mlm/predictor_mlm/#mlm.predictor_mlm.MLMPredictor","title":"<code>MLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MLMBatch, MLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/mlm/predictor_mlm/#mlm.predictor_mlm.MLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None, skip_special_tokens=True)</code>","text":"<p>Initialize MLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MLMModel]</code> <p>The MLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/mlm/predictor_mlm/#mlm.predictor_mlm.MLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/mlm/types_mlm/","title":"types_mlm","text":""},{"location":"reference/mlm/types_mlm/#mlm.types_mlm","title":"<code>mlm.types_mlm</code>","text":""},{"location":"reference/mlm/types_mlm/#mlm.types_mlm.MLMBatch","title":"<code>MLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM. Attributes:     input_ids (Integer[TT, \" batch seq_len\"]): The input ids to the model.     attention_mask (Integer[TT, \" batch seq_len\"]): 1 for tokens that are not padding.     target_ids (Optional[Integer[TT, \" batch seq_len\"]]): The target ids to the model.</p>"},{"location":"reference/mlm/types_mlm/#mlm.types_mlm.MLMSeq2SeqPredictionBatch","title":"<code>MLMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM for predicting suffix given the prefix.</p>"},{"location":"reference/mlm/types_mlm/#mlm.types_mlm.MLMUncondtionalPredictionBatch","title":"<code>MLMUncondtionalPredictionBatch</code>","text":"<p>Input to the MLM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model. All masks.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p>"},{"location":"reference/mlm/types_mlm/#mlm.types_mlm.MLMLossDict","title":"<code>MLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p>"},{"location":"reference/mlm/types_mlm/#mlm.types_mlm.MLMPredictionDict","title":"<code>MLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for MLM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>Integer[Tensor, ' batch']</code> <p>The index of the first token in the output.</p>"},{"location":"reference/ilm/","title":"ilm","text":""},{"location":"reference/ilm/#ilm","title":"<code>ilm</code>","text":"<p>ILM - Infilling Language Model for XLM Framework</p> <p>This package implements the ILM model with all necessary components: - Model architecture (model_ilm.py) - Loss function (loss_ilm.py) - Predictor for inference (predictor_ilm.py) - Data module (datamodule_ilm.py) - Metrics computation (metrics_ilm.py) - Type definitions (types_ilm.py) - Neural network utilities (nn.py)</p> <p>This model was migrated from xlm.lm.ilm to be an external model.</p>"},{"location":"reference/ilm/#ilm.RotaryTransformerILMModelWithClassification","title":"<code>RotaryTransformerILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseRotaryTransformerILMModel</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/ilm/#ilm.RotaryTransformerILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None, cls_position=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/#ilm.GPT2ILMModelWithClassification","title":"<code>GPT2ILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseGPT2ILMModel</code></p>"},{"location":"reference/ilm/#ilm.GPT2ILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/#ilm.ILMPredictor","title":"<code>ILMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/ilm/#ilm.ILMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, input_constraint=False)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/ilm/#ilm.ILMPredictorWithLengthClassification","title":"<code>ILMPredictorWithLengthClassification</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/ilm/#ilm.ILMPredictorWithLengthClassification.__init__","title":"<code>__init__(max_steps, max_length, stopping_threshold=0.5, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, force_predict_first_step=False, input_constraint=False, use_high_precision=False, stopping_temperature=1.0)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> <code>0.5</code> <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/ilm/#ilm.DefaultILMCollator","title":"<code>DefaultILMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/ilm/#ilm.ILMSeq2SeqCollator","title":"<code>ILMSeq2SeqCollator</code>","text":"<p>Drops tokens from the suffix only.</p>"},{"location":"reference/ilm/#ilm.ILMSeq2SeqPredCollator","title":"<code>ILMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ILMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/ilm/#ilm.ILMBatch","title":"<code>ILMBatch</code>","text":"<p>               Bases: <code>BaseBatch</code></p> <p>Input to the ILM.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p> <code>n_drops</code> <code>Bool[Tensor, ' batch post_seq_len']</code> <p>1 for tokens that are dropped.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch post_seq_len vocab_size']</code> <p>The target ids to the model.</p> <code>constraint</code> <code>Optional[Bool[Tensor, ' batch post_seq_len']]</code> <p>1 for tokens that should not be predicted. Mostly used during prediction only.</p> <code>cls_position</code> <code>Optional[Integer[Tensor, ' batch']]</code> <p>The position of the CLS token.</p>"},{"location":"reference/ilm/#ilm.ILMSeq2SeqPredictionBatch","title":"<code>ILMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for predicting suffix given the prefix. Note that the target_ids are different from the ILMBatch</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch suffix_seq_len']</code> <p>The target ids to the model.</p>"},{"location":"reference/ilm/#ilm.ILMUncondtionalPredictionBatch","title":"<code>ILMUncondtionalPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p>"},{"location":"reference/ilm/#ilm.ILMInfillPredictionBatch","title":"<code>ILMInfillPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for infilling.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model with tokens to be infilled dropped.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>gap_positions</code> <code>Integer[Tensor, ' batch max_gap_positions']</code> <p>The positions of the gaps in the input_ids which specify locations to be filled in. Padded using value -1.</p> <code>target_ids</code> <code>Integer[Tensor, ' total_gaps_in_batch max_infill_length']</code> <p>The target ids to be filled in. One can map the targets to the exact gap using the gap_positions.</p>"},{"location":"reference/ilm/#ilm.ILMLossDict","title":"<code>ILMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p> <code>batch_loss</code> <code>Float[Tensor, ' batch']</code> <p>Loss value for each example in the batch.</p>"},{"location":"reference/ilm/#ilm.ILMPredictionDict","title":"<code>ILMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for ILM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text without special tokens.</p> <code>text_with_spl_tokens</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Attention mask accompanying the generated ids.</p> <code>positions</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of positions of the generated tokens accompanying the ids.</p> <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>The batch of history. Each entry is a list of tuples, where each tuple contains (current_string, time, step_number) of when some change is made to the generated string.</p>"},{"location":"reference/ilm/datamodule_ilm/","title":"datamodule_ilm","text":""},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm","title":"<code>ilm.datamodule_ilm</code>","text":""},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.ILMEmptyDataset","title":"<code>ILMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.ILMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer_kwargs</code> <p>Keyword arguments for the tokenizer.</p> required <code>empty_text</code> <p>For MLM, you will want to set the <code>empty_text</code> to a sequence of all mask tokens.</p> required"},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.DefaultILMCollator","title":"<code>DefaultILMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for pre-training.</p>"},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.ILMSeq2SeqCollator","title":"<code>ILMSeq2SeqCollator</code>","text":"<p>Drops tokens from the suffix only.</p>"},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.ILMSeq2SeqPredCollator","title":"<code>ILMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>ILMSeq2SeqCollator</code></p> <p>Drops all the suffix/target tokens and sends them in the target_ids of shape (batch_size, target_seq_len)</p>"},{"location":"reference/ilm/datamodule_ilm/#ilm.datamodule_ilm.ilm_drop_fn","title":"<code>ilm_drop_fn(segment_input_ids, bos_token_id, cls_token_id, global_offset=0, sample_n_drops_fn=_n_drop_uniformly, drop_indices_fn=_drop_uniformly)</code>","text":"<p>Drops tokens from a single segment of a single sequence. Adds bos. Adds cls as requested.</p>"},{"location":"reference/ilm/loss_ilm/","title":"loss_ilm","text":""},{"location":"reference/ilm/loss_ilm/#ilm.loss_ilm","title":"<code>ilm.loss_ilm</code>","text":""},{"location":"reference/ilm/metrics_ilm/","title":"metrics_ilm","text":""},{"location":"reference/ilm/metrics_ilm/#ilm.metrics_ilm","title":"<code>ilm.metrics_ilm</code>","text":""},{"location":"reference/ilm/model_ilm/","title":"model_ilm","text":""},{"location":"reference/ilm/model_ilm/#ilm.model_ilm","title":"<code>ilm.model_ilm</code>","text":""},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.BaseRotaryTransformerILMModel","title":"<code>BaseRotaryTransformerILMModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Model</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.BaseRotaryTransformerILMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None, cls_position=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.RotaryTransformerILMModelWithClassification","title":"<code>RotaryTransformerILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseRotaryTransformerILMModel</code></p> <p>Rotary embedding based transformer decoder.</p>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.RotaryTransformerILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None, cls_position=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.BaseGPT2ILMModel","title":"<code>BaseGPT2ILMModel</code>","text":"<p>               Bases: <code>GPT</code>, <code>Model</code></p>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.BaseGPT2ILMModel.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.GPT2ILMModelWithClassification","title":"<code>GPT2ILMModelWithClassification</code>","text":"<p>               Bases: <code>BaseGPT2ILMModel</code></p>"},{"location":"reference/ilm/model_ilm/#ilm.model_ilm.GPT2ILMModelWithClassification.forward","title":"<code>forward(x_t, attention_mask=None, positions=None, token_type_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>t</code> <p>The timesteps of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/ilm/nn/","title":"nn","text":""},{"location":"reference/ilm/nn/#ilm.nn","title":"<code>ilm.nn</code>","text":""},{"location":"reference/ilm/nn/#ilm.nn.remove_tokens","title":"<code>remove_tokens(token_ids, ids_to_remove, pad_token_id)</code>","text":"<p>Remove all ids_to_remove (e.g. mask tokens) from token_ids and shift the non-mask tokens to fill the gap. The resulting tensor has the same shape as token_ids. The extra (empty) slots at the end of each row are filled with pad_token_id.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>Tensor</code> <p>Tensor of shape (batch, seq_len) containing token ids.</p> required <code>ids_to_remove</code> <code>int</code> <p>The id of the mask token that should be removed or a tensor of shape (n,) containing the ids to remove.</p> required <code>pad_token_id</code> <code>int</code> <p>The id to use for padding the empty positions.</p> required <code>hold_mask</code> <code>bool</code> <p>For the positions where this is true, we will consider them as tokens even if they are in ids_to_remove.</p> required <p>Returns:</p> Type Description <code>Integer[Tensor, 'batch seq_len']</code> <p>torch.Tensor: A tensor of the same shape as token_ids with ids_to_remove removed.</p>"},{"location":"reference/ilm/nn/#ilm.nn.masked_ce_last_two_dims","title":"<code>masked_ce_last_two_dims(logits, target, mask, min_value, inplace=False)</code>","text":"<p>Computes cross entropy using <code>logits</code> and <code>target</code> probabilities.</p> <p>The <code>mask</code> entries of <code>target</code> are ignored by setting them of -infty (effectively). Ideally, pytorch should handle -infy in the logits values that represent 0 predicted probability, but it currenlty does not: https://github.com/pytorch/pytorch/issues/49844</p> <p>Note: If <code>inplace</code> is True, the logits will not be usable after this call.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Float[Tensor, 'batch seq vocab']</code> <p>Unnormalized logits of shape (*batch, seq, vocab).</p> required <code>target</code> <code>Integer[Tensor, 'batch seq']</code> <p>Target probabilities of shape (*batch, seq).</p> required <code>mask</code> <code>Bool[Tensor, 'batch seq vocab']</code> <p>Mask of shape (*batch, seq).</p> required <code>min_value</code> <code>float</code> <p>The minimum value to use for the logits.</p> required <code>inplace</code> <code>bool</code> <p>If True, the logits will be modified in place.</p> <code>False</code>"},{"location":"reference/ilm/nn/#ilm.nn.topk_over_last_two_dims","title":"<code>topk_over_last_two_dims(tensor, k)</code>","text":"<p>Compute top-k values and their indices over dimensions 1 and 2 of a 3D tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, dim1, dim2).</p> required <code>k</code> <code>int</code> <p>Number of top elements to retrieve.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, '*batch k']</code> <p>torch.Tensor: Top-k values, shape (batch_size, k).</p> <code>Float[Tensor, '*batch k 2']</code> <p>torch.Tensor: Unraveled indices of top-k values, shape (batch_size, k, 2).</p>"},{"location":"reference/ilm/nn/#ilm.nn.max_over_last_two_dims","title":"<code>max_over_last_two_dims(x)</code>","text":"<p>Compute the maximum values and their indices over dimensions 1 and 2 of a 3D tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, dim1, dim2).</p> required <p>Returns:</p> Type Description <code>Float[Tensor, '*batch']</code> <p>torch.Tensor: Maximum values, shape (batch_size,).</p> <code>Tuple[Integer[Tensor, '*batch'], Integer[Tensor, '*batch']]</code> <p>torch.Tensor: Unraveled indices of maximum values, shape (batch_size, 2).</p>"},{"location":"reference/ilm/nn/#ilm.nn.sample_over_last_two_dims","title":"<code>sample_over_last_two_dims(logits, sampling_function)</code>","text":"<p>Sample values and their indices over dimensions 1 and 2 of a 3D tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, dim1, dim2). It can represent probabilities or unnormalized logits.</p> required <p>Returns:</p> Type Description <code>Tuple[Integer[Tensor, ' *batch'], Integer[Tensor, ' *batch']]</code> <p>Tuple[Integer[TT, \"batch\"], Integer[TT, \"batch\"]]: - Sampled values, shape (batch_size,). - Unraveled indices of sampled values, shape (2, batch_size).</p>"},{"location":"reference/ilm/nn/#ilm.nn.general_sample_over_last_two_dims","title":"<code>general_sample_over_last_two_dims(logits, sampling_function, second_sampling_function)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Float[Tensor, '*batch seq vocab']</code> <p>Joint logits of shape (*batch, seq, vocab).</p> required <code>sampling_function</code> <code>Callable[[Float[Tensor, '*batch cat']], Integer[Tensor, '*batch']]</code> <p>If second_sampling_fuction is None, this will be used to jointly sample the sequence and vocabulary dimensions. If second_sampling_function is not None, this will be used to sample from the vocab dimension.</p> required <code>second_sampling_function</code> <code>Optional[Callable[[Float[Tensor, '*batch cat']], Integer[Tensor, '*batch']]]</code> <p>If given, it will be use for the sequence dimension.</p> required <p>Returns:     sequence_indices, vocabulary_indices</p>"},{"location":"reference/ilm/predictor_ilm/","title":"predictor_ilm","text":""},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm","title":"<code>ilm.predictor_ilm</code>","text":""},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictorUtilitiesMixin","title":"<code>ILMPredictorUtilitiesMixin</code>","text":""},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictorUtilitiesMixin.clean_up_pred_ids","title":"<code>clean_up_pred_ids(pred_ids, hold_mask=None)</code>","text":"<p>Remove mask tokens inserted due to batched prediction.</p>"},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictor","title":"<code>ILMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictor.__init__","title":"<code>__init__(max_steps, max_length, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, input_constraint=False)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictorWithLengthClassification","title":"<code>ILMPredictorWithLengthClassification</code>","text":"<p>               Bases: <code>Module</code>, <code>ILMPredictorUtilitiesMixin</code>, <code>Predictor[ILMBatch, ILMPredictionDict]</code></p>"},{"location":"reference/ilm/predictor_ilm/#ilm.predictor_ilm.ILMPredictorWithLengthClassification.__init__","title":"<code>__init__(max_steps, max_length, stopping_threshold=0.5, tokenizer=None, noise_schedule=None, tokens_to_suppress=None, return_history=False, sampling_method='sample', top=1000, p=0.9, second_sampling_method=None, second_top=1000, second_p=0.9, model=None, force_predict_first_step=False, input_constraint=False, use_high_precision=False, stopping_temperature=1.0)</code>","text":"<p>Constructor for ILMPredictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>The maximum number of steps to take.</p> required <code>max_length</code> <code>int</code> <p>The maximum length (excluding special tokens like PAD and MASK) of the generated text.</p> required <code>stopping_threshold</code> <code>float</code> <p>The threshold for stopping use on the length classification scores.</p> <code>0.5</code> <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>The noise schedule. Typically, set after initialization but before calling predict.</p> <code>None</code> <code>tokens_to_suppress</code> <code>List[str]</code> <p>The tokens to suppress during generation.</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the history.</p> <code>False</code> <code>sampling_method</code> <code>Literal['sample', 'sample_top_k', 'sample_top_p']</code> <p>The sampling method. When <code>second_sampling_method</code> is not provided, the specified method here is used to sample from the joint distribution of positions and tokens. When <code>second_sampling_method</code> is provided, the specified method here is used to sample from the token distribution (conditional) given the postions sampled using the <code>second_sampling_method</code>. \"sample\" means vanilla sampling from the distribution. \"sample_top_k\" means sampling from the top-k distribution. \"sample_top_p\" means sampling from the top-p distribution (neuclius samplingn).</p> <code>'sample'</code> <code>top</code> <code>int</code> <p>The top-k sampling parameter for <code>sampling_method</code>.</p> <code>1000</code> <code>p</code> <code>float</code> <p>The top-p sampling parameter for <code>sampling_method</code>.</p> <code>0.9</code> <code>second_sampling_method</code> <code>Optional[Literal['sample', 'sample_top_k', 'sample_top_p']]</code> <p>The second sampling method.</p> <code>None</code> <code>second_top</code> <code>int</code> <p>The second top-k sampling parameter for <code>second_sampling_method</code>.</p> <code>1000</code> <code>second_p</code> <code>float</code> <p>The second top-p sampling parameter for <code>second_sampling_method</code>.</p> <code>0.9</code> <code>model</code> <code>Optional[ILMModel]</code> <p>The model. Typically, set after initialization but before calling predict.</p> <code>None</code>"},{"location":"reference/ilm/types_ilm/","title":"types_ilm","text":""},{"location":"reference/ilm/types_ilm/#ilm.types_ilm","title":"<code>ilm.types_ilm</code>","text":""},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMBatch","title":"<code>ILMBatch</code>","text":"<p>               Bases: <code>BaseBatch</code></p> <p>Input to the ILM.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch post_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p> <code>n_drops</code> <code>Bool[Tensor, ' batch post_seq_len']</code> <p>1 for tokens that are dropped.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch post_seq_len vocab_size']</code> <p>The target ids to the model.</p> <code>constraint</code> <code>Optional[Bool[Tensor, ' batch post_seq_len']]</code> <p>1 for tokens that should not be predicted. Mostly used during prediction only.</p> <code>cls_position</code> <code>Optional[Integer[Tensor, ' batch']]</code> <p>The position of the CLS token.</p>"},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMSeq2SeqPredictionBatch","title":"<code>ILMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for predicting suffix given the prefix. Note that the target_ids are different from the ILMBatch</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p> <code>target_ids</code> <code>Integer[Tensor, ' batch suffix_seq_len']</code> <p>The target ids to the model.</p>"},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMUncondtionalPredictionBatch","title":"<code>ILMUncondtionalPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>token_type_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>0 for CLS, 1 for BOS and prefix, 2 for other tokens.</p>"},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMInfillPredictionBatch","title":"<code>ILMInfillPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the ILM for infilling.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>The input ids to the model with tokens to be infilled dropped.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch prefix_seq_len']</code> <p>1 for tokens that are not padding.</p> <code>gap_positions</code> <code>Integer[Tensor, ' batch max_gap_positions']</code> <p>The positions of the gaps in the input_ids which specify locations to be filled in. Padded using value -1.</p> <code>target_ids</code> <code>Integer[Tensor, ' total_gaps_in_batch max_infill_length']</code> <p>The target ids to be filled in. One can map the targets to the exact gap using the gap_positions.</p>"},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMLossDict","title":"<code>ILMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p> <code>batch_loss</code> <code>Float[Tensor, ' batch']</code> <p>Loss value for each example in the batch.</p>"},{"location":"reference/ilm/types_ilm/#ilm.types_ilm.ILMPredictionDict","title":"<code>ILMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for ILM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text without special tokens.</p> <code>text_with_spl_tokens</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>attention_mask</code> <code>Bool[Tensor, ' batch seq_len']</code> <p>Attention mask accompanying the generated ids.</p> <code>positions</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of positions of the generated tokens accompanying the ids.</p> <code>history</code> <code>List[List[Tuple[str, float, int]]]</code> <p>The batch of history. Each entry is a list of tuples, where each tuple contains (current_string, time, step_number) of when some change is made to the generated string.</p>"},{"location":"reference/mdlm/","title":"mdlm","text":""},{"location":"reference/mdlm/#mdlm","title":"<code>mdlm</code>","text":"<p>MDLM - Masked Diffusion Language Model for XLM Framework</p> <p>This package implements the MDLM model with all necessary components: - Model architecture (model_mdlm.py) - Loss function (loss_mdlm.py) - Predictor for inference (predictor_mdlm.py) - Data module (datamodule_mdlm.py) - Metrics computation (metrics_mdlm.py) - Type definitions (types_mdlm.py) - Noise functions (noise_mdlm.py)</p> <p>This model was migrated from xlm.lm.mdlm to be an external model.</p>"},{"location":"reference/mdlm/#mdlm.MDLMPredictor","title":"<code>MDLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MDLMBatch, MDLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/mdlm/#mdlm.MDLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None)</code>","text":"<p>Initialize MDLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MDLMModel]</code> <p>The MDLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/mdlm/#mdlm.MDLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MDLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/mdlm/#mdlm.DefaultMDLMCollator","title":"<code>DefaultMDLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MDLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mdlm/#mdlm.MDLMSeq2SeqPredCollator","title":"<code>MDLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p> <p>How is this different from MDLMSeq2SeqTrainCollator?     -  MDLMSeq2SeqTrainCollator's input_ids contain the joined sequence and target_ids also contain the target for the whole sequence. But MDLMSeq2SeqPredCollator's input_ids contain only the prefix and target_ids contain only the suffix if present.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: Input contains only the prefix</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: Target contains only the suffix if present.</li> <li>noise_rate: Float[TT, \" batch\"]: The noise rate for the model.</li> <li>total_noise: Float[TT, \" batch\"]: The total noise for the model.</li> <li>t: Float[TT, \" batch\"]: The time step for the model.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position.</li> </ul>"},{"location":"reference/mdlm/#mdlm.MDLMBatch","title":"<code>MDLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM. Attributes:     input_ids (Integer[TT, \" batch seq_len\"]): The input ids to the model.     attention_mask (Integer[TT, \" batch seq_len\"]): 1 for tokens that are not padding.     target_ids (Optional[Integer[TT, \" batch seq_len\"]]): The target ids to the model.</p>"},{"location":"reference/mdlm/#mdlm.MDLMSeq2SeqPredictionBatch","title":"<code>MDLMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM for predicting suffix given the prefix.</p>"},{"location":"reference/mdlm/#mdlm.MDLMUncondtionalPredictionBatch","title":"<code>MDLMUncondtionalPredictionBatch</code>","text":"<p>Input to the MLM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model. All masks.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p>"},{"location":"reference/mdlm/#mdlm.MDLMLossDict","title":"<code>MDLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p>"},{"location":"reference/mdlm/#mdlm.MDLMPredictionDict","title":"<code>MDLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for MLM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>Integer[Tensor, ' batch']</code> <p>The index of the first token in the output.</p>"},{"location":"reference/mdlm/datamodule_mdlm/","title":"datamodule_mdlm","text":""},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm","title":"<code>mdlm.datamodule_mdlm</code>","text":""},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.MDLMEmptyDataset","title":"<code>MDLMEmptyDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.MDLMEmptyDataset.__init__","title":"<code>__init__(tokenizer, num_examples, max_length)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer_kwargs</code> <p>Keyword arguments for the tokenizer.</p> required <code>TODO</code> <p>Might want the option to add BOS.</p> required"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.DefaultMDLMCollator","title":"<code>DefaultMDLMCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Used for MDLM pre-training with padded-truncated sequences.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.MDLMSeq2SeqTrainCollator","title":"<code>MDLMSeq2SeqTrainCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>MDLM training for seq2seq tasks.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: The input for the model with masks.</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: The target ids to the model where the input if copied as is and masks are replaced with the correct token.</li> </ol> Padding <ul> <li>Padding is done on the right.</li> </ul>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.MDLMSeq2SeqPredCollator","title":"<code>MDLMSeq2SeqPredCollator</code>","text":"<p>               Bases: <code>Collator</code></p> <p>Input contains only the prefix and target_ids contain only the suffix if present.</p> <p>How is this different from MDLMSeq2SeqTrainCollator?     -  MDLMSeq2SeqTrainCollator's input_ids contain the joined sequence and target_ids also contain the target for the whole sequence. But MDLMSeq2SeqPredCollator's input_ids contain only the prefix and target_ids contain only the suffix if present.</p> Batch <ol> <li>input_ids: Integer[TT, \" batch seq_len\"]: Input contains only the prefix</li> <li>attention_mask: Integer[TT, \" batch seq_len\"]: 1 for tokens that are not padding.</li> <li>target_ids: Integer[TT, \" batch seq_len\"]: Target contains only the suffix if present.</li> <li>noise_rate: Float[TT, \" batch\"]: The noise rate for the model.</li> <li>total_noise: Float[TT, \" batch\"]: The total noise for the model.</li> <li>t: Float[TT, \" batch\"]: The time step for the model.</li> </ol> Padding <ul> <li>There is padding on both sides because all prefixes end at the same position.</li> </ul>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.prepare_prefix_ids","title":"<code>prepare_prefix_ids(prefix_ids, pad_token_id, max_seq_len=None, truncate='block')</code>","text":"<p>Prepare prefix ids for seq2seq tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_ids</code> <code>List[List[int]]</code> <p>List[List[int]]</p> required <code>pad_token_id</code> <code>int</code> <p>int</p> required <code>max_seq_len</code> <code>Optional[int]</code> <p>Optional[int]</p> <code>None</code> <code>truncate</code> <code>Literal['max', 'block', None]</code> <ul> <li>\"max\": Truncate to max(max_seq_len, max_in_batch).<ul> <li>when max_seq_len is not provided, it is the max in the batch.</li> </ul> </li> <li>\"block\": Pad-Truncate to max_seq_len.</li> <li>None: Pad to max in the batch.</li> </ul> <code>'block'</code> <p>Note: Prefixes if truncated will be truncated from the left. Returns:     Dict[str, TT]:         input_ids: Integer[TT, \" batch seq_len\"]         attention_mask: Integer[TT, \" batch seq_len\"]</p>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.prepare_prefix_suffix_ids","title":"<code>prepare_prefix_suffix_ids(prefix_ids, suffix_ids, noise_schedule, pad_token_id, mask_token_id, eos_token_id=None, bos_token_id=None, max_seq_len=None, truncate='block', loss_on_padding=True, bos_location='after_prefix')</code>","text":"<p>Prepare concatenated prefix and suffix ids for seq2seq tasks with padding on the right only</p> <p>Parameters:</p> Name Type Description Default <code>loss_on_padding</code> <code>bool</code> <p>bool - If true, pad token is treated as a normal token: it has attention on it, it is predicted as a target token. - If false, it has no attention on it, it is not predicted as a target token (-100)</p> <code>True</code>"},{"location":"reference/mdlm/datamodule_mdlm/#mdlm.datamodule_mdlm.print_batch_mdlm","title":"<code>print_batch_mdlm(batch, split, tokenizer, dataloader_name='')</code>","text":"<p>Print batch information for debugging MLM batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch to print.</p> required <code>split</code> <code>Literal['train', 'val', 'test', 'predict']</code> <p>The split name.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer to decode tokens.</p> required <code>dataloader_name</code> <code>str</code> <p>Name of the dataloader.</p> <code>''</code>"},{"location":"reference/mdlm/loss_mdlm/","title":"loss_mdlm","text":""},{"location":"reference/mdlm/loss_mdlm/#mdlm.loss_mdlm","title":"<code>mdlm.loss_mdlm</code>","text":""},{"location":"reference/mdlm/metrics_mdlm/","title":"metrics_mdlm","text":""},{"location":"reference/mdlm/metrics_mdlm/#mdlm.metrics_mdlm","title":"<code>mdlm.metrics_mdlm</code>","text":""},{"location":"reference/mdlm/metrics_mdlm/#mdlm.metrics_mdlm.seq2seq_exact_match_update_fn","title":"<code>seq2seq_exact_match_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mdlm/metrics_mdlm/#mdlm.metrics_mdlm.seq2seq_token_accuracy_update_fn","title":"<code>seq2seq_token_accuracy_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"target_ids\": Integer[TT, \" batch target_seq_len\"] - \"input_ids\": Integer[TT, \" batch input_seq_len\"]</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]. Should contain the following keys: - \"ids\": Integer[TT, \" *batch input_seq_len+target_seq_len\"]</p> required"},{"location":"reference/mdlm/metrics_mdlm/#mdlm.metrics_mdlm.mean_metric_update_fn","title":"<code>mean_metric_update_fn(batch, loss_dict, tokenizer=None)</code>","text":"<p>Update function for mean loss metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>Input batch.</p> required <code>loss_dict</code> <code>Dict[str, Any]</code> <p>Loss dictionary containing loss (since we don't use batch_loss for ARLM).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with mean loss value.</p>"},{"location":"reference/mdlm/model_mdlm/","title":"model_mdlm","text":""},{"location":"reference/mdlm/model_mdlm/#mdlm.model_mdlm","title":"<code>mdlm.model_mdlm</code>","text":""},{"location":"reference/mdlm/model_mdlm/#mdlm.model_mdlm.MDLMModel","title":"<code>MDLMModel</code>","text":"<p>               Bases: <code>BaseMDLMModel</code></p> <p>DDiT based transformer that represents time/noise using AdaLN and uses rotary positional embeddings.</p>"},{"location":"reference/mdlm/model_mdlm/#mdlm.model_mdlm.MDLMModel.forward","title":"<code>forward(x_t, noise, attention_mask=None, positions=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Integer[Tensor, ' *batch seq_len']</code> <p>The input tokens of shape (*batch, seq_len)</p> required <code>noise</code> <code>Float[Tensor, ' *batch']</code> <p>The noise of shape (*batch)</p> required <code>attention_mask</code> <code>Optional[Bool[Tensor, ' *batch seq_len']]</code> <p>The attention mask of shape (*batch, seq_len), which is True for non-padding tokens.</p> <code>None</code> <code>positions</code> <code>Optional[Integer[Tensor, ' *batch seq_len']]</code> <p>The positions of the tokens of shape (*batch, seq_len)</p> <code>None</code>"},{"location":"reference/mdlm/noise_mdlm/","title":"noise_mdlm","text":""},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm","title":"<code>mdlm.noise_mdlm</code>","text":""},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule","title":"<code>ContinousTimeNoiseSchedule</code>","text":"<p>               Bases: <code>Module</code>, <code>NoiseSchedule</code></p> <p>Base class for continuous time noise schedules for absorbing diffusion.</p> <p>For absorbing diffusion in continuous time, we only need $\\sigma(t)$ and the integral $\\dot\\sigma(t)$, which we call noise_rate and total_noise respectively.</p> Note <p>We assume that for continous time, $t \\in [0, 1]$.</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.__init__","title":"<code>__init__(antithetic_sampling=True, importance_sampling=False, grad=False, eps=0.001)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>antithetic_sampling</code> <code>bool</code> <p>If true, the sampled time steps in a batch are sampled around points of a uniform grid over [0, 1], insted of sampling directly from a uniform distribution over [0, 1].</p> <code>True</code> <code>importance_sampling</code> <code>bool</code> <p>The goal is have a desired distribution over the noise level $\\sigma$, sampling of $t$ is just a way of obtaining a value of $\\sigma$. Since $\\sigma(t)$ is non-linear function of $t$, if we want to have a desired distribution over $\\sigma$ for training, which is indeed the case, we cannot simply sample $t$ uniformly and then transform it to $\\sigma(t)$. Setting importance_sampling=True, will sample uniformly directly over $\\sigma$ in the range $[\\sigma_{ ext{min}}, \\sigma_{     ext{max}}]$.</p> <code>False</code>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.noise_rate","title":"<code>noise_rate(t)</code>","text":"<p>Return the noise level at time t.</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.total_noise","title":"<code>total_noise(t)</code>","text":"<p>Return the total noise at time t.</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.t_from_noise_rate","title":"<code>t_from_noise_rate(noise_rate)</code>","text":"<p>Return the time step t from the noise level sigma.</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.t_from_total_noise","title":"<code>t_from_total_noise(total_noise)</code>","text":"<p>Return the time step t from the total noise.</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.forward","title":"<code>forward(t)</code>","text":"<p>Return the noise level at time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Float[Tensor, ' batch']</code> <p>The time step tensor of shape (batch)</p> required <p>Returns:     The noise level tensor of shape (batch)     The total noise tensor of shape (batch)</p>"},{"location":"reference/mdlm/noise_mdlm/#mdlm.noise_mdlm.ContinousTimeNoiseSchedule.sample_t","title":"<code>sample_t(batch_size, device=torch.device('cpu'))</code>","text":"<p>Sample a t uniformly from [0, 1].</p>"},{"location":"reference/mdlm/predictor_mdlm/","title":"predictor_mdlm","text":""},{"location":"reference/mdlm/predictor_mdlm/#mdlm.predictor_mdlm","title":"<code>mdlm.predictor_mdlm</code>","text":""},{"location":"reference/mdlm/predictor_mdlm/#mdlm.predictor_mdlm.MDLMPredictor","title":"<code>MDLMPredictor</code>","text":"<p>               Bases: <code>Module</code>, <code>Predictor[MDLMBatch, MDLMPredictionDict]</code></p> <p>Base predictor for MLM. Stochastically selects positions to unmask based on max_steps and max_new_tokens.</p>"},{"location":"reference/mdlm/predictor_mdlm/#mdlm.predictor_mdlm.MDLMPredictor.__init__","title":"<code>__init__(max_steps, max_new_tokens=None, tokenizer=None, model=None, noise_schedule=None, top_k=None, top_p=None)</code>","text":"<p>Initialize MDLM Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of prediction steps.</p> required <code>tokenizer</code> <code>Optional[Tokenizer]</code> <p>Tokenizer for encoding/decoding.</p> <code>None</code> <code>noise_schedule</code> <code>Optional[NoiseSchedule]</code> <p>Noise schedule for the diffusion process.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Top-p sampling parameter.</p> <code>None</code> <code>model</code> <code>Optional[MDLMModel]</code> <p>The MDLM model to use for predictions.</p> <code>None</code>"},{"location":"reference/mdlm/predictor_mdlm/#mdlm.predictor_mdlm.MDLMPredictor.decode","title":"<code>decode(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>MDLMStepResults</code> <p>x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p> required <p>Returns:     out: List[str] Decoded sequence with special tokens.     x: Integer[TT, \" batch seq_len\"] Current predicted sequence.</p>"},{"location":"reference/mdlm/types_mdlm/","title":"types_mdlm","text":""},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm","title":"<code>mdlm.types_mdlm</code>","text":""},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm.MDLMBatch","title":"<code>MDLMBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM. Attributes:     input_ids (Integer[TT, \" batch seq_len\"]): The input ids to the model.     attention_mask (Integer[TT, \" batch seq_len\"]): 1 for tokens that are not padding.     target_ids (Optional[Integer[TT, \" batch seq_len\"]]): The target ids to the model.</p>"},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm.MDLMSeq2SeqPredictionBatch","title":"<code>MDLMSeq2SeqPredictionBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input to the MLM for predicting suffix given the prefix.</p>"},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm.MDLMUncondtionalPredictionBatch","title":"<code>MDLMUncondtionalPredictionBatch</code>","text":"<p>Input to the MLM for unconditional generation.</p> <p>Attributes:</p> Name Type Description <code>input_ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The input ids to the model. All masks.</p> <code>attention_mask</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>1 for tokens that are not padding.</p>"},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm.MDLMLossDict","title":"<code>MDLMLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the LossFunction Callable.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Float[Tensor, '']</code> <p>The total loss value.</p>"},{"location":"reference/mdlm/types_mdlm/#mdlm.types_mdlm.MDLMPredictionDict","title":"<code>MDLMPredictionDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the Predictor for MLM.</p> <p>Attributes:</p> Name Type Description <code>loss</code> <code>Optional[Float[Tensor, batch]]</code> <p>The loss value. Typically None.</p> <code>text</code> <code>List[str]</code> <p>The batch of generated text with special tokens.</p> <code>ids</code> <code>Integer[Tensor, ' batch seq_len']</code> <p>The batch of generated token_ids.</p> <code>time_taken</code> <code>List[float]</code> <p>Time taken for each prediction.</p> <code>output_start_idx</code> <code>Integer[Tensor, ' batch']</code> <p>The index of the first token in the output.</p>"}]}