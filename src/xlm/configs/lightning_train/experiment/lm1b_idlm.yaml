# @package _global_
defaults:
  - text_idlm
  - override /datamodule: lm1b_idlm


per_device_batch_size: 128
global_batch_size: 512
block_size: 128
num_dataloader_workers: 5

datamodule:
  print_batch_fn: xlm.lm.idlm.datamodule.print_batch_idlm

# override from model_type
global_components:
  noise_schedule:
    _target_: xlm.lm.idlm.noise_schedule.LogLinearNoiseSchedule
    sigma_max: 500
    sigma_min: 5   #  specific for lm1b
    eps: 1e-4

trainer:
  max_steps: 1000_000
  val_check_interval: 50000
  num_sanity_val_steps: 0
  gradient_clip_val: 0.5

callbacks:
  checkpoint_every_n_steps:
    every_n_train_steps: 2500
    keep_multiple: 40 # Keep every (keep_multiple * every_n_train_steps) permanently = 2500 * 40 = 100_000

optimizer:
  lr: 0.00005

lr_scheduler:
  name: "constant_with_warmup" # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L435
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 2000
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"

predictor:
  sampling_method: sample_top_p
  p: 0.2
  second_sampling_method: sample_top_k
  second_top: 1

loss:
  length_loss_weight: 0.001
  ce_weight: 0.001