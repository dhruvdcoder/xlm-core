# @package _global_
defaults:
  - text_idlm
  - override /datamodule: owt_idlm
  

per_device_batch_size: 32
global_batch_size: 512
block_size: 1024
num_dataloader_workers: 5

# override from model_type
global_components:
  noise_schedule:
    _target_: xlm.lm.idlm.noise_schedule.LogLinearNoiseSchedule
    sigma_max: 3000
    sigma_min: 50   #  specific for owt
    eps: 1e-4

loss:
  length_loss_weight: 0.001


trainer:
  max_steps: 1000_000 # 1M
  val_check_interval: 50000
  num_sanity_val_steps: 3


optimizer:
  lr: 0.00005

lr_scheduler:
  name: "constant_with_warmup" # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L435
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 2000
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"


predictor:
  sampling_method: sample_top_p
  p: 0.2
  second_sampling_method: sample_top_k
  second_top: 1




callbacks:
  checkpoint_every_n_steps:
    every_n_train_steps: 2500
    keep_multiple: 40 # Keep every (keep_multiple * every_n_train_steps) permanently = 2500 * 40 = 100_000