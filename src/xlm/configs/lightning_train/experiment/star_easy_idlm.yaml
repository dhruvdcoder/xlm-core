# @package _global_
defaults:
  - override /datamodule: star_easy_idlm
  - override /model_type: idlm
  - override /model: idlm
  - override /noise_schedule: idlm_loglinear

per_device_batch_size: 64
global_batch_size: 64
input_block_size: 28
block_size: 14

datamodule:
  print_batch_fn: xlm.lm.idlm.collators.print_batch_idlm

global_components:
  tokenizer:
    _target_: xlm.datamodule.SimpleSpaceTokenizer.for_numbers
    vocab_size: 20 # 20 (easy,medium), 56 (hard)
  noise_schedule:
    _target_: xlm.lm.idlm.noise_schedule.LogLinearNoiseSchedule
    sigma_max: 50  # specific for star hard
    sigma_min: 5   #  specific for star hard
    eps: 1e-3

trainer:
  max_steps: 80000 
  val_check_interval: null
  num_sanity_val_steps: 3
  check_val_every_n_epoch: 2

log_predictions:
  _target_: xlm.log_predictions.LogPredictions
  fields_to_keep_in_output:
    - text
    - truth
  inject_target: target_ids
  writers:
    - file
    - logger

callbacks:
  checkpoint_monitor:
    monitor: val/lm/accumulated_loss

optimizer:
  lr: 0.0001

lr_scheduler:
  name: "constant" 
  num_warmup_steps: 500
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"

predictor:
  sampling_method: sample_top_k
  top: 1
  second_sampling_method: sample_top_k
  second_top: 1
  max_steps: 200
  length_temperature: 1.0
  use_first_step_factor: true
  send_t_to_model: true
