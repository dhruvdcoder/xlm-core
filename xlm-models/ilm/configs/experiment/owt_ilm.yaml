# @package _global_
defaults:
  - text_ilm
  - override /datamodule: owt_ilm
  

per_device_batch_size: 32
global_batch_size: 512
input_block_size: null
block_size: 1024
monitored_metric: val/lm/accumulated_loss
num_dataloader_workers: 5

datamodule:
  print_batch_fn: ilm.datamodule_ilm.print_batch_ilm
  dataset_managers:
    val:
      unconditional_prediction:
        dataset_constructor_str: ilm.datamodule_ilm.ILMEmptyDataset


loss:
  length_loss_weight: 0.001
  stopping_class_weight: 0.9


trainer:
  max_steps: 1000_000 # 1M
  val_check_interval: 50000
  num_sanity_val_steps: 3


optimizer:
  lr: 0.0001

lr_scheduler:
  name: "constant_with_warmup" # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L435
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 2000
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"


predictor:
  sampling_method: sample_top_p
  p: 0.9
  second_sampling_method: sample_top_k
  second_top: 1
  max_steps: 1024
  max_length: 1024
  stopping_threshold: 0.9


callbacks:
  checkpoint_every_n_steps:
    every_n_train_steps: 2500
    keep_multiple: 40 # Keep every (keep_multiple * every_n_train_steps) permanently = 2500 * 40 = 100_000