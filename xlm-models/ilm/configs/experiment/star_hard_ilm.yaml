# @package _global_
defaults:
  - star_easy_ilm
  - override /datamodule: star_hard_ilm
  


input_block_size: 116
block_size: 28
monitored_metric: val/lm/accumulated_loss

datamodule:
  print_batch_fn: ilm.datamodule_ilm.print_batch_ilm

global_components:
  tokenizer:
    _target_: xlm.datamodule.SimpleSpaceTokenizer.for_numbers
    vocab_size: 56 # 20 (easy,medium), 56 (hard) 
  

optimizer:
  lr: 0.0001

lr_scheduler:
  name: "constant" # https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L435
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 500
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"


predictor:
  sampling_method: sample_top_k
  top: 1
  second_sampling_method: sample_top_k
  second_top: 2